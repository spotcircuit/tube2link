=== VIDEO METADATA ===
Title: The 9 Best Ways to Scrape Any Website in N8N
Channel: Nick Saraev
Duration: 1:08:45
Views: 1,230
Likes: 88

Description:
Join Maker School & get your first automation customer ‚§µÔ∏è
https://www.skool.com/makerschool/about

Summary ‚§µÔ∏è
Here I show you the 9 best ways to scrape websites using N8N.

First, I'll cover the basics of handling static and dynamic sites (and the differences between them. I'll then show you how to manage direct HTTP requests in N8N before demoing tools like Firecrawl, RapidAPI, and Browserless. Plus, I'll explore options like the Web Scraper Chrome Extension and Apify for advanced tasks. Let me know if q's!

My software, tools, & deals (some give me kickbacks‚Äîthank you!)
üöÄ Instantly: https://link.nicksaraev.com/instantly-short
üìß Anymailfinder: https://link.nicksaraev.com/amf-short
ü§ñ Apify: https://console.apify.com/sign-up (30% off with code 30NICKSARAEV)
üßëüèΩ‚Äçüíª n8n: https://n8n.partnerlinks.io/h372ujv8cw80
üìà Rize: https://link.nicksaraev.com/rize-short (25% off with promo code NICK)

Follow me on other platforms üòà
üì∏ Instagram: https://www.instagram.com/nick_saraev
üïäÔ∏è Twitter/X: https://twitter.com/nicksaraev
ü§ô Blog: https://nicksaraev.com

Why watch?
If this is your first view‚Äîhi, I‚Äôm Nick! TLDR: I spent six years building automated businesses with Make.com (most notably 1SecondCopy, a content company that hit 7 figures). Today a lot of people talk about automation, but I‚Äôve noticed that very few have practical, real world success making money with it. So this channel is me chiming in and showing you what *real* systems that make *real* revenue look like.

Hopefully I can help you improve your business, and in doing so, the rest of your life üôè

Like, subscribe, and leave me a comment if you have a specific request! Thanks.

Chapters
00:00:040Introduction to Web Scraping in N8N
00:00:42 Understanding Static vs Dynamic Sites
00:02:44 First Method: HTTP Requests
00:05:44 Converting HTML to Markdown
00:09:51 Using OpenAI for Data Processing
00:10:17 Second Method: Firecrawl Service
00:11:25 Signing Up for Firecrawl
00:14:10 Third Method: Rapid API Marketplace
00:19:24 Fourth Method: Web Scraper Chrome Extension
00:25:36 Fifth Method: Appify for Scraping
00:34:30 Sixth Method: Data for SEO API
00:41:18 Seventh Method: Crawlbase for High Volume Requests
00:48:51 Eighth Method: Octoparse for Local Scraping
01:03:30 Ninth Method: Browserless for Dynamic Sites
01:08:20 Conclusion and Future Content Suggestions

Tags: automation, make.com, content creation, ai content, google sheets, chatgpt, wordpress, openai, blogging, integromat, make, automating, automate, gpt-4, gpt, openai api, indie hacking, small business, $20K/mo, cold email, make money online, make.com for people who want to make real money, make.com money, AIAA, ai automation, ai automation agency, ai automation guide, n8n scraping, n8n how to scrape, n8n scraper, n8n scrape, n8n scraping setup, n8n scrapers, n8n ai scraper, firecrawl

=== TRANSCRIPTION ===
[00:00] hey Nick here today I'm going to show
[00:01] you the nine best ways to scrape any
[00:03] website in nadn you're going to be able
[00:05] to scrape static sites Dynamic sites
[00:07] JavaScript social media whatever the
[00:09] heck you want by the end of the video
[00:10] you'll know how to do it I scaled my
[00:12] automation agency to 72k a month using
[00:14] no code tools like make and nadn and
[00:16] scraping was a big part of that so this
[00:18] video is just going to give you all the
[00:18] sauce you're going to learn everything
[00:19] you need to scrape websites like that on
[00:21] your own let's get into it all right I'm
[00:23] going to jump into NN in a minute and
[00:24] actually build these alongside you and
[00:25] one other thing I'm going to do is I'm
[00:26] actually going to sign up to all the
[00:27] services in front of you walk you
[00:29] through the Authentication and the
[00:30] onboarding flows and get your API keys
[00:32] and stuff like that but just before I do
[00:34] want to explain very quickly the
[00:36] difference between a static site and
[00:38] then a dynamic site because if you don't
[00:40] know this um scraping just gets a lot
[00:42] harder and so we're just going to cover
[00:43] this in like 30 seconds and we can move
[00:45] on so basically um if this is you okay
[00:47] you're just this wonderful smiley person
[00:50] and you want to access a static website
[00:52] what you're doing is you're sending a
[00:54] request over basically to just like some
[00:56] document you know think about this as
[00:58] just like a piece of paper on a cupboard
[00:59] and there's a bunch of text on this
[01:01] piece of paper and what you do is you
[01:02] say hey can I have this piece of paper
[01:04] and then the piece of paper just comes
[01:06] back to you with all of the information
[01:07] inside of the piece of paper okay this
[01:09] is a very simplified version of what's
[01:11] actually going on but static sites are
[01:13] by far the easiest thing to scrape and
[01:15] so um this is where you know a lot of
[01:17] people think all websites are are at and
[01:20] then they kind of confuse it with this
[01:21] next step which is dynamic a dynamic
[01:23] site essentially is not like that at all
[01:26] basically what you're doing is you're
[01:27] sending a request to a piece of paper
[01:30] but the piece of paper has nothing on it
[01:32] okay what happens is this piece of paper
[01:34] then sends a request to some other dude
[01:36] which I guess in this case is just a
[01:37] server really who will then he has a
[01:40] trusty pen in his hand and he'll
[01:41] actually write all of the stuff on said
[01:44] piece of paper and then you'll get the
[01:46] piece of paper back so um there's
[01:48] actually that intermediate step okay
[01:50] where basically you are pinging some
[01:51] sort of uh you know domain name or
[01:53] whatever then that domain name shoots
[01:55] some code over forces a server to
[01:57] generate all of the contents on that
[01:58] domain and then you get it
[02:00] this is obviously kind of a two-step
[02:02] process and then this is a three-step
[02:04] process so if you just understand that
[02:08] um you know when you scrape a dynamic
[02:09] resource what you're really doing is
[02:10] you're sending a request to a page which
[02:12] sends a request back to another server
[02:14] which then fills your thing this element
[02:16] eliminates 99% of the confusion because
[02:18] most of the time like scraping issues
[02:19] are hey I just ping this page but I got
[02:21] nothing back you know the HTTP request
[02:23] or or or whatever I I sent you know it
[02:26] was fine but for some weird reason
[02:27] there's nothing on this page of of any
[02:29] substance well if that was the case for
[02:31] you it was most likely um because this
[02:32] was empty before you sent it over um
[02:35] whereas you know simple scraped static
[02:37] resources tend to just like give you
[02:38] what you want really quickly and it's
[02:39] really easy all right so hopefully we at
[02:42] least understand that there's that
[02:43] difference between static and dynamic
[02:44] sites here um I'm not going to go into
[02:46] it more than that we're actually just
[02:47] going to dive in with both feet start
[02:49] doing a little bit of scraping and then
[02:50] we'll kind of see where we land I find
[02:51] the best way to do this stuff is just by
[02:53] example and and you know being practical
[02:54] about it so the first major way to
[02:57] scrape websites in NN is using direct h
[02:59] HTTP requests this is also what I like
[03:02] to think of as the Magic in scraping
[03:05] itself what we're going to do is we're
[03:07] going to use a node called the HTTP
[03:08] request node to send a get request to
[03:10] the website we want this is going to
[03:12] work with static websites and non
[03:14] JavaScript resources so let me give you
[03:16] guys a website that I'm going to be
[03:17] scraping here this is my own site it's
[03:19] called left click I'm about to do a
[03:20] redesign um but this is a static
[03:22] resource I know this because I built the
[03:24] site you know I I did it in code and
[03:26] basically all this is is just a document
[03:27] somewhere on my or on on a server some
[03:29] more so what I'm going to want to do is
[03:31] and I'm just going to pretend that I
[03:32] haven't done any of this so uh we're
[03:34] just going to go HTTP
[03:37] request HTTP request node looks like
[03:39] this we have a method field a URL field
[03:42] authentication field query parameters
[03:44] headers body and then some options down
[03:46] here as well all I'm going to do is I'm
[03:48] just going to paste in the website that
[03:50] I want to visit okay then I'm just going
[03:52] to test the step it's that easy now the
[03:55] response from this on the right hand
[03:56] side see all this code over here this is
[03:59] what's called HTML if you're unfamiliar
[04:01] and HTML is basically just the like it's
[04:04] it's the code behind the site so if I
[04:07] were to zoom in over here you see where
[04:09] it says I don't know let's let's go to
[04:10] my website let's just find a little bit
[04:11] of little bit of texture build hands off
[04:14] growth systems okay if I just command F
[04:17] and paste this in we actually have that
[04:19] text buried somewhere in this big long
[04:21] HTML string right so all that this HTML
[04:25] is is this is the code that is sent to
[04:28] my browser which is Google Chrome in
[04:30] this case then my browser takes the code
[04:32] and it just renders it into this
[04:34] beautiful looking thing well beautiful
[04:35] is a subjective State I would say but
[04:38] this uh wonderful looking thing in front
[04:39] of us which is this website with like
[04:41] sizing and the tabs and the divs and all
[04:44] that fun stuff okay so basically what
[04:47] I'm trying to say is everything over
[04:48] here on the right hand side this is the
[04:50] entire site we can do anything we want
[04:52] with this information um and we can
[04:54] carry this information forward to to do
[04:56] any one of our any one of many flows so
[04:59] in my case right looking at a bunch of
[05:00] code isn't really very pretty so one big
[05:02] thing that you'll find in the vast
[05:03] majority of modern um scraping
[05:05] applications is you'll find that they'll
[05:06] take that HTML which we saw earlier and
[05:08] they'll convert it to something called
[05:09] markdown okay so um this is a markdown
[05:13] node we have a mode of HTML to markdown
[05:16] and all I'm going to do is I'm going to
[05:18] grab that data and I'm going stick it in
[05:20] the HTML section of the HTML to markdown
[05:22] converter what do you think is going to
[05:24] happen when I test this well we're going
[05:25] to convert this from this big long ugly
[05:27] super dense uh thing with a much these
[05:30] like greater than and less than symbols
[05:31] and we're just going to convert it into
[05:33] something a little bit shorter a little
[05:34] bit simpler This Is Us just manipulating
[05:36] file formats by the way and I find that
[05:38] manipulating file formats is a big part
[05:39] of what makes a good scraper a good
[05:41] scraper so now we have something in
[05:43] what's called markdown format what's the
[05:45] value there well markdown format does
[05:46] two things for us one it's much easier
[05:48] to parse parse just means we can extract
[05:50] different sections of the text we want
[05:52] we can structure it in some sort of
[05:54] other data format um and then in my case
[05:56] because I love using AI for everything
[05:58] it's much easier and shorter for us to
[06:00] use with AI so I'm going to give you
[06:01] guys a very simple example where we take
[06:03] this text from the static resource and
[06:05] then we just use um AI to tell us
[06:08] something about it so I'll go down to
[06:09] open Ai and then what I'm going to do is
[06:11] I'll do the message a model just have to
[06:14] connect my credential here I'm assuming
[06:16] that you've already connected a
[06:16] credential if not you're going to have
[06:18] to go to opena website when you do the
[06:20] connection um and grab your API key and
[06:22] paste it in there's some instructions
[06:24] that allow you to do so right over
[06:26] here uh what I'm going to do is I'm
[06:28] going to grab the G PT 40 Mini model
[06:31] that's just the uh I want to say most
[06:33] cost effective one as of the time of
[06:34] this recording and then what I'm going
[06:36] to do is I'm going to add three prompt
[06:37] I'm going to add a system prompt first
[06:39] I'll say you are a helpful intelligent
[06:42] web scraping
[06:43] assistant then I'm going to add a user
[06:46] prompt and I'll say your task is to take
[06:48] the
[06:49] raw
[06:51] markdown of a website and convert it
[06:54] into structured data use the following
[06:57] format and then I'm going to give it an
[06:59] example of what I want in what's called
[07:00] Json JavaScript object notation format
[07:02] so the very first thing I'm going to do
[07:04] is I'm going to have it just pull out
[07:05] all the links on the website because I
[07:07] find that that's a very common scraping
[07:08] application so I go links and then I'm
[07:10] just going to show an example of un
[07:13] array of we'll go absolute URLs this is
[07:19] very important that they're absolute
[07:20] URLs any thing that we're going to build
[07:22] after this is going to be making use of
[07:23] the absolute URLs not the relative URLs
[07:25] if you're unfamiliar with what that
[07:26] means if we Zoom way in here you see how
[07:28] there's this B uh SL left click log.png
[07:31] this is what's called a relative URL if
[07:33] you were to copy this and paste this
[07:35] into here this wouldn't actually do
[07:37] anything for us right uh what we what we
[07:39] want is we want this instead we want
[07:41] left click aka the root of the domain
[07:43] and then um left click _ logogram and
[07:46] that's how we get to the actual file
[07:47] asset so uh if we go back over
[07:50] here so if we go back over here um yeah
[07:53] you know basically we want a link array
[07:56] of absolute URLs and then I'm just going
[07:58] to want main text website copy this is
[08:02] going to be a long string containing all
[08:05] of the website
[08:06] copy containing just the text of the
[08:09] site no
[08:11] formatting and then why don't we do one
[08:13] more thing why don't we have like a
[08:16] summarized or summary let's do one line
[08:19] summary just to show you guys you can
[08:21] also use AI to do other cool stuff you
[08:23] could take this oneline summary and feed
[08:25] it into some big sequence you could have
[08:26] ai write an icebreaker for an email you
[08:29] could do a things with this but I'll say
[08:30] on line summary um brief summarization
[08:34] of what the site is and
[08:37] how what the site is about let's do that
[08:42] okay so this is our example I'm going to
[08:45] say your website URL
[08:48] is left click URL for the relative to
[08:53] Absolute
[08:54] conversions is left click. and then the
[08:56] final thing is I'm going to add one more
[08:58] user prompt I'm just going to draw drag
[08:59] all of that markdown data in here then
[09:01] I'm going to click output content as
[09:03] Json I'm going to test the step I'm
[09:06] going to take a sip of my coffee while
[09:07] this puppy processes and we now have our
[09:09] output on the right hand side if we go
[09:11] to schema view what you can see is we've
[09:14] now
[09:15] generated basically an array of links on
[09:17] the rightand side which contains every
[09:19] link on this website very cool looks
[09:21] like the vast majority of these are type
[09:23] form links for some reason don't really
[09:24] know what's about that oh right it's
[09:26] because that's basically the only thing
[09:27] on my website it's just a one P with a
[09:29] bunch of different links to time for
[09:31] that's funny um anyway you could
[09:33] obviously just get it to Output one link
[09:35] or tell it like make sure all the links
[09:36] are unique or something um and then we
[09:38] have a big chunk of plain text website
[09:40] copy right then we have a oneline
[09:41] summary of the site so this is a very
[09:44] simple example of scraping we're
[09:46] scraping a static resource obviously but
[09:48] when I build scrapers for clients or for
[09:50] my own business this is always my first
[09:52] pass I will always just make a basic
[09:54] HTTP request to the resource that I'm
[09:56] looking at because if I can make that
[09:58] http request work whether it's a get
[10:00] request or whatever the the the rest of
[10:03] my life building scraper building the
[10:05] scraper is so easy I just take the data
[10:08] I process it usually using AI or some
[10:09] very cheap Tok cheap per token thing and
[10:12] then voila you know like we've basically
[10:14] built out a scraper in this case and
[10:15] it's only taken us what three nodes
[10:17] right so that's number one the second
[10:19] way to scrape websites in NN is using a
[10:21] third party service called fir crawl and
[10:22] making an HTTP request to it I'm using
[10:25] something called their extract endpoint
[10:27] but just to make a long story short fire
[10:29] craw is a very simple but High uh
[10:32] bandwidth service that turns websites
[10:35] into large language model ready data and
[10:37] basically you know how earlier we had to
[10:38] do HTTP request and then we had to
[10:40] convert all that stuff into markdown and
[10:41] then we had to you know manipulate that
[10:43] markdown what this does is it just does
[10:45] a lot of that stuff for you it'll
[10:46] actually allow you to go scrape and then
[10:48] it will automatically convert text into
[10:49] markdown for you um so that you can do
[10:52] whatever the heck you want they turn it
[10:53] into structure data using Ai and and so
[10:54] on and so on and so forth so if I were
[10:56] to do the same thing that I just did
[10:58] earlier
[10:59] with my own
[11:00] website then I were to you know run an
[11:02] example of this what it would go do is
[11:05] it would basically spin up a server for
[11:07] me and that would actually go and
[11:08] generate markdown of the same format um
[11:10] the only difference here is it's
[11:11] actually generated new lines between
[11:13] sections of text how beautiful um and
[11:15] then now you know we have basically the
[11:17] same thing you also get it in Json which
[11:19] is pretty cool um and you know you can
[11:20] slot this into any workflow this is
[11:22] basically like the simple and easy way
[11:23] of getting started um what we're going
[11:26] to be showing you today is the extract
[11:28] endpoint which allows you to extract
[11:30] data just using a natural language
[11:32] prompt which is pretty cool and from
[11:34] here we're going to be able to take any
[11:35] URL and just turn it into structure data
[11:37] but we're not actually going to have to
[11:37] know how to parse we're not going to
[11:38] have to know any code we're not going to
[11:39] have to know any of that stuff so let me
[11:41] actually run through the signup process
[11:42] with you guys go to fire. Dev here just
[11:47] going to open this up in an incognito to
[11:48] show you guys what this looks like all
[11:49] you do is you just go sign up I'm going
[11:53] to add a password we're then going to
[11:55] have to validate this one
[11:58] oh guess these guys are Mega
[12:01] secure so now I'm going to go back to my
[12:03] email
[12:04] address I'm going to count this up and
[12:09] we have a call back here I just need to
[12:10] paste this URL and put it in here that's
[12:12] just because I you know I'm doing this
[12:14] in an incognito tab normally when you do
[12:16] this you're not going to have that step
[12:17] okay great now we're inside a fir craw
[12:20] they give you I think something like 500
[12:22] um free credits something of that nature
[12:24] anyway so what I'm going to do is I'm
[12:26] going to go through and just like give
[12:28] give this extracting point just a basic
[12:30] natural language query so um let's go
[12:34] from the homepage at left
[12:37] click. I want to
[12:40] extract a oneline summary of the website
[12:45] let's do all of the text on the
[12:50] website all of the copy on the website
[12:53] in plain text let's do a oneline summary
[12:56] of the website a oneline Icebreaker I
[12:58] can use as the first line of of an of a
[13:01] cold email to the
[13:05] owner and uh the company name and a list
[13:11] of the services they provide let's do
[13:14] that this is a lot of requests we're
[13:17] asking it to do like seven or eight
[13:19] things but all I need to do in order to
[13:21] make this work is I click generate
[13:22] parameters it's going to basically now
[13:24] generate me a big object with a bunch of
[13:26] things so copy summary Icebreaker
[13:28] company name and now I can actually go
[13:30] and I can run this okay this is the URL
[13:33] it just parsed as well let's give it a
[13:35] run what it's doing now is it's scraping
[13:38] the pages using their high throughput
[13:39] server I just love this thing like I'm
[13:42] not sponsored by fire crawl or anything
[13:43] like that but I love their uh I don't
[13:45] know I just love the design I love this
[13:47] little like burning Ember or whatever
[13:49] the heck you want to call it I love how
[13:51] simple they've tried to make everything
[13:52] it's it's great honestly okay awesome
[13:55] and now you guys see we have basically a
[13:57] big array with a bunch of sub objects we
[14:00] have a summary like I asked for a list
[14:02] of services looks like we even have
[14:04] links to the specific places oh okay
[14:06] links from the resource we have an
[14:08] icebreaker and then we have the company
[14:10] name as well so we can do a lot with
[14:11] this right but right now this is just um
[14:13] this is just on on a website how do we
[14:14] actually bring this in naden uh well
[14:16] it's pretty simple as you see there's an
[14:17] integrate Now button you can either get
[14:19] code or you can use it in zap here
[14:21] basically what we're going to want to do
[14:22] is we're going to want to run a request
[14:24] to um their endpoint and then we're
[14:26] going to want to turn that into
[14:29] basically our HTTP request let me show
[14:30] you what that looks like I'm just going
[14:33] to do all of this stuff in curl so if we
[14:35] go to curl as you can see what we need
[14:38] to do is we need to format a request
[14:40] that looks something like this but we
[14:41] need to make sure it's using the extract
[14:43] endpoint okay so I'm going to go down to
[14:45] extract and then now I have this big
[14:47] long beautiful string what I'm going to
[14:50] do is I'm going to copy
[14:51] this I'm going to go back to my NN
[14:55] instance which is right over here and
[14:56] then what I need to do is just open up
[14:58] an HTTP request module and then click
[14:59] import curl just paste all the stuff
[15:01] inside now this is an example request
[15:03] but that's okay we can actually use that
[15:04] example request to very quickly and
[15:05] easily format our our real request okay
[15:08] so we're sending a bunch of headers this
[15:09] is the endpoint that we're calling api.
[15:11] fire. dv1 extract so basically what
[15:14] we're doing now is we're like we're
[15:15] sending a request to fir craw which will
[15:16] then send a request to the website right
[15:18] so kind of a kind of a middleman and
[15:20] then all I'm going to do so if I go back
[15:22] to my example we have an API key here
[15:24] which we're going to need so I'm going
[15:25] to go here and then paste in an API key
[15:28] so that's how that work works right
[15:29] authorization is going to be the name
[15:30] value is going to be bear with a capital
[15:32] b space and then the API key and then we
[15:35] also have a body that we need to uh
[15:37] adjust or edit and this body is where
[15:39] we're going to put the links that we
[15:40] want to actually have scraped with the
[15:41] extract end point so what I'm going to
[15:44] do is I'm going to delete most of these
[15:46] I'll go back to my left click. a just
[15:48] like this the prompts um because you
[15:50] know I was just using their playground
[15:51] before we're actually going to need to
[15:52] convert this into a request for my
[15:56] service so I'm just going to paste The
[15:57] Prompt in here voila
[15:59] and now we need to put together what's
[16:00] called a schema where we have the
[16:02] objects that we asked for so in my case
[16:04] we asked for copy right so I'm going to
[16:06] go Copy Type string then summary so
[16:09] we're going to go summary type string
[16:12] then Icebreaker it's going to be
[16:14] Icebreaker type string then guess what
[16:16] we have last but not least company name
[16:18] which is going to be type string we're
[16:21] also going to want to make these fields
[16:22] required like uh you know you can set it
[16:24] up so they're not actually required when
[16:25] you do a request a fire call I'm I'm
[16:27] going to make it so they're required so
[16:28] I'll go copy
[16:29] summary Icebreaker and then company name
[16:32] actually you know what maybe I'll leave
[16:34] company name as unrequired if you think
[16:36] about it logically maybe not all the
[16:38] websites we're going to be scraping
[16:39] using this service are going to have the
[16:40] company names visible on the website I
[16:42] don't know but maybe so maybe I'll
[16:44] actually leave that as off okay great so
[16:46] now we have the API request formatted
[16:48] correctly um all we need to do at this
[16:50] point is just click test step it looks
[16:52] like we're getting a Json breaking um
[16:55] error and I think that's because I have
[16:56] this last comma and I'm just going to
[16:58] check to see if there are any commas in
[16:59] Jason you can't actually have the last
[17:00] element in an array have a comma on it
[17:03] so I think that's okay let me test it
[17:05] again all right so as you can see we
[17:06] just received an ID we've got a success
[17:08] and then we have a URL Trace array which
[17:10] is empty um if you think about this
[17:12] logically we don't actually get all the
[17:14] data that we send immediately because we
[17:16] need fir crawl to whip up the scraper
[17:18] you know do things to the data we could
[17:20] be feeding in 50 URLs here right so
[17:22] instead of just having the data
[17:23] available to us right now immediately
[17:25] what we need to do is we need to
[17:25] basically wait a little while wait until
[17:27] it's done and we need to Ping it and the
[17:29] reason why they've given us this ID
[17:31] parameter so that we could do the
[17:32] pinging so the way that you do this is
[17:34] you'd have to send a second HTTP
[17:36] request using this
[17:38] structure so the good news is we could
[17:41] just copy this
[17:42] over and then we can
[17:45] add a second um HTTP request I don't
[17:49] know where that went but I guess I'm
[17:50] just going to create it over here I'm
[17:53] going to import the curl to this request
[17:55] just like
[17:56] that then keep in mind that we just need
[17:58] to add our API key again because the
[18:00] previous node had it but this one
[18:01] doesn't so just going to go over here
[18:03] I'm going to copy this
[18:05] puppy go back over here I'm going to
[18:08] paste this in now technically what this
[18:10] is called is this is called polling um
[18:12] polling uh is where you know you're
[18:15] you're you're attempting to request a
[18:17] resource that you don't know whether or
[18:18] not is ready and there's a fair amount
[18:20] of logic that I'd recommend like putting
[18:22] into a polling flow where like when you
[18:24] try it and if it doesn't work basically
[18:25] you wait a certain amount of time and
[18:26] you retry again for the purpose of this
[18:28] video I'm not going to put all that
[18:29] stuff inside but um what I'm going to do
[18:31] is just set up this request I'm going to
[18:33] give this puppy a test let's just feed
[18:36] that in on the back end we got to put
[18:37] the extract ID right right over here
[18:40] where it said extract ID then I'm just
[18:41] going to give this a test uh looks like
[18:44] I've issued a malformed request we just
[18:46] have to make sure that everything here
[18:48] is okay specify body let me just make
[18:50] sure there's nothing else in here it was
[18:51] a get request this is a get cool we're
[18:54] not going to send a body
[18:56] then awesome and now we have all of the
[18:58] data available to us automate your
[19:01] business in the copy field summary field
[19:02] left clicks an ad performance
[19:04] optimization agency Icebreaker hi Nick I
[19:06] came across left click I'm impressed by
[19:07] you help B2B Founders scale their
[19:08] business automation keep in mind I never
[19:11] gave it my name it went it found my name
[19:12] on the website uh and then company name
[19:14] left click so quick and easy way uh
[19:17] you're going to have access to this
[19:18] template obviously without my API key in
[19:19] it um and feel free to you know use fir
[19:21] craw go nuts check out their
[19:22] documentation build out as complex a
[19:24] scraping flow as need be the third way
[19:25] to scrape websites in nadn is using
[19:28] rapid API for those of you that are
[19:29] unfamiliar rapid API is basically a
[19:31] giant Marketplace of third party
[19:33] scrapers similar to appify which I'll
[19:34] cover in a moment but instead of looking
[19:36] for um you know building out your own
[19:38] scraper for a resource let's say you're
[19:40] wanting to scrape Instagram or something
[19:41] that's not a simple static site what you
[19:43] can do is you could just get a scraper
[19:44] that somebody's already developed that
[19:46] does specifically that using proxies and
[19:48] all that tough stuff that I tend to
[19:50] abstract away um and then you just
[19:52] request uh to Rapid API which
[19:55] automatically handles the API request to
[19:56] the other thing that they want and then
[19:57] they format it and send it all back to
[19:59] and then you know you have beautiful um
[20:01] data that you could use for basically
[20:02] anything so this is what rapid API looks
[20:04] like it's basically a big Marketplace I
[20:05] just pumped in a search for website over
[20:08] here and we see 2,97 results to give you
[20:11] guys some context you can do everything
[20:12] from you know scraping social data like
[20:15] emails phone numbers and stuff like that
[20:17] from a website you could ping the ah
[20:19] refs SEO API you could find uh I don't
[20:23] know like unofficial medium data that
[20:25] they don't necessarily allow people to
[20:26] do so this is just a quick and easy way
[20:28] to I guess do a first pass after you've
[20:31] run through fir crawl maybe that doesn't
[20:32] work after you've run through HTTP
[20:33] request that doesn't work um just do a
[20:35] first pass look for something that
[20:36] scrapes the exact resource you're
[20:37] looking for and then take it from there
[20:39] so obviously for the purpose of this I'm
[20:40] just going to use the website to scraper
[20:41] API which is sort of just like a wrapper
[20:44] around what we're doing right now in
[20:45] nadn um but this website scraper API
[20:48] allows you to scrape some more Dynamic
[20:49] data um now I'm not signed up to this so
[20:51] I'm going to have to go through the
[20:52] signup process and I'm going to show you
[20:53] guys what that looks like um but yeah
[20:55] we're going to we're going to run
[20:55] through an API request to Rapid API
[20:57] which is going to make this a lot easier
[21:00] just going to put in all of my
[21:02] information
[21:04] here and then I'm going to do the
[21:07] classic email verification okay just
[21:11] copy this puppy over no thank you rise I
[21:13] use a time management app called rise
[21:16] and every time I go on my Gmail I set my
[21:18] Gmail up as like a definitely do
[21:22] not uh do during your workday let's just
[21:25] call it personal projects they don't ask
[21:26] me all these questions my goal today is
[21:28] to browse available apis awesome so
[21:31] that's their onboarding I think we're
[21:32] going to have to like pay a little bit
[21:33] of money or something like that which
[21:34] I'll sort out in a moment um but the
[21:36] scraper that I want is I just want the
[21:37] website one right so I'm going to type
[21:38] website in
[21:39] here uh I'm going to look
[21:42] for wherever it was earlier website
[21:44] scraper API and now check this out what
[21:47] we have is we have the app which is the
[21:49] name of the specific API that we're
[21:51] requesting we have an x-raid api-key and
[21:55] this is the API key we're going to use
[21:56] to make the request then we have the
[21:58] request URL which is basically what
[22:00] we're pinging and what we can do here is
[22:01] we can feed in the parameters okay what
[22:03] website we want to we want to scrape and
[22:05] then we can actually just like give it
[22:06] give it a run so I'm going to have to
[22:08] subscribe to this in order to test it uh
[22:10] I'm just going to go to the um basic
[22:12] plan and I'm going to pay money per
[22:14] month that probably seems the simplest
[22:15] way to do so okay and I just ran through
[22:17] the payment let's actually head over
[22:18] here and let's just run a test using my
[22:20] website URL we're going to test this
[22:22] endpoint now and now this actually going
[22:24] to go through Rapid API it's going to
[22:25] spin up the server and then it's going
[22:27] to send it and what we see here is we
[22:29] have multiple fields that Rapid apis or
[22:31] this particular scraper gives us let me
[22:32] just make this easier for you all to see
[22:34] we have a text content field with all of
[22:35] the content of the website which is cool
[22:37] this is basically what I did earlier um
[22:39] but instead of me having to formulate
[22:40] this request try and parse it and try
[22:42] and use AI tokens what I did is I sent
[22:43] the request to uh rapid API and did it
[22:45] all for me then we also have an HTML
[22:47] content
[22:48] field I think we have one more here
[22:51] scroll all the way down to the bottom as
[22:53] you can see there is a ton of HTML um
[22:55] and then we also have a list of all of
[22:56] the images on the website which is very
[22:58] very cool and easily formatted again
[23:00] something that I tried to do manually
[23:01] using AI but now you know we have
[23:03] everything in that nice absolute URL
[23:04] format um and then if they find any
[23:06] social media links I don't believe um
[23:09] there were more than Twitter but um if
[23:11] they find anything that's at their
[23:12] Twitter Instagram whatever then we have
[23:14] the link right over here it looks like
[23:15] they even give you the scraping time and
[23:17] if they scrape emails or phone numbers
[23:18] um they'll be there as well so I mean
[23:20] rapid AP is obviously fantastic this is
[23:21] a high throughput sort of thing and why
[23:23] don't we actually run through what this
[23:25] would look like if we were to run a curl
[23:26] request you see how it's automatically
[23:29] just formatting it as curl well that
[23:30] just means we just jump back here
[23:32] connect this to my HTTP request module
[23:35] click import curl paste it in like this
[23:38] import and it's actually going to go
[23:39] through and it's going to automatically
[23:41] map all these fields for me right query
[23:43] parameter URL left click. beautiful um
[23:45] API key x-raid API host here's the host
[23:49] here's the name of the API key here's
[23:51] everything we need well I can actually
[23:52] just recreate this request now inside of
[23:54] NN as opposed to being on rapid API and
[23:57] then I have all the data accessible to
[23:58] me here how cool is that so we can do
[24:01] this for any any major website really um
[24:03] you know there are a lot of specific
[24:06] bespoke scrapers obviously which um I
[24:07] don't know if you wanted to scrape uh
[24:09] let's go back to Discovery if you wanted
[24:11] to scrape like Instagram or something
[24:14] you could scrape um Instagram uh you
[24:16] could do like Facebook scraping you
[24:18] could scrape these large giants that are
[24:20] quite difficult to do So Meta ad Library
[24:22] Facebook ad scraper and depending on the
[24:24] plan that you're at it might be more
[24:25] cost- effective for you to sign up to
[24:27] some sort of monthly recurring thing
[24:28] rather than just pay two cents every
[24:29] single time you make one of these
[24:30] requests you just kind of got to do that
[24:32] determination yourself right like if
[24:33] you're scraping uh I don't know 50 every
[24:36] day or 100 every day or something might
[24:38] be a dollar or two a day which is
[24:39] reasonable but maybe if you want to
[24:41] scrape like 5,000 doing it the way that
[24:42] I was doing it a moment ago might might
[24:43] be infusible the next way to scrape
[24:45] websites in nadn is using the web
[24:47] scraper Chrome extension and then tying
[24:49] that to a cloud service that delivers
[24:51] the data that you just created using
[24:53] their no code tool um in nicely bundled
[24:55] formats it's called Cloud sync as of the
[24:57] time of this recording I think they
[24:58] changed the name a couple of times but
[25:00] um that's where we're at here is the
[25:02] name of the service web scraper here is
[25:05] their website essentially what happens
[25:07] is you install a little Chrome plugin
[25:08] which I'll show you guys how to do then
[25:10] you select the fields that you want
[25:11] scraped in various data formats and then
[25:14] what you do is it handles JavaScript
[25:16] sites Dynamic sites all that fun stuff
[25:19] and then you can um export that
[25:22] data as a cloud run to then send back
[25:28] sorry big sneeze to then send back to
[25:31] some API or some service um and then
[25:33] automatically do parsing and stuff like
[25:34] that so very cool I'm going to show you
[25:36] guys what that looks like um this is
[25:37] sort of a more customized way to build
[25:38] the stuff but I've seen a lot of people
[25:40] do this with naden um so we're going to
[25:42] run through what it looks like so first
[25:43] thing I'm going to want to do is I'm
[25:45] going to want to let's just go Cloud
[25:47] login or sorry um start free 7-Day trial
[25:51] as you can see you know there's a free
[25:52] browser extension here if you wanted to
[25:54] do uh I don't know like highs scale
[25:56] stuff you'd choose probably their
[25:58] project um endpoint where we Sorry
[26:01] project plan where we have 5,000 URL
[26:03] credits we can run a bunch of tasks in
[26:05] parallel we could scrape Dynamic sites
[26:07] JavaScript sites we have a bunch of
[26:08] different export options then we can
[26:09] also just connect it directly to all of
[26:11] these um what I'm going to do just
[26:12] because I want this to kind of work as a
[26:14] first go is I'm just going to sign up to
[26:15] a free Tri here beautiful just created
[26:17] my account just go left click give it a
[26:21] phone number we'll go left click. a
[26:24] we're going to go I don't know academic
[26:26] records needed per month we'll go 0 to
[26:28] th000 length of the project uh I don't
[26:30] know let's go two to 3
[26:31] months okay great so now we can import
[26:34] and run our own site map or we can use a
[26:36] premade Community sit map um what I'm
[26:38] going to do is I'm just going to import
[26:39] this we're then going to get the Chrome
[26:41] extension web
[26:43] scraper let me add that extension and
[26:45] it's going to download it do all that
[26:47] fun stuff beautiful so now we have it
[26:50] right over here I'm just going to pin it
[26:52] to my browser to make my life easier go
[26:54] to left click. a open up this puppy now
[26:57] there's a bunch of like tutorials and
[26:58] how to use this stuff um that's not that
[26:59] big of a deal but basically the thing
[27:01] you need is you need to hold command
[27:03] plus option plus I to open up your
[27:04] developer tools and you'll just find it
[27:05] on the in my case the far right so
[27:07] command option I that'll open up Dev
[27:10] tools you see all the way on the right
[27:11] hand side here I have a couple other
[27:12] things like make and and cookie editor
[27:14] but all the way on the right hand side
[27:15] here we have this web scraper thing um
[27:18] so we got what you're going to want to
[27:19] do first you're going to want to create
[27:20] a site map for the resource that you're
[27:21] going to want to scrape I'm just going
[27:22] to call it left click and I just want to
[27:24] scrape left click. okay once we have our
[27:27] sitemap if I just give a quick little
[27:28] click I can then add a new selector and
[27:30] the really cool thing about this web
[27:32] scraper is um if I just zoom out a
[27:34] little bit here uh what you can do is
[27:35] you can you can select the elements on
[27:37] the website that you want scraped so for
[27:39] instance it's a very quick and easy way
[27:40] to do this if you think about it is like
[27:42] just to show you guys an example
[27:43] structure data is uh sort of like an
[27:45] e-commerce application let's say you
[27:46] have like the title of a product and you
[27:47] have like I don't know the the
[27:49] description of a product so on my
[27:50] website really quick and easy way to do
[27:51] this is let's just call this
[27:53] products and it's a type text what I'm
[27:56] going to do is I'm going to click select
[27:58] then I'll just click on this I'll click
[28:01] on this as well and as you see it'll
[28:03] automatically find all of the headings
[28:06] that I'm looking for so that's products
[28:09] we are going to then click data I'm
[28:10] going to click done selecting data
[28:12] preview as you can see it only selected
[28:14] one of them the very first so what we're
[28:15] going to want to do is go multiple and
[28:17] now if I data preview we get all of the
[28:19] headings which is very cool so now we
[28:21] have a basically like a list of headings
[28:23] um from here I'm going to save this
[28:25] selector I'm add a new one let's go
[28:27] product
[28:28] descriptions and then going to select
[28:31] this this it'll select all of them I'll
[28:34] go multiple data preview just to make
[28:36] sure that it looks good I'm getting no
[28:37] data extracted here oh sorry I didn't
[28:39] actually select the um didn't actually
[28:41] finish it now we're getting product
[28:42] descriptions that's pretty cool um this
[28:44] is me doing this sort of like one at a
[28:46] time you can also um group The selectors
[28:49] there you go it's actually um offered to
[28:51] group it for me so we can uh group this
[28:54] into one object with products and then
[28:56] product descriptions so it's automatic
[28:58] group it now we have wrapper for
[28:59] products and products descriptions then
[29:01] we have products and product
[29:02] descriptions buried underneath we could
[29:04] go as far as we want with this but
[29:05] basically what I'm what I'm trying to
[29:06] show you guys is very simple and easy
[29:08] just drag your mouse over the specific
[29:10] thing you want if you select more than
[29:12] one it'll automatically find all of them
[29:13] on the website which is really cool okay
[29:15] great once we have this um what I can do
[29:17] is I can actually go export sitemap so
[29:20] now I have all of the code on the
[29:21] website that actually goes and finds it
[29:23] for me then I can paste this in here
[29:26] I'll just call this left click scraper
[29:28] and I'm going to import this to my cloud
[29:31] scraper uh I think I'm running into oh
[29:34] sorry I don't think we can do a space
[29:36] there my bad just call it left click and
[29:38] now what we can do is we can actually
[29:39] just like run a server instance that
[29:41] goes out and then scrapes this for us
[29:42] okay so I'm going to click scrape it
[29:45] looks like I need to verify my email so
[29:47] just make sure you do that before you
[29:48] try and get ahead of yourself like I
[29:52] was okay looks like we just verified the
[29:54] email let's head back over here refresh
[29:57] then scrape we've now scheduled a
[29:59] scraping job for this sitemap scheduling
[30:02] you know in their lingo just means that
[30:03] it's now part of their big long queue of
[30:05] thousands of other things that they're
[30:06] probably scraping through their server
[30:07] and that's fine okay I just gave this a
[30:09] refresh and as we see we have now
[30:10] finished said scraping job we have all
[30:12] of the data available to us using their
[30:14] UI but now that we've gone through this
[30:16] process of you know building out this
[30:18] this thing um how do we actually take
[30:20] that and then use it in our nadn flows
[30:22] so variety of ways um if you wanted to
[30:24] connect this let's say to specific
[30:25] service like Dropbox um Google
[30:28] you know dump anow or something Google
[30:30] Drive I'd recommend just doing it
[30:31] directly through their integration it's
[30:32] just a lot easier to get the data there
[30:34] and then you can just connect it to n
[30:36] and watch the data as it comes in or
[30:37] something you can also use the web
[30:39] scraper API uh this is pretty neat
[30:42] because you can you know that's what
[30:43] we're going to end up using it was
[30:45] pretty neat because you can uh like
[30:47] schedule jobs you can send jobs you can
[30:48] do basically everything just through the
[30:50] NN interface and then we can just
[30:52] retrieve the data afterwards which is
[30:53] pretty neat um this is basically what
[30:55] you end up getting you end up with
[30:57] scraping job ID status sitemap all this
[30:59] fun stuff and then we can set like a web
[31:02] hook URL where we we receive the request
[31:05] so um let me check we need a scraping
[31:07] for testing you need a scraping job that
[31:09] has already been finished I think our
[31:10] scraping job has already been finished
[31:12] I'm just going to go htps uh back to my
[31:15] n8n flow I'm actually going to build an
[31:18] n8n web hook give that a click I'm not
[31:22] going to have any authentication let me
[31:24] just turn all this off basically what we
[31:26] want is we we want to use this as our
[31:27] test
[31:29] event we're going to go back to the
[31:32] API paste this in
[31:35] save and I'm just going to want to give
[31:36] it a test endpoint here so
[31:39] test looks like um the push notification
[31:42] was failed the reason why is because
[31:44] it's saying this web Hook is not
[31:45] registered for post request did you mean
[31:47] to make a get request beautiful thank
[31:48] you naden we absolutely did so I'm going
[31:50] to stop listening change your HTTP HTTP
[31:53] method here to post there's basically
[31:54] two ways to call a website and this is
[31:55] one of them I'm going to listen for test
[31:57] events go back here and then
[31:59] retest awesome looks like we've now
[32:01] triggered the beginning of our workflow
[32:03] using this data let's see what sort of
[32:05] information was in
[32:06] it okay great we have the scraping job
[32:09] ID the status execution
[32:11] mode okay great so we basically have
[32:13] everything we need now to set up a flow
[32:16] where we can schedule something in this
[32:17] web scraper service that maybe monitors
[32:19] some I don't know list of e-commerce
[32:21] product or something every 12 hours and
[32:23] then we can set up a web hook in NN that
[32:24] will catch the notification get the
[32:27] update now we can do is we can ping um
[32:30] we can ping the web scraping API which
[32:32] I'll show you to set up in a second to
[32:34] request the data from that particular
[32:35] scraping run and from here we can take
[32:38] that data do whatever the heck we want
[32:39] with it but obviously let me show you an
[32:41] example of what the the actual data
[32:42] looks like so we just got the data from
[32:44] web hook let's set up an HTTP request to
[32:48] their API now where we basically get the
[32:50] ID of the thing and then we can call uh
[32:53] we can call that back so got my API
[32:55] token over here I'm going head over to
[32:56] their API documentation first okay and
[32:59] then what we want to do is download
[33:00] these scrape data in CSV format at least
[33:03] in my case I imagine most of you guys
[33:04] are going to add this to a spreadsheet
[33:05] or whatever um you can very easily do
[33:08] whatever you want there's also a Json
[33:09] format endpoint here um but let's just
[33:12] do CSV for Simplicity so I've already
[33:14] gone ahead and I've gotten the method
[33:16] which was a get request so I've added
[33:18] that up here the URL was this over here
[33:21] with the scraping job ID and then your
[33:23] API token there so what I've done is
[33:25] I've grabbed the API token and the
[33:27] scraping job ID I mean I hardcoded it in
[33:28] here just while I was doing the testing
[33:30] let's actually make this Dynamic now
[33:33] drag the scraping job ID right over here
[33:36] voila and then the API token if you guys
[33:38] remember back here on the API page you
[33:41] have your access to API token so just
[33:42] copy that
[33:43] over uh great and now if I run this I'm
[33:47] actually selecting that specific job
[33:49] then from here we have all the data that
[33:50] we just scraped as you can see there's
[33:51] like a uh the way that CSV Works
[33:54] actually let me just copy this over here
[33:55] I just wanted to give this to you guys
[33:56] as an example of a different data type
[33:58] but maybe some people here aren't really
[34:00] familiar with it basically the way that
[34:01] it works is if I just paste this into
[34:02] like a Google sheet you see how it looks
[34:04] like this what what you can do is if you
[34:05] just um split the text to columns you
[34:08] kind of see
[34:09] how kind of see how there's like these
[34:11] four pettings there's web scraper order
[34:13] web scraper startup products and product
[34:15] descriptions right I'm imagine scraping
[34:17] this for some lead genen applica sorry
[34:19] some some e-commerce application list of
[34:21] products here product descriptions maybe
[34:23] product prices maybe product whatever
[34:24] the heck you want um so yeah you can you
[34:27] can put in like a number of formats and
[34:29] I just wanted to give you guys an
[34:29] example what that looks like the next
[34:31] way to scrape websites in naden is using
[34:33] appify if you guys are no strangers to
[34:35] this channel you know that I do appify
[34:37] all the time and I talk about them all
[34:38] the time because I think that they're
[34:39] just a great service um they've now
[34:41] given me a 30% discount where anybody
[34:44] can use it for I was initially under the
[34:46] impression it was lifetime I think it's
[34:47] three months so you probably get 30% off
[34:48] your first three months just check the
[34:50] um description if you want that but Cent
[34:52] how appify is is it is a Marketplace
[34:54] very similar to Rapid API um although
[34:56] extraordinarily well Main ained and they
[34:58] also have a ton of guides set up to help
[35:00] you get you know up and running with
[35:02] scraping any sort of application so just
[35:05] as we had earlier we have Instagram
[35:06] scrapers we have Tik Tok scrapers we
[35:09] have email scrapers we have map scrapers
[35:12] Google Maps we could do I don't know
[35:15] Twitter
[35:15] scrapers uh medium scrapers right
[35:18] basically any any service out there that
[35:22] has this Dynamic aspect to it that's not
[35:24] a simple HTTP request you can make you
[35:25] could scrape it using ampify and then
[35:27] obviously you you have things too like
[35:28] just like basic website crawlers you can
[35:30] generate screenshots of sites I mean
[35:32] there's just there's so many things let
[35:33] me walk you guys through what it looks
[35:35] like now in my case I'm not actually
[35:37] going to sign up to appify because I
[35:38] have like 400 accounts but trust me when
[35:40] I say it is a very easy and simple
[35:41] process you go to app ay.com you go get
[35:45] started you put in your email and your
[35:47] password they'll give you $5 in free
[35:50] platform credit you don't need any
[35:51] credit card and you can just get up and
[35:53] running and start using this for
[35:54] yourself super easily then the second
[35:56] that you have all that you'll be Creed
[35:57] with this screen it is a console screen
[36:00] don't be concerned when you see this um
[36:03] you know this is super simple and and
[36:05] easy and and not a big deal this is one
[36:06] of my free accounts um so I just wanted
[36:08] to show you guys what you can do with a
[36:09] free account uh but from here what you
[36:12] do is you go to the store and as you can
[36:13] see I'm just dark mode all this is the
[36:15] same thing we were just looking at
[36:16] before and then um we're just going to
[36:17] run a test on the thing that we want to
[36:19] scrape okay so what I'm going to want to
[36:21] do is for the purposes of this I'm now
[36:23] going to do something different from
[36:25] what I was doing before like which was
[36:26] just left click over and over and over I
[36:27] think that kind of gets boring what I'm
[36:29] going to do is I'm going to scrape
[36:30] Instagram posts okay so what I'm going
[36:32] to do is I'm going to feed in a name
[36:34] nickf this is just my um
[36:38] Instagram uh which almost hit 10K in God
[36:41] like 15 days or something like that but
[36:42] I'm going to feed in my Instagram here
[36:44] and then I'm just going to grab like I
[36:45] don't know the last 10 posts okay save
[36:48] and start this is now going to run an
[36:49] actor actor is just their term for
[36:51] scraper which will go out it'll extract
[36:54] data from my Nick surve Instagram and as
[36:57] you can see will get a ton of fields
[36:58] caption owner full name owner Instagram
[37:00] URL comments count first comment likes
[37:02] count timestamp query tag we get
[37:05] everything from these guys which is
[37:06] really cool this might take you know 30
[37:08] 40 50 seconds we are spinning up a
[37:10] server in real time every time you do
[37:12] this as you see in bottom left hand
[37:13] corner there's a little memory tab which
[37:14] shows that we are legitimately running a
[37:16] server with one gigabyte of memory right
[37:18] now so generally my recommendation when
[37:19] you use appify is not to use it for
[37:21] oneoff requests like this feed in 5 to
[37:23] 10 15 20 Instagram Pages uh but you know
[37:26] I just got the back and voila we we have
[37:28] it it's right in front of us we have all
[37:29] of the data of that person's Instagram
[37:32] profile so you can see it's quite
[37:33] scalable in that way um so the question
[37:35] is obviously how do you get this in NN
[37:37] well appify has a really easy to use um
[37:39] API which I like
[37:40] doing all you have is if we wanted to
[37:44] get the uh let's see get data set items
[37:47] okay all I'm going to do is I'm just
[37:49] going to copy
[37:50] this go back here and then connect this
[37:53] to an HTTP request
[37:55] module as you could see we have this big
[37:58] long field here with my API appify API
[38:01] token and this specific data set that
[38:03] I'm looking for I'll show you how to get
[38:04] it dynamically but I just wanted to like
[38:06] allow you to see how to get data in
[38:07] naden really quickly now if we go to the
[38:10] schem of view we can see we legitimately
[38:12] we we already have all of the data that
[38:13] we we had from appify a second ago okay
[38:16] super easy and quick and simple to get
[38:17] up and running um we have the input URL
[38:19] field the ID field the type the short
[38:21] code caption now this is Instagram um
[38:25] every looks like we have some comments I
[38:27] don't have any style how do I create my
[38:29] man you just got to fake it till you
[38:31] make it I don't have any style either
[38:33] just some nerd in my mom's basement uh
[38:36] yeah so you you can scrape any resource
[38:38] you want here um obviously I was
[38:40] scraping an Instagram resource but like
[38:41] if you were scraping something else
[38:43] there'd be no change to this at all no
[38:45] change whatsoever now uh basically what
[38:47] we need in order to make this Dynamic
[38:49] basically make us able to run something
[38:51] in appify and then get it in NN so we
[38:54] need to set up an integration so just
[38:55] head over to this tab set up integration
[38:57] and then all you want to do is you just
[38:59] want to do web hook send an HTTP post
[39:02] web Hook when a specific actor event
[39:03] happens the actor event that we're going
[39:05] to want is basically when the run is
[39:07] succeeded the URL we're going to want to
[39:09] send this to if you think about it we
[39:11] just actually make another web hook
[39:13] request here web
[39:15] hook the URL we're going to want to send
[39:17] it to is going to be this test URL over
[39:21] here now I'm just going to delete all
[39:22] the header off stuff here because um it
[39:24] just uh complicates it especially for
[39:26] beginners um but we're going to copy
[39:28] this over head back over here paste in
[39:30] this URL and then let me see this is a
[39:32] post request I think I don't actually
[39:34] remember so we're going to have to
[39:35] double check I think it's a post
[39:37] request yeah and then what I'm going to
[39:39] do is I'm going to listen for a test
[39:40] event run the test web
[39:43] hook so we're listening we're making a
[39:45] get request okay so the fact that it
[39:47] hasn't connected yet probably tells me
[39:48] it's a post request so let's move over
[39:50] here move this down to post now let's
[39:52] listen to a test event let's run this
[39:55] puppy one more time so we just
[39:57] dispatched it and yeah the post request
[39:59] succeeded and what did we get we got
[40:00] tons of information we got a body with a
[40:02] user ID created at event data joke right
[40:05] looks like when you test something out
[40:06] they just send you a joke about how
[40:08] Chuck nurse can sketi a cow in two
[40:10] minutes have you ever heard of the word
[40:11] sketi before this moment I haven't I
[40:13] want to be known for my ability to sketi
[40:16] we'll go Instagram website
[40:19] scraper okay and now if we go back here
[40:22] right we're now listening for a test
[40:24] event so I'm going to listen for this
[40:26] test event I'm going to run the same
[40:27] scraper again maybe we'll make it five
[40:28] posts per profile just to make it a
[40:29] little
[40:30] faster and um once this is done what
[40:33] it's going to do is it's going to send a
[40:35] record of all the information we need to
[40:37] get the data over to Ann we're going to
[40:40] catch that information and then we're
[40:42] going to use it to query the the the
[40:44] database basically that it created for
[40:45] that particular Instagram run which will
[40:48] then enable us to do whatever the heck
[40:49] we want with it so it's now starting to
[40:51] crawl as we see here we had five
[40:53] requests so it should be able to do this
[40:54] in like the next 5 seconds or so okay
[40:56] and once that's done we now have an
[40:57] actor succeeded event um and then we
[41:00] have uh let me see the data that we want
[41:04] would be the default data set ID down
[41:06] over here so if we just go to that next
[41:08] HTTP request node what I can do is I can
[41:11] feed that in as a variable right
[41:14] here let going to a default data set
[41:18] ID drag that in between these two little
[41:23] lines and now we can test that step with
[41:25] actual live data now we have everything
[41:26] that we need
[41:27] so I don't know maybe now you want to
[41:29] feed this into Ai and you want to have
[41:30] ai tell you something about the last
[41:32] five posts tell you wow those last five
[41:34] posts were amazing Nick I loved the
[41:37] specifically the one on Korea and I just
[41:39] wanted to send you over some quick
[41:40] assets to help you out right you can now
[41:42] do super Dynamic and structured Outreach
[41:46] you could take that data and use it to
[41:48] like draft up your own post I mean the
[41:50] options are ultimately unlimited that's
[41:52] why I love appify so much the sixth way
[41:54] to scrape websites with NN is data for
[41:57] Co this is another thirdparty service
[41:59] but it's a very high quality one that's
[42:00] specifically geared towards search
[42:02] engine optimization requests you guys
[42:04] haven't seen data for SEO before it's
[42:06] basically this big API stack that allows
[42:08] you to do things like automatically
[42:10] query a service maybe some e-commerce
[42:12] website or some content website and then
[42:14] like extract things in nicely structured
[42:16] formatting um again specifically for SEO
[42:19] purposes tons of apis here as well I
[42:21] mean a lot of these services are now
[42:22] going towards like more Marketplace
[42:24] style stuff but just to give you guys an
[42:26] example you could like Google really
[42:28] quickly to scrape a big list of Google
[42:30] search results for a term and then you
[42:31] could like feed that into one of any of
[42:33] the other scrapers that we set up here
[42:34] to get data on stuff you could go Google
[42:36] Images Google Maps you could do Bing BYO
[42:39] YouTube Google's uh their own data set
[42:41] feature I don't really know what that is
[42:42] but I imagine it's pretty cool uh and
[42:44] then you can you can take this data and
[42:45] do really fun stuff with it so I'm just
[42:47] going to click try for free over here in
[42:48] the top right hand corner show you guys
[42:50] what that looks
[42:52] like and as you see here um I signed in
[42:55] to data for SEO to my own account looks
[42:57] like I have 38 million bajillion
[43:00] dollars um but obviously you'd have to
[43:03] go through the rig Rolla creating your
[43:04] own account so why don't actually just
[43:06] do that with you and then I'll just use
[43:07] that account that is 38 million
[43:08] bajillion dollars we'll click try for
[43:11] free we'll go Nikki
[43:15] Wiki uh let's use a different email I
[43:17] need a business email huh that's
[43:21] unfortunate okay I do agree to the terms
[43:23] of use
[43:25] absolutely uh bicycle
[43:28] is that a bicycle that's not a
[43:32] bicycle what does it mean when I can't
[43:35] answer these does it mean that I'm a
[43:38] robot if you look at some of my posts
[43:40] some of my comments people would
[43:42] absolutely say yes it means that that
[43:45] you're a robot um I don't know why
[43:47] people keep saying stuff like dude Nick
[43:49] nice AI Avatar bro but I'm it's not an
[43:52] AI Avatar it's not an AI Avatar at all
[43:55] it's actually just me okay anyway so I
[43:57] need to activate my account doesn't look
[43:59] like it allows you to feed in the code
[44:01] here so I'm just going to feed it in
[44:02] myself uh it's obviously you're getting
[44:04] a lot of spammers hence
[44:05] this um bicycle stuff I don't know why
[44:09] the code isn't working here let me just
[44:12] copy this link address paste it in here
[44:15] instead there you go okay great so now
[44:17] you can sign
[44:19] in and once you're in you got also um
[44:23] they're actually really big on on
[44:24] bicycles they're training um a model to
[44:26] convert all ads on planet Earth into
[44:28] bicycles they'll actually give you a
[44:29] dollar worth of API access uh credits
[44:32] which is pretty cool um I'm not going to
[44:34] do that I'm just going to go over to
[44:35] mine which is$ 38 million bajillion
[44:36] dollars with 99,999 estimated days to go
[44:40] um and yeah let's actually run through
[44:41] this the first thing that I recommend
[44:42] you do is go over to playground on the
[44:43] Le hand side there's all of their
[44:45] different API endpoints that you can
[44:47] call um what I'm going to do is I'll
[44:48] just go to serp for now just to show you
[44:50] that you could scrape Google with this
[44:51] pretty easily so maybe I'm in the UK and
[44:54] I want to scrape um let me see
[44:58] a keyword ni arrive okay then I'm going
[45:01] to send a request to this
[45:03] API there's there's a bunch of other
[45:05] terms here that are going to make more
[45:06] sense if you're a SEO person um but now
[45:09] we receive as output a structured object
[45:12] with a ton of stuff right we have um the
[45:15] first result here it's like an organic
[45:16] one with some big URL a bunch of chips
[45:20] um I'm I have like a Knowledge Graph
[45:22] profile which is cool apparently it
[45:23] finds it says I'm a freelance writer um
[45:27] you know we have a bun bunch of data
[45:28] here bunch of data you know you can use
[45:30] this to get URLs of specific things and
[45:32] then with the URLs you can then feed
[45:34] that into scrapers um that do more like
[45:36] I talked about earlier maybe appify or
[45:38] maybe rap API maybe fir crawl so a lot
[45:42] of options here to like create your own
[45:43] very complex
[45:45] flows you can do other stuff as well um
[45:48] you grab a bunch of keyword data so
[45:50] maybe you wanted to find a keyword and
[45:53] maybe again it's Nicks or location you
[45:55] want let's do United States that'll
[45:57] probably be
[45:58] better language um I'm just not going to
[46:01] select an language and then I'll do a
[46:03] request so now it's going to find us um
[46:06] a bunch of search volume related stuff
[46:08] so I don't actually know how many people
[46:10] are searching for me in 2025 apparently
[46:13] 390 is this per month H wonder if it's
[46:16] per month per day that's interesting uh
[46:20] I don't really know why they break it
[46:21] down by like the month date yeah looks
[46:24] like it's 390 per month so to the 390
[46:26] people that are Googling me who are you
[46:28] and what do you want I'm just kidding um
[46:31] you can do things like you could find
[46:32] back links so you could find links um
[46:35] for I believe you feed in a website URL
[46:38] and then it finds back links to that
[46:40] website so this is you technically now
[46:41] scraping a bunch of other websites
[46:43] looking for links to the specific
[46:45] resource that you have that's kind of
[46:49] neat it looks like that found it
[46:51] basically immediately which is really
[46:52] really
[46:54] cool and it looks like they're referring
[46:56] top level links that are Dooms BG bgs
[46:59] would be interesting I wonder where
[47:00] that's coming from um there's a Content
[47:03] generation API playground so you could
[47:05] you know feed in some text and then have
[47:07] it generate other stuff but I think
[47:08] we're kind of getting away from um the
[47:10] actual thing that matters which is the
[47:11] scraping of the uh scraping of the
[47:13] websites so yeah lots of stuff lots of
[47:16] stuff for sure now that's all good um
[47:18] but let's actually turn this into an API
[47:20] call if we head over to the API of do
[47:22] data for SEO so in my case docs. datafor
[47:24] seo.com V3 _ page SL contentor parsing
[47:30] live that's what I'm I'm curious about
[47:32] you'll see that we have a post request
[47:33] that we need to send to this URL um well
[47:36] I have a curl just like this which I can
[47:39] feed into um an API request that's what
[47:42] I'm going to do so I'm going to go back
[47:43] over here and I'm just going to import
[47:45] this curl
[47:47] import and it's going to go through and
[47:49] it's basically going to um parse out all
[47:52] these fields that I'm interested in with
[47:53] the URL which I'll go htps
[47:57] left click. AI um and then we have sort
[48:00] of like a gacha here that a lot of
[48:01] people don't understand this is the um
[48:03] authorization the authorization is a
[48:05] little bit different from most of the
[48:06] easy authorizations we've had so far we
[48:07] actually have to convert it um we have
[48:08] to go one one more step basically to
[48:10] make this work if I check out the let's
[48:15] see um
[48:17] authorization
[48:19] here what we need is we need to um get
[48:23] the login and then the P so this is your
[48:26] username and then your password then we
[48:28] have to Hash it or not hash it but we
[48:29] have to convert it into something called
[48:30] base 64 um this is just how they do
[48:33] their API key stuff I guess it's kind of
[48:35] annoying but it's just part and parcel
[48:37] of working with some apis you're just
[48:38] not always going to have it available to
[48:40] you really easily so I'm just going to
[48:42] go back to data for SEO and then I'm
[48:45] going to grab my credentials okay so
[48:47] what we need to do is we need to base 64
[48:50] encode the username and the password um
[48:52] I'm just going to leave that at what
[48:53] I've done is I've actually gone through
[48:54] and done it in this edit Fields node um
[48:57] basically what you need to do is you
[48:58] need to have your username or your login
[49:02] so maybe this is me searching Nix or
[49:04] have Reddit uh Nick left click. so that
[49:07] might be my username and then my
[49:09] password is What's called the API
[49:10] password you can find that really
[49:11] quickly and easily just by going over
[49:13] here to API access and then API password
[49:16] if you just signed up it'll be visible
[49:17] right here if it's been more than 24
[49:18] hours you actually have to send it by
[49:19] email but anyway so that's um that's
[49:21] where i' get the API password from uh
[49:24] and then once you feed it in over here
[49:26] where you're going to want to do is
[49:27] you're going to want to base 64 encode
[49:28] it like this they just require you to
[49:31] use these creds um or to operate with
[49:35] these creds as base 64 encoded versions
[49:36] Bas 64 is just a way to like translate
[49:38] into a slightly different number format
[49:40] so once you have that you would just
[49:41] feed in the variable right over
[49:43] here Ju Just as follows and then you can
[49:46] make a request to their API and receive
[49:47] data so uh it looks like I was doing
[49:50] their content parsing live you know what
[49:53] I wanted to do is I just wanted to
[49:55] call their endpoint which I think was
[49:58] their like instant
[50:02] Pages this one right over here so it's
[50:04] just V3 uh once you've sorted this out
[50:06] by the way the AP gets like
[50:07] extraordinarily easy to manage you just
[50:09] need to like figure out the
[50:09] authentication from there on out all
[50:11] you're literally doing is just swapping
[50:12] out the requests so you know if you
[50:14] wanted to do instant Pages all I'm doing
[50:16] is pumping that in there I just sent a
[50:19] request and now I receive a bunch of
[50:20] links with different headings and and so
[50:22] on and so forth that's easy the seventh
[50:24] way to scrape websites and Ed end is
[50:25] using a third party application called
[50:27] crawl Bas they're known for their
[50:29] rotating proxies which allow you to send
[50:31] very high volume um API requests so um
[50:33] it's very proxy driven this is their
[50:35] website so it's a scraping platform
[50:37] similar to Rapid API um and uh you know
[50:40] appify they support many of the major
[50:42] websites here and um the reason why
[50:44] they're so good at this is just because
[50:45] they you know as I mentioned they rotate
[50:47] the hell out of these proxies so we're
[50:49] just going to sign up to Tri it free
[50:51] I'll use my business email
[50:54] here and then continue with Emil
[50:57] email we got to add a phone number
[51:00] obviously we're going to do less than a
[51:01] thousand I'm a CTO I don't want to
[51:04] what's the animal right is that an
[51:06] animal yes it's an animal good God beep
[51:10] boop uh we're going to head over to my
[51:14] Gmail and receive this
[51:18] now so we need to confirm my account
[51:20] just going to copy this link address
[51:22] that I can do this in one
[51:23] page awesome we should be good to log in
[51:26] so that's what's
[51:27] happening we need to select the animal
[51:29] again just doesn't it doesn't believe
[51:31] really just doesn't
[51:34] believe okay great so now we have a
[51:36] crawling API smart proxy thing if you
[51:39] guys want to run like uh I don't know
[51:41] use in apps that have a proxy field
[51:43] specifically I'm just going to keep
[51:44] things simple we're doing this in n8n so
[51:46] we're going to go crawl base API we have
[51:47] a th000 free crawls remaining very first
[51:50] thing we're going to want to do is just
[51:51] click start crawling now just to get up
[51:52] and running with the
[51:54] API um and as you see here the these
[51:56] guys have probably one of the simplest
[51:57] apis possible all API URLs start with
[52:00] the folling base part click and then all
[52:02] you need to do in order to make an API
[52:04] call is run the following sort of line
[52:06] so this is a curl request obviously
[52:08] we're in n8n and one of the value
[52:11] valuable parts of NN is we can just
[52:12] import a COR request so well I'm going
[52:14] to import it as you can see here we have
[52:17] a token field then we just have the URL
[52:19] field of the place we want to crawl so
[52:21] I'm going to do left click. for now um I
[52:24] don't know if this token field was
[52:26] actually my real token I don't believe
[52:27] so maybe we'll give it a try maybe it's
[52:29] like a test token or
[52:31] something so I'm now running this and it
[52:34] looks like we just received a bunch of
[52:36] very spooky data I don't like the spooky
[52:38] data no spooky data for
[52:41] us um sometimes spooky data like
[52:47] this H this seems kind of weird to me
[52:49] actually just give me one second to make
[52:51] sure that's right we are receiving a
[52:53] data parameter back which is nice but
[52:56] yeah something about this is a little
[52:57] bit spooky um was it a get request or
[53:00] was it a post request no I guess it's a
[53:01] get request
[53:03] strange very very
[53:06] strange okay anyway they give you two
[53:09] types of tokens here um this is why I'm
[53:11] talking about it to begin with I'm also
[53:12] because I just used it before for a
[53:13] couple of applications and I found it
[53:15] very easy they give you a normal token
[53:16] and they give you a JavaScript um token
[53:19] as well so the reason why that's
[53:20] valuable is because if you're scraping
[53:22] one of these websites I talked about
[53:23] before where when you send a simple HTTP
[53:25] request nothing pops up like this is the
[53:27] this is the purpose of this you actually
[53:28] feed in a JavaScript token um when you
[53:31] use the JavaScript token it'll
[53:32] automatically launch a browser instance
[53:34] inside of um craw base for you so
[53:37] instead of you getting just like that
[53:39] empty thing back that I mentioned I'm
[53:40] you're actually going to get uh you're
[53:42] going to get like a JavaScript version
[53:44] of the website where somebody went on
[53:46] the website it loaded really briefly and
[53:48] then they grabbed the code
[53:50] afterwards so yeah we have some some API
[53:52] call stuff over here um this one's just
[53:54] using Amazon this is pretty interesting
[53:56] so I might actually give that a a go
[53:58] just to give you guys an example of said
[54:00] Amazon
[54:02] scrape uh let's just go
[54:09] www.amazon.com oh right Amazon might be
[54:11] JavaScript actually so maybe we give
[54:13] that a go no it looks like we um we got
[54:15] the data from from Amazon which is
[54:16] pretty cool if you feed that into the
[54:18] markdown converter like we had before
[54:20] it's going to feed in the HTML here pump
[54:23] it into a data
[54:24] key we've now converted this
[54:28] into uh this is very long let's go
[54:30] tabular we've now converted this into
[54:32] markdown which is cool and this is
[54:34] pretty long right obviously has all of
[54:36] the images and has all of the
[54:38] information on the site which is cool
[54:39] and then we can feed it into open AI
[54:41] like I did
[54:42] before where I message a model and I'm
[54:46] just going to copy um from my uh
[54:48] previous application here to make my
[54:49] life a little bit
[54:50] easier where the heck are you
[54:56] and then we're just going to feed in the
[54:58] code here and then because I didn't feed
[55:02] in this we should now run this we're
[55:06] going to grab data from the site and
[55:08] we're going to try and I mean you know
[55:09] we kind of all know what Amazon is and
[55:11] what it does right so I'm not expecting
[55:13] expecting anything spectacular but it's
[55:15] still going to go it's going to give me
[55:16] all of the text on this Amazon page and
[55:18] then I'm going to get a bunch of list of
[55:19] links basically absolute URLs ideally
[55:22] should play some Jeopardy
[55:25] music or going be able to play um Star
[55:28] Wars music that'd be kind of cool okay
[55:31] we now have a schema with all of the
[55:33] links on the page which is pretty cool
[55:34] we have the plain text website copy we
[55:37] have a on line summary uh you know plain
[55:39] text website copy is a lot longer than
[55:41] this obviously it's just shortening and
[55:42] truncating it for us but yeah very quick
[55:44] and easy way to use crawl base for this
[55:46] now the value in crawl base is not
[55:48] necessarily just to send them to static
[55:49] websites like I talked about it's to use
[55:51] like highly scalable scraping where
[55:55] you're scraping any applications
[55:58] consistently um as you see here the
[56:00] average API response time is between 4
[56:02] to 10 seconds so you you will receive
[56:03] results back pretty quick if you wanted
[56:05] to just send one request or 20 requests
[56:08] every second think about it like 20
[56:10] requests a second times 60 seconds a
[56:12] minute is 1,200 requests times 60
[56:14] minutes and an hour 72,000 requests
[56:16] right um sorry just jumping around the
[56:18] place here you can send 72,000 requests
[56:20] basically an hour which is crazy um and
[56:23] you can do so as quickly and as easily
[56:24] as just like adding an API call like
[56:26] and then it'll automatically distinguish
[56:28] between like a a plain text thing or a
[56:30] JavaScript thing okay the eighth way to
[56:31] scrape data in nadn specifically website
[56:34] resources is octop parse octoparse is
[56:37] very similar to some of the other
[56:38] services that we've talked about um it
[56:40] is a web scraping tool that actually
[56:41] gives you quote unquote free web
[56:43] crawlers and I'm just a fan of their ux
[56:44] I think it's very clean I think the way
[56:46] that they have their signup flow and
[56:47] stuff's really easy so if you made it to
[56:49] this part of the uh tutorial and you
[56:50] have yet to sign up to one of these
[56:52] Services give octoparse um give
[56:54] octoparse your thoughts
[56:56] let's double
[56:58] check that I haven't actually created an
[57:00] account using this no I haven't
[57:01] fantastic so I should be able to jump
[57:03] through and show you guys what this
[57:05] looks like we have a verification code
[57:07] I'm going to paste in if you're not
[57:09] familiar with jumping around and stuff
[57:11] like this um or if you're wondering how
[57:13] I'm jumping around I'm just using a
[57:14] bunch of website
[57:19] hotkeys okay great account is now ready
[57:22] so we can start a free premium trial if
[57:24] you want I think you're going to have to
[57:25] add a card um I don't know if I have
[57:26] enough credits to actually do anything
[57:29] but if I'm not then I'll start that
[57:30] trial in a second what you're going to
[57:32] have to do in order to make this work is
[57:33] you're going to want to have to download
[57:34] you're going to want to download the
[57:36] octoparse desktop app so let's give it a
[57:39] quick and easy go just going to drag
[57:42] this puppy um if you are using something
[57:45] that is not Mac OS you will not have
[57:47] this strange drag and drop feature here
[57:50] once that is done you will have octo
[57:51] parse accessible just open that up yes I
[57:54] want to open this thank you
[57:57] and the cool thing about
[57:59] octoparse um kind of relative to what
[58:03] else you know like the other scraping
[58:05] applications I talked about is this is
[58:07] just running in a desktop app um like
[58:09] kind of in in your
[58:10] computer so like it's cool because it's
[58:13] just easy to get up and running with um
[58:14] and it's also local as opposed to a lot
[58:16] of these other ones which are not so I'm
[58:19] going to Auto log in on my desktop app
[58:21] remember my password beautiful the
[58:22] simplest and easiest way to scrape a s a
[58:24] service is just to pump in the the URL
[58:26] here then click Start and basically
[58:28] what'll happen is um it'll actually
[58:30] launch like an instance of your browser
[58:31] here with this little tool that allow
[58:33] you similarly the web scraping Chrome
[58:35] extension select the elements on the
[58:36] page you want scraped so I don't know
[58:39] maybe I want these logos scraped the
[58:41] second that I tapped one you'll see it
[58:42] automatically found six similar elements
[58:45] so now I'm actually like scraping all of
[58:47] this stuff okay now we have access to
[58:50] this sort of drag and drop or um
[58:52] selector thing similar to what we had
[58:55] before if you click on one of these
[58:56] you'll see it allow you to select all
[58:58] similar
[58:59] Elements which is pretty sweet and then
[59:02] um you can also do things like click
[59:04] elements and so on and so forth extract
[59:06] the text Data here um you can also tie
[59:09] that to other things right so as you see
[59:12] I'm now mapping each of these very
[59:13] similarly to how I was doing before
[59:15] between the first field which is the
[59:17] title of the product and then the second
[59:18] field which is like the field to uh so
[59:21] that's pretty sweet we could do the same
[59:22] thing with a number of things you could
[59:23] extract like the headings and then the
[59:25] values and so on and so on and so forth
[59:27] but I'll kind of leave it there um so
[59:29] once you're done selecting all the
[59:30] elements that you want all you do is you
[59:31] click run and you have a choice between
[59:33] running it on your device versus running
[59:35] it on the cloud so um on the cloud is
[59:38] API supported that's how you're going to
[59:39] get stuff in NM but I just want you guys
[59:40] to know that you can also just run it
[59:41] here you could run it here load up the
[59:43] URL scrape all the things that you want
[59:45] on the specific page you're feeding in
[59:46] and then you can be done with it so I
[59:48] just selected run in the cloud it's now
[59:49] going to open up said Cloud instances as
[59:51] we could see we have this little field
[59:53] where it's running and extracting the
[59:54] data we're now done so I can export this
[59:58] data locally um but I could also do a
[60:00] lot of other stuff which we'll show you
[60:01] in a second so um you can dump this
[60:03] automatically to Google Sheets you could
[60:05] do zapier to connect to Google Sheets do
[60:07] like some sort of web Hook connection
[60:08] export to cloud storage uh similar stuff
[60:10] to the the web scraping Chrome extension
[60:13] um but for now let's just export this as
[60:16] Json give ourselves a little ad Json
[60:19] file here thank
[60:21] you and yeah now we have it locally now
[60:23] in order to connect with the octop par
[60:24] CPI what you're going to have to do is
[60:26] first you get up to request an access
[60:28] token the way that you do this is you
[60:29] send a post request to this URL here and
[60:33] the way that you format it is you need
[60:34] to send your username your password and
[60:38] then have the grantor type as password
[60:40] okay now password obviously just put in
[60:42] whatever your password is don't store it
[60:44] in PL text like I'm doing um with my
[60:45] hypothetical password put it somewhere
[60:47] else and then grab that data and then
[60:49] use it um but the the output of this is
[60:51] we have this big long access token
[60:52] variable which is great after that if I
[60:55] just go back to their API here um once
[60:57] we're here we can actually extract the
[60:58] data that we need so basically the thing
[61:01] that you're going to want is you're
[61:02] going to want um get data by Offset you
[61:04] can also use get non-exported data which
[61:06] is interesting so I think this just like
[61:07] dumps all of the data as not exported um
[61:10] and then sends that over to you I
[61:12] believe but anyway you could also get
[61:14] the data by offset so if I go a get
[61:17] request to open api. octop course.com SL
[61:20] all and then I just send a header with
[61:21] the URL parameter this is a get request
[61:25] uh we're going to send a header with the
[61:27] token so
[61:29] authorization
[61:30] Bearer and then feed in the access token
[61:33] here just make sure that this is just
[61:35] one space no it's
[61:37] two if I feed this in um it's saying
[61:41] that it's a bad request let me just
[61:42] triple check why I think we need three
[61:46] Fields yeah I think we need three Fields
[61:49] actually my bad um we need uh this get
[61:53] request then we need the authorization
[61:56] header like I talked about then we need
[61:58] three Fields task ID yeah right
[62:00] obviously we need to feed in the task ID
[62:01] so you need task ID offset or size so um
[62:05] we'll feed this in as query parameters
[62:07] here so send query parameters the first
[62:10] value was task ID second one was
[62:14] offset and uh offset is no Capital the
[62:18] third was size offset's going to be zero
[62:20] size going to be I don't know let's just
[62:22] do 1,000 and what we need now is we need
[62:24] the task ID of the specific run that we
[62:26] just finished oh in order to get the
[62:27] task list you head over to task list top
[62:29] right hand corner here task ID API so we
[62:32] now have access to this so if we go back
[62:35] to our NN instance we could feed that in
[62:37] here by test the step you'll see that we
[62:40] now have all the data that we just asked
[62:42] for earlier so a variety of ways to do
[62:44] this um in practice like octop par
[62:45] allows you to schedule runs you could
[62:47] schedule them um using their you know
[62:49] whatever it is uh uh like cloud service
[62:52] you could use it to scrape I don't know
[62:54] Twitter uh they have a variety of like
[62:56] other scrapers um that you can check out
[62:58] just heading over
[63:00] to this new here uh if we just go sorry
[63:03] go down to templates um there's a
[63:05] variety of other ways to scrape Google
[63:06] job scraper glass door scraper super
[63:08] Pages scraper you could schedule these
[63:10] right and then what you can do in na is
[63:11] you can just query it once a day grab
[63:13] all the data like I showed you how to do
[63:14] a moment ago dump that into some sheet
[63:17] octoparse is pretty cool it's like more
[63:18] of like an industrial Enterprise level
[63:20] application um to be honest so there
[63:23] might be some gotas if you're not super
[63:24] familiar with working with like desktop
[63:27] apps and stuff but I I like the idea
[63:28] that you can also just scrape locally
[63:30] which is pretty sweet and the last of
[63:31] our nine best ways to scrape websites in
[63:33] nadn is browserless now browserless runs
[63:36] a headless Chrome instance in the cloud
[63:38] this stuff is great for dynamic or heavy
[63:40] JavaScript websites if you've never used
[63:42] browser list before the cool part about
[63:45] browser list is allows you to actually
[63:46] bypass captas which is a big issue that
[63:48] a lot of people have um so I'm going to
[63:50] click try it for free I'm going to enter
[63:52] my email address over here verify I need
[63:56] to submit a code so let's head back over
[63:58] here thank you thank you thank you thank
[64:02] you we have a ton of free trial signups
[64:07] obviously I don't have a promo code or
[64:09] anything don't have a company name I'm
[64:11] just going to enter a password I'm using
[64:13] this to get past uh to avoid setting up
[64:15] a puppeteer and playright server sure
[64:17] I'm going to click complete we're now
[64:19] going to have a th credits inside of
[64:21] browser list which is pretty sweet um
[64:23] and we'll get a we'll get a full plan
[64:25] eventually we now have an API token so I
[64:27] can figure out how all of the stuff
[64:28] works here I'm just going to dive right
[64:29] into the API I can figure out how all of
[64:31] the API stuff works using their API docs
[64:33] which are fantastic by the way um and we
[64:35] don't want to do any of this stuff we
[64:37] just want to do HTTP apis brow list API
[64:39] index okay great so here's where we're
[64:41] at um if you want to send and receive a
[64:42] request what you need to do is uh you
[64:46] send a request to one of these endpoints
[64:48] content unblock download function PDF
[64:53] screenshot scrape or performance
[64:56] what we want for the purpose of this is
[64:58] just uh let's do content okay this is
[65:01] the request right over here so I'm just
[65:03] going to paste my API token up here copy
[65:05] this
[65:06] request feed it into nadn in the HTTP
[65:09] request module as per
[65:11] usual nice quick and easy I'm going to
[65:14] grab my API token and where it says your
[65:16] API token here I'm going to feed that in
[65:19] what I want as a website is just left
[65:21] click. a I'm going to run test step we
[65:24] are now quering the pi and in seconds we
[65:27] have access to the data same thing that
[65:30] we had before but now we're using a pass
[65:33] through and browser list is a great pass
[65:35] through um because you know uh they they
[65:38] allow you to scrape things that go far
[65:40] beyond the usual static site thing so
[65:44] like honestly and I'm just leaving this
[65:46] as a secret and sort of a little I guess
[65:48] Easter egg for people that have made it
[65:49] this far in the video like my go-to when
[65:51] scraping websites is as I mentioned do
[65:54] that HTTP request trans forg that works
[65:55] then do something like fir C.D but if
[65:57] that doesn't work I I do something like
[65:59] browserless that has all of this stuff
[66:01] built in um and I especially use browser
[66:05] list anytime that there's some sort of
[66:07] you know application where I'm just
[66:08] going to save this so I can make all my
[66:10] HTP requests really easy um especially
[66:12] when you know there's issues with captas
[66:14] and and accessing resources and stuff
[66:16] check this out not only can you do um
[66:19] the actual scrape you can do a
[66:21] screenshot of the page as well and
[66:22] because I've entered my token up here
[66:24] the requests that I'm going to setting
[66:25] up are as simple as importing the
[66:28] curl then clicking test step so
[66:31] straightforward we now have a file which
[66:34] is the screenshot now I used example
[66:36] domain there let's go left
[66:39] click. run this test now you can see
[66:42] we've actually like received a
[66:43] screenshot of the of the website
[66:47] view very sexy and my website's pretty
[66:49] long so keep in
[66:51] mind um and yeah you know obviously a
[66:53] lot you could do with that you can
[66:54] download the site you can turn the site
[66:56] into a PDF so um that's pretty neat I
[66:59] don't think I've actually used this one
[67:00] before but for the purposes of this
[67:02] demonstration why don't we give it a try
[67:04] we'll go over here import the curl paste
[67:07] it in voila the website I'm going to do
[67:10] is left click. aai going to test this
[67:14] step so now there servers doing a couple
[67:16] things like I'm scraping the site then
[67:18] converting it all into PDF format um
[67:20] probably screenshotting a bunch of stuff
[67:21] too if I view this now we now have my my
[67:24] file looks like it didn't capture all of
[67:26] the color aspects um that might just be
[67:29] difficult or whatever but I still have
[67:30] like a PDF of the site which is pretty
[67:31] neat um and yeah let you guys kind of
[67:34] screw around with this on your own but
[67:35] there are a variety of cool applications
[67:36] you can use browless for all right I
[67:39] hope you guys appreciated the nine best
[67:40] ways to scrape websites in nadn as you
[67:42] guys could see it's a combination of on
[67:45] platform scraping using the HTTP request
[67:47] module a lot of like API documentation
[67:49] stuff like that if you want to get good
[67:51] at this I'm releasing a master class on
[67:52] API stuff um uh as part of my next na
[67:56] tutorial video uh and then you know
[67:58] navigating this and then and then taking
[68:00] the data from these services and using
[68:01] them to do something that you want to do
[68:03] like artificial intelligence to give you
[68:04] a summary of the site or generate ice
[68:07] breakers for you or do something else um
[68:09] whether you're using a local application
[68:11] like octop parse or maybe the web
[68:13] scraping CH uh Chrome extension or using
[68:16] something like firra browserless appify
[68:19] rapid API and so on and so forth um you
[68:21] now have everything that you need in
[68:22] order to scrape static sites Dynamic
[68:24] sites super Js heavy websites and even
[68:26] social media websites like Tik Tok
[68:28] Twitter and Instagram thanks so much for
[68:29] making it to this point in the video if
[68:31] you have any suggestions for future
[68:32] content drop them down below more than
[68:34] happy to take your idea and run with it
[68:36] assuming it's something that I haven't
[68:37] done before and then if you guys could
[68:39] do me a really big solid like subscribe
[68:41] do all that fun YouTube stuff and I'll
[68:42] catch you on the next video thank you
[68:43] very much

=== PROMPTS ===

--- SUMMARY PROMPT ---

You are a professional content creator specializing in LinkedIn posts. Create an engaging LinkedIn post based on the following YouTube video content.
Focus on extracting and highlighting the key insights while maintaining a professional tone.

Video Title: The 9 Best Ways to Scrape Any Website in N8N
Channel: Nick Saraev
Duration: 1:08:45
Views: 1,230
Likes: 88
Description: Join Maker School & get your first automation customer ‚§µÔ∏è
https://www.skool.com/makerschool/about

Summary ‚§µÔ∏è
Here I show you the 9 best ways to scrape websites using N8N.

First, I'll cover the basics of handling static and dynamic sites (and the differences between them. I'll then show you how to manage direct HTTP requests in N8N before demoing tools like Firecrawl, RapidAPI, and Browserless. Plus, I'll explore options like the Web Scraper Chrome Extension and Apify for advanced tasks. Let me know if q's!

My software, tools, & deals (some give me kickbacks‚Äîthank you!)
üöÄ Instantly: https://link.nicksaraev.com/instantly-short
üìß Anymailfinder: https://link.nicksaraev.com/amf-short
ü§ñ Apify: https://console.apify.com/sign-up (30% off with code 30NICKSARAEV)
üßëüèΩ‚Äçüíª n8n: https://n8n.partnerlinks.io/h372ujv8cw80
üìà Rize: https://link.nicksaraev.com/rize-short (25% off with promo code NICK)

Follow me on other platforms üòà
üì∏ Instagram: https://www.instagram.com/nick_saraev
üïäÔ∏è Twitter/X: https://twitter.com/nicksaraev
ü§ô Blog: https://nicksaraev.com

Why watch?
If this is your first view‚Äîhi, I‚Äôm Nick! TLDR: I spent six years building automated businesses with Make.com (most notably 1SecondCopy, a content company that hit 7 figures). Today a lot of people talk about automation, but I‚Äôve noticed that very few have practical, real world success making money with it. So this channel is me chiming in and showing you what *real* systems that make *real* revenue look like.

Hopefully I can help you improve your business, and in doing so, the rest of your life üôè

Like, subscribe, and leave me a comment if you have a specific request! Thanks.

Chapters
00:00:040Introduction to Web Scraping in N8N
00:00:42 Understanding Static vs Dynamic Sites
00:02:44 First Method: HTTP Requests
00:05:44 Converting HTML to Markdown
00:09:51 Using OpenAI for Data Processing
00:10:17 Second Method: Firecrawl Service
00:11:25 Signing Up for Firecrawl
00:14:10 Third Method: Rapid API Marketplace
00:19:24 Fourth Method: Web Scraper Chrome Extension
00:25:36 Fifth Method: Appify for Scraping
00:34:30 Sixth Method: Data for SEO API
00:41:18 Seventh Method: Crawlbase for High Volume Requests
00:48:51 Eighth Method: Octoparse for Local Scraping
01:03:30 Ninth Method: Browserless for Dynamic Sites
01:08:20 Conclusion and Future Content Suggestions
Tags: automation, make.com, content creation, ai content, google sheets, chatgpt, wordpress, openai, blogging, integromat, make, automating, automate, gpt-4, gpt, openai api, indie hacking, small business, $20K/mo, cold email, make money online, make.com for people who want to make real money, make.com money, AIAA, ai automation, ai automation agency, ai automation guide, n8n scraping, n8n how to scrape, n8n scraper, n8n scrape, n8n scraping setup, n8n scrapers, n8n ai scraper, firecrawl

Transcription:
[00:00] hey Nick here today I'm going to show
[00:01] you the nine best ways to scrape any
[00:03] website in nadn you're going to be able
[00:05] to scrape static sites Dynamic sites
[00:07] JavaScript social media whatever the
[00:09] heck you want by the end of the video
[00:10] you'll know how to do it I scaled my
[00:12] automation agency to 72k a month using
[00:14] no code tools like make and nadn and
[00:16] scraping was a big part of that so this
[00:18] video is just going to give you all the
[00:18] sauce you're going to learn everything
[00:19] you need to scrape websites like that on
[00:21] your own let's get into it all right I'm
[00:23] going to jump into NN in a minute and
[00:24] actually build these alongside you and
[00:25] one other thing I'm going to do is I'm
[00:26] actually going to sign up to all the
[00:27] services in front of you walk you
[00:29] through the Authentication and the
[00:30] onboarding flows and get your API keys
[00:32] and stuff like that but just before I do
[00:34] want to explain very quickly the
[00:36] difference between a static site and
[00:38] then a dynamic site because if you don't
[00:40] know this um scraping just gets a lot
[00:42] harder and so we're just going to cover
[00:43] this in like 30 seconds and we can move
[00:45] on so basically um if this is you okay
[00:47] you're just this wonderful smiley person
[00:50] and you want to access a static website
[00:52] what you're doing is you're sending a
[00:54] request over basically to just like some
[00:56] document you know think about this as
[00:58] just like a piece of paper on a cupboard
[00:59] and there's a bunch of text on this
[01:01] piece of paper and what you do is you
[01:02] say hey can I have this piece of paper
[01:04] and then the piece of paper just comes
[01:06] back to you with all of the information
[01:07] inside of the piece of paper okay this
[01:09] is a very simplified version of what's
[01:11] actually going on but static sites are
[01:13] by far the easiest thing to scrape and
[01:15] so um this is where you know a lot of
[01:17] people think all websites are are at and
[01:20] then they kind of confuse it with this
[01:21] next step which is dynamic a dynamic
[01:23] site essentially is not like that at all
[01:26] basically what you're doing is you're
[01:27] sending a request to a piece of paper
[01:30] but the piece of paper has nothing on it
[01:32] okay what happens is this piece of paper
[01:34] then sends a request to some other dude
[01:36] which I guess in this case is just a
[01:37] server really who will then he has a
[01:40] trusty pen in his hand and he'll
[01:41] actually write all of the stuff on said
[01:44] piece of paper and then you'll get the
[01:46] piece of paper back so um there's
[01:48] actually that intermediate step okay
[01:50] where basically you are pinging some
[01:51] sort of uh you know domain name or
[01:53] whatever then that domain name shoots
[01:55] some code over forces a server to
[01:57] generate all of the contents on that
[01:58] domain and then you get it
[02:00] this is obviously kind of a two-step
[02:02] process and then this is a three-step
[02:04] process so if you just understand that
[02:08] um you know when you scrape a dynamic
[02:09] resource what you're really doing is
[02:10] you're sending a request to a page which
[02:12] sends a request back to another server
[02:14] which then fills your thing this element
[02:16] eliminates 99% of the confusion because
[02:18] most of the time like scraping issues
[02:19] are hey I just ping this page but I got
[02:21] nothing back you know the HTTP request
[02:23] or or or whatever I I sent you know it
[02:26] was fine but for some weird reason
[02:27] there's nothing on this page of of any
[02:29] substance well if that was the case for
[02:31] you it was most likely um because this
[02:32] was empty before you sent it over um
[02:35] whereas you know simple scraped static
[02:37] resources tend to just like give you
[02:38] what you want really quickly and it's
[02:39] really easy all right so hopefully we at
[02:42] least understand that there's that
[02:43] difference between static and dynamic
[02:44] sites here um I'm not going to go into
[02:46] it more than that we're actually just
[02:47] going to dive in with both feet start
[02:49] doing a little bit of scraping and then
[02:50] we'll kind of see where we land I find
[02:51] the best way to do this stuff is just by
[02:53] example and and you know being practical
[02:54] about it so the first major way to
[02:57] scrape websites in NN is using direct h
[02:59] HTTP requests this is also what I like
[03:02] to think of as the Magic in scraping
[03:05] itself what we're going to do is we're
[03:07] going to use a node called the HTTP
[03:08] request node to send a get request to
[03:10] the website we want this is going to
[03:12] work with static websites and non
[03:14] JavaScript resources so let me give you
[03:16] guys a website that I'm going to be
[03:17] scraping here this is my own site it's
[03:19] called left click I'm about to do a
[03:20] redesign um but this is a static
[03:22] resource I know this because I built the
[03:24] site you know I I did it in code and
[03:26] basically all this is is just a document
[03:27] somewhere on my or on on a server some
[03:29] more so what I'm going to want to do is
[03:31] and I'm just going to pretend that I
[03:32] haven't done any of this so uh we're
[03:34] just going to go HTTP
[03:37] request HTTP request node looks like
[03:39] this we have a method field a URL field
[03:42] authentication field query parameters
[03:44] headers body and then some options down
[03:46] here as well all I'm going to do is I'm
[03:48] just going to paste in the website that
[03:50] I want to visit okay then I'm just going
[03:52] to test the step it's that easy now the
[03:55] response from this on the right hand
[03:56] side see all this code over here this is
[03:59] what's called HTML if you're unfamiliar
[04:01] and HTML is basically just the like it's
[04:04] it's the code behind the site so if I
[04:07] were to zoom in over here you see where
[04:09] it says I don't know let's let's go to
[04:10] my website let's just find a little bit
[04:11] of little bit of texture build hands off
[04:14] growth systems okay if I just command F
[04:17] and paste this in we actually have that
[04:19] text buried somewhere in this big long
[04:21] HTML string right so all that this HTML
[04:25] is is this is the code that is sent to
[04:28] my browser which is Google Chrome in
[04:30] this case then my browser takes the code
[04:32] and it just renders it into this
[04:34] beautiful looking thing well beautiful
[04:35] is a subjective State I would say but
[04:38] this uh wonderful looking thing in front
[04:39] of us which is this website with like
[04:41] sizing and the tabs and the divs and all
[04:44] that fun stuff okay so basically what
[04:47] I'm trying to say is everything over
[04:48] here on the right hand side this is the
[04:50] entire site we can do anything we want
[04:52] with this information um and we can
[04:54] carry this information forward to to do
[04:56] any one of our any one of many flows so
[04:59] in my case right looking at a bunch of
[05:00] code isn't really very pretty so one big
[05:02] thing that you'll find in the vast
[05:03] majority of modern um scraping
[05:05] applications is you'll find that they'll
[05:06] take that HTML which we saw earlier and
[05:08] they'll convert it to something called
[05:09] markdown okay so um this is a markdown
[05:13] node we have a mode of HTML to markdown
[05:16] and all I'm going to do is I'm going to
[05:18] grab that data and I'm going stick it in
[05:20] the HTML section of the HTML to markdown
[05:22] converter what do you think is going to
[05:24] happen when I test this well we're going
[05:25] to convert this from this big long ugly
[05:27] super dense uh thing with a much these
[05:30] like greater than and less than symbols
[05:31] and we're just going to convert it into
[05:33] something a little bit shorter a little
[05:34] bit simpler This Is Us just manipulating
[05:36] file formats by the way and I find that
[05:38] manipulating file formats is a big part
[05:39] of what makes a good scraper a good
[05:41] scraper so now we have something in
[05:43] what's called markdown format what's the
[05:45] value there well markdown format does
[05:46] two things for us one it's much easier
[05:48] to parse parse just means we can extract
[05:50] different sections of the text we want
[05:52] we can structure it in some sort of
[05:54] other data format um and then in my case
[05:56] because I love using AI for everything
[05:58] it's much easier and shorter for us to
[06:00] use with AI so I'm going to give you
[06:01] guys a very simple example where we take
[06:03] this text from the static resource and
[06:05] then we just use um AI to tell us
[06:08] something about it so I'll go down to
[06:09] open Ai and then what I'm going to do is
[06:11] I'll do the message a model just have to
[06:14] connect my credential here I'm assuming
[06:16] that you've already connected a
[06:16] credential if not you're going to have
[06:18] to go to opena website when you do the
[06:20] connection um and grab your API key and
[06:22] paste it in there's some instructions
[06:24] that allow you to do so right over
[06:26] here uh what I'm going to do is I'm
[06:28] going to grab the G PT 40 Mini model
[06:31] that's just the uh I want to say most
[06:33] cost effective one as of the time of
[06:34] this recording and then what I'm going
[06:36] to do is I'm going to add three prompt
[06:37] I'm going to add a system prompt first
[06:39] I'll say you are a helpful intelligent
[06:42] web scraping
[06:43] assistant then I'm going to add a user
[06:46] prompt and I'll say your task is to take
[06:48] the
[06:49] raw
[06:51] markdown of a website and convert it
[06:54] into structured data use the following
[06:57] format and then I'm going to give it an
[06:59] example of what I want in what's called
[07:00] Json JavaScript object notation format
[07:02] so the very first thing I'm going to do
[07:04] is I'm going to have it just pull out
[07:05] all the links on the website because I
[07:07] find that that's a very common scraping
[07:08] application so I go links and then I'm
[07:10] just going to show an example of un
[07:13] array of we'll go absolute URLs this is
[07:19] very important that they're absolute
[07:20] URLs any thing that we're going to build
[07:22] after this is going to be making use of
[07:23] the absolute URLs not the relative URLs
[07:25] if you're unfamiliar with what that
[07:26] means if we Zoom way in here you see how
[07:28] there's this B uh SL left click log.png
[07:31] this is what's called a relative URL if
[07:33] you were to copy this and paste this
[07:35] into here this wouldn't actually do
[07:37] anything for us right uh what we what we
[07:39] want is we want this instead we want
[07:41] left click aka the root of the domain
[07:43] and then um left click _ logogram and
[07:46] that's how we get to the actual file
[07:47] asset so uh if we go back over
[07:50] here so if we go back over here um yeah
[07:53] you know basically we want a link array
[07:56] of absolute URLs and then I'm just going
[07:58] to want main text website copy this is
[08:02] going to be a long string containing all
[08:05] of the website
[08:06] copy containing just the text of the
[08:09] site no
[08:11] formatting and then why don't we do one
[08:13] more thing why don't we have like a
[08:16] summarized or summary let's do one line
[08:19] summary just to show you guys you can
[08:21] also use AI to do other cool stuff you
[08:23] could take this oneline summary and feed
[08:25] it into some big sequence you could have
[08:26] ai write an icebreaker for an email you
[08:29] could do a things with this but I'll say
[08:30] on line summary um brief summarization
[08:34] of what the site is and
[08:37] how what the site is about let's do that
[08:42] okay so this is our example I'm going to
[08:45] say your website URL
[08:48] is left click URL for the relative to
[08:53] Absolute
[08:54] conversions is left click. and then the
[08:56] final thing is I'm going to add one more
[08:58] user prompt I'm just going to draw drag
[08:59] all of that markdown data in here then
[09:01] I'm going to click output content as
[09:03] Json I'm going to test the step I'm
[09:06] going to take a sip of my coffee while
[09:07] this puppy processes and we now have our
[09:09] output on the right hand side if we go
[09:11] to schema view what you can see is we've
[09:14] now
[09:15] generated basically an array of links on
[09:17] the rightand side which contains every
[09:19] link on this website very cool looks
[09:21] like the vast majority of these are type
[09:23] form links for some reason don't really
[09:24] know what's about that oh right it's
[09:26] because that's basically the only thing
[09:27] on my website it's just a one P with a
[09:29] bunch of different links to time for
[09:31] that's funny um anyway you could
[09:33] obviously just get it to Output one link
[09:35] or tell it like make sure all the links
[09:36] are unique or something um and then we
[09:38] have a big chunk of plain text website
[09:40] copy right then we have a oneline
[09:41] summary of the site so this is a very
[09:44] simple example of scraping we're
[09:46] scraping a static resource obviously but
[09:48] when I build scrapers for clients or for
[09:50] my own business this is always my first
[09:52] pass I will always just make a basic
[09:54] HTTP request to the resource that I'm
[09:56] looking at because if I can make that
[09:58] http request work whether it's a get
[10:00] request or whatever the the the rest of
[10:03] my life building scraper building the
[10:05] scraper is so easy I just take the data
[10:08] I process it usually using AI or some
[10:09] very cheap Tok cheap per token thing and
[10:12] then voila you know like we've basically
[10:14] built out a scraper in this case and
[10:15] it's only taken us what three nodes
[10:17] right so that's number one the second
[10:19] way to scrape websites in NN is using a
[10:21] third party service called fir crawl and
[10:22] making an HTTP request to it I'm using
[10:25] something called their extract endpoint
[10:27] but just to make a long story short fire
[10:29] craw is a very simple but High uh
[10:32] bandwidth service that turns websites
[10:35] into large language model ready data and
[10:37] basically you know how earlier we had to
[10:38] do HTTP request and then we had to
[10:40] convert all that stuff into markdown and
[10:41] then we had to you know manipulate that
[10:43] markdown what this does is it just does
[10:45] a lot of that stuff for you it'll
[10:46] actually allow you to go scrape and then
[10:48] it will automatically convert text into
[10:49] markdown for you um so that you can do
[10:52] whatever the heck you want they turn it
[10:53] into structure data using Ai and and so
[10:54] on and so on and so forth so if I were
[10:56] to do the same thing that I just did
[10:58] earlier
[10:59] with my own
[11:00] website then I were to you know run an
[11:02] example of this what it would go do is
[11:05] it would basically spin up a server for
[11:07] me and that would actually go and
[11:08] generate markdown of the same format um
[11:10] the only difference here is it's
[11:11] actually generated new lines between
[11:13] sections of text how beautiful um and
[11:15] then now you know we have basically the
[11:17] same thing you also get it in Json which
[11:19] is pretty cool um and you know you can
[11:20] slot this into any workflow this is
[11:22] basically like the simple and easy way
[11:23] of getting started um what we're going
[11:26] to be showing you today is the extract
[11:28] endpoint which allows you to extract
[11:30] data just using a natural language
[11:32] prompt which is pretty cool and from
[11:34] here we're going to be able to take any
[11:35] URL and just turn it into structure data
[11:37] but we're not actually going to have to
[11:37] know how to parse we're not going to
[11:38] have to know any code we're not going to
[11:39] have to know any of that stuff so let me
[11:41] actually run through the signup process
[11:42] with you guys go to fire. Dev here just
[11:47] going to open this up in an incognito to
[11:48] show you guys what this looks like all
[11:49] you do is you just go sign up I'm going
[11:53] to add a password we're then going to
[11:55] have to validate this one
[11:58] oh guess these guys are Mega
[12:01] secure so now I'm going to go back to my
[12:03] email
[12:04] address I'm going to count this up and
[12:09] we have a call back here I just need to
[12:10] paste this URL and put it in here that's
[12:12] just because I you know I'm doing this
[12:14] in an incognito tab normally when you do
[12:16] this you're not going to have that step
[12:17] okay great now we're inside a fir craw
[12:20] they give you I think something like 500
[12:22] um free credits something of that nature
[12:24] anyway so what I'm going to do is I'm
[12:26] going to go through and just like give
[12:28] give this extracting point just a basic
[12:30] natural language query so um let's go
[12:34] from the homepage at left
[12:37] click. I want to
[12:40] extract a oneline summary of the website
[12:45] let's do all of the text on the
[12:50] website all of the copy on the website
[12:53] in plain text let's do a oneline summary
[12:56] of the website a oneline Icebreaker I
[12:58] can use as the first line of of an of a
[13:01] cold email to the
[13:05] owner and uh the company name and a list
[13:11] of the services they provide let's do
[13:14] that this is a lot of requests we're
[13:17] asking it to do like seven or eight
[13:19] things but all I need to do in order to
[13:21] make this work is I click generate
[13:22] parameters it's going to basically now
[13:24] generate me a big object with a bunch of
[13:26] things so copy summary Icebreaker
[13:28] company name and now I can actually go
[13:30] and I can run this okay this is the URL
[13:33] it just parsed as well let's give it a
[13:35] run what it's doing now is it's scraping
[13:38] the pages using their high throughput
[13:39] server I just love this thing like I'm
[13:42] not sponsored by fire crawl or anything
[13:43] like that but I love their uh I don't
[13:45] know I just love the design I love this
[13:47] little like burning Ember or whatever
[13:49] the heck you want to call it I love how
[13:51] simple they've tried to make everything
[13:52] it's it's great honestly okay awesome
[13:55] and now you guys see we have basically a
[13:57] big array with a bunch of sub objects we
[14:00] have a summary like I asked for a list
[14:02] of services looks like we even have
[14:04] links to the specific places oh okay
[14:06] links from the resource we have an
[14:08] icebreaker and then we have the company
[14:10] name as well so we can do a lot with
[14:11] this right but right now this is just um
[14:13] this is just on on a website how do we
[14:14] actually bring this in naden uh well
[14:16] it's pretty simple as you see there's an
[14:17] integrate Now button you can either get
[14:19] code or you can use it in zap here
[14:21] basically what we're going to want to do
[14:22] is we're going to want to run a request
[14:24] to um their endpoint and then we're
[14:26] going to want to turn that into
[14:29] basically our HTTP request let me show
[14:30] you what that looks like I'm just going
[14:33] to do all of this stuff in curl so if we
[14:35] go to curl as you can see what we need
[14:38] to do is we need to format a request
[14:40] that looks something like this but we
[14:41] need to make sure it's using the extract
[14:43] endpoint okay so I'm going to go down to
[14:45] extract and then now I have this big
[14:47] long beautiful string what I'm going to
[14:50] do is I'm going to copy
[14:51] this I'm going to go back to my NN
[14:55] instance which is right over here and
[14:56] then what I need to do is just open up
[14:58] an HTTP request module and then click
[14:59] import curl just paste all the stuff
[15:01] inside now this is an example request
[15:03] but that's okay we can actually use that
[15:04] example request to very quickly and
[15:05] easily format our our real request okay
[15:08] so we're sending a bunch of headers this
[15:09] is the endpoint that we're calling api.
[15:11] fire. dv1 extract so basically what
[15:14] we're doing now is we're like we're
[15:15] sending a request to fir craw which will
[15:16] then send a request to the website right
[15:18] so kind of a kind of a middleman and
[15:20] then all I'm going to do so if I go back
[15:22] to my example we have an API key here
[15:24] which we're going to need so I'm going
[15:25] to go here and then paste in an API key
[15:28] so that's how that work works right
[15:29] authorization is going to be the name
[15:30] value is going to be bear with a capital
[15:32] b space and then the API key and then we
[15:35] also have a body that we need to uh
[15:37] adjust or edit and this body is where
[15:39] we're going to put the links that we
[15:40] want to actually have scraped with the
[15:41] extract end point so what I'm going to
[15:44] do is I'm going to delete most of these
[15:46] I'll go back to my left click. a just
[15:48] like this the prompts um because you
[15:50] know I was just using their playground
[15:51] before we're actually going to need to
[15:52] convert this into a request for my
[15:56] service so I'm just going to paste The
[15:57] Prompt in here voila
[15:59] and now we need to put together what's
[16:00] called a schema where we have the
[16:02] objects that we asked for so in my case
[16:04] we asked for copy right so I'm going to
[16:06] go Copy Type string then summary so
[16:09] we're going to go summary type string
[16:12] then Icebreaker it's going to be
[16:14] Icebreaker type string then guess what
[16:16] we have last but not least company name
[16:18] which is going to be type string we're
[16:21] also going to want to make these fields
[16:22] required like uh you know you can set it
[16:24] up so they're not actually required when
[16:25] you do a request a fire call I'm I'm
[16:27] going to make it so they're required so
[16:28] I'll go copy
[16:29] summary Icebreaker and then company name
[16:32] actually you know what maybe I'll leave
[16:34] company name as unrequired if you think
[16:36] about it logically maybe not all the
[16:38] websites we're going to be scraping
[16:39] using this service are going to have the
[16:40] company names visible on the website I
[16:42] don't know but maybe so maybe I'll
[16:44] actually leave that as off okay great so
[16:46] now we have the API request formatted
[16:48] correctly um all we need to do at this
[16:50] point is just click test step it looks
[16:52] like we're getting a Json breaking um
[16:55] error and I think that's because I have
[16:56] this last comma and I'm just going to
[16:58] check to see if there are any commas in
[16:59] Jason you can't actually have the last
[17:00] element in an array have a comma on it
[17:03] so I think that's okay let me test it
[17:05] again all right so as you can see we
[17:06] just received an ID we've got a success
[17:08] and then we have a URL Trace array which
[17:10] is empty um if you think about this
[17:12] logically we don't actually get all the
[17:14] data that we send immediately because we
[17:16] need fir crawl to whip up the scraper
[17:18] you know do things to the data we could
[17:20] be feeding in 50 URLs here right so
[17:22] instead of just having the data
[17:23] available to us right now immediately
[17:25] what we need to do is we need to
[17:25] basically wait a little while wait until
[17:27] it's done and we need to Ping it and the
[17:29] reason why they've given us this ID
[17:31] parameter so that we could do the
[17:32] pinging so the way that you do this is
[17:34] you'd have to send a second HTTP
[17:36] request using this
[17:38] structure so the good news is we could
[17:41] just copy this
[17:42] over and then we can
[17:45] add a second um HTTP request I don't
[17:49] know where that went but I guess I'm
[17:50] just going to create it over here I'm
[17:53] going to import the curl to this request
[17:55] just like
[17:56] that then keep in mind that we just need
[17:58] to add our API key again because the
[18:00] previous node had it but this one
[18:01] doesn't so just going to go over here
[18:03] I'm going to copy this
[18:05] puppy go back over here I'm going to
[18:08] paste this in now technically what this
[18:10] is called is this is called polling um
[18:12] polling uh is where you know you're
[18:15] you're you're attempting to request a
[18:17] resource that you don't know whether or
[18:18] not is ready and there's a fair amount
[18:20] of logic that I'd recommend like putting
[18:22] into a polling flow where like when you
[18:24] try it and if it doesn't work basically
[18:25] you wait a certain amount of time and
[18:26] you retry again for the purpose of this
[18:28] video I'm not going to put all that
[18:29] stuff inside but um what I'm going to do
[18:31] is just set up this request I'm going to
[18:33] give this puppy a test let's just feed
[18:36] that in on the back end we got to put
[18:37] the extract ID right right over here
[18:40] where it said extract ID then I'm just
[18:41] going to give this a test uh looks like
[18:44] I've issued a malformed request we just
[18:46] have to make sure that everything here
[18:48] is okay specify body let me just make
[18:50] sure there's nothing else in here it was
[18:51] a get request this is a get cool we're
[18:54] not going to send a body
[18:56] then awesome and now we have all of the
[18:58] data available to us automate your
[19:01] business in the copy field summary field
[19:02] left clicks an ad performance
[19:04] optimization agency Icebreaker hi Nick I
[19:06] came across left click I'm impressed by
[19:07] you help B2B Founders scale their
[19:08] business automation keep in mind I never
[19:11] gave it my name it went it found my name
[19:12] on the website uh and then company name
[19:14] left click so quick and easy way uh
[19:17] you're going to have access to this
[19:18] template obviously without my API key in
[19:19] it um and feel free to you know use fir
[19:21] craw go nuts check out their
[19:22] documentation build out as complex a
[19:24] scraping flow as need be the third way
[19:25] to scrape websites in nadn is using
[19:28] rapid API for those of you that are
[19:29] unfamiliar rapid API is basically a
[19:31] giant Marketplace of third party
[19:33] scrapers similar to appify which I'll
[19:34] cover in a moment but instead of looking
[19:36] for um you know building out your own
[19:38] scraper for a resource let's say you're
[19:40] wanting to scrape Instagram or something
[19:41] that's not a simple static site what you
[19:43] can do is you could just get a scraper
[19:44] that somebody's already developed that
[19:46] does specifically that using proxies and
[19:48] all that tough stuff that I tend to
[19:50] abstract away um and then you just
[19:52] request uh to Rapid API which
[19:55] automatically handles the API request to
[19:56] the other thing that they want and then
[19:57] they format it and send it all back to
[19:59] and then you know you have beautiful um
[20:01] data that you could use for basically
[20:02] anything so this is what rapid API looks
[20:04] like it's basically a big Marketplace I
[20:05] just pumped in a search for website over
[20:08] here and we see 2,97 results to give you
[20:11] guys some context you can do everything
[20:12] from you know scraping social data like
[20:15] emails phone numbers and stuff like that
[20:17] from a website you could ping the ah
[20:19] refs SEO API you could find uh I don't
[20:23] know like unofficial medium data that
[20:25] they don't necessarily allow people to
[20:26] do so this is just a quick and easy way
[20:28] to I guess do a first pass after you've
[20:31] run through fir crawl maybe that doesn't
[20:32] work after you've run through HTTP
[20:33] request that doesn't work um just do a
[20:35] first pass look for something that
[20:36] scrapes the exact resource you're
[20:37] looking for and then take it from there
[20:39] so obviously for the purpose of this I'm
[20:40] just going to use the website to scraper
[20:41] API which is sort of just like a wrapper
[20:44] around what we're doing right now in
[20:45] nadn um but this website scraper API
[20:48] allows you to scrape some more Dynamic
[20:49] data um now I'm not signed up to this so
[20:51] I'm going to have to go through the
[20:52] signup process and I'm going to show you
[20:53] guys what that looks like um but yeah
[20:55] we're going to we're going to run
[20:55] through an API request to Rapid API
[20:57] which is going to make this a lot easier
[21:00] just going to put in all of my
[21:02] information
[21:04] here and then I'm going to do the
[21:07] classic email verification okay just
[21:11] copy this puppy over no thank you rise I
[21:13] use a time management app called rise
[21:16] and every time I go on my Gmail I set my
[21:18] Gmail up as like a definitely do
[21:22] not uh do during your workday let's just
[21:25] call it personal projects they don't ask
[21:26] me all these questions my goal today is
[21:28] to browse available apis awesome so
[21:31] that's their onboarding I think we're
[21:32] going to have to like pay a little bit
[21:33] of money or something like that which
[21:34] I'll sort out in a moment um but the
[21:36] scraper that I want is I just want the
[21:37] website one right so I'm going to type
[21:38] website in
[21:39] here uh I'm going to look
[21:42] for wherever it was earlier website
[21:44] scraper API and now check this out what
[21:47] we have is we have the app which is the
[21:49] name of the specific API that we're
[21:51] requesting we have an x-raid api-key and
[21:55] this is the API key we're going to use
[21:56] to make the request then we have the
[21:58] request URL which is basically what
[22:00] we're pinging and what we can do here is
[22:01] we can feed in the parameters okay what
[22:03] website we want to we want to scrape and
[22:05] then we can actually just like give it
[22:06] give it a run so I'm going to have to
[22:08] subscribe to this in order to test it uh
[22:10] I'm just going to go to the um basic
[22:12] plan and I'm going to pay money per
[22:14] month that probably seems the simplest
[22:15] way to do so okay and I just ran through
[22:17] the payment let's actually head over
[22:18] here and let's just run a test using my
[22:20] website URL we're going to test this
[22:22] endpoint now and now this actually going
[22:24] to go through Rapid API it's going to
[22:25] spin up the server and then it's going
[22:27] to send it and what we see here is we
[22:29] have multiple fields that Rapid apis or
[22:31] this particular scraper gives us let me
[22:32] just make this easier for you all to see
[22:34] we have a text content field with all of
[22:35] the content of the website which is cool
[22:37] this is basically what I did earlier um
[22:39] but instead of me having to formulate
[22:40] this request try and parse it and try
[22:42] and use AI tokens what I did is I sent
[22:43] the request to uh rapid API and did it
[22:45] all for me then we also have an HTML
[22:47] content
[22:48] field I think we have one more here
[22:51] scroll all the way down to the bottom as
[22:53] you can see there is a ton of HTML um
[22:55] and then we also have a list of all of
[22:56] the images on the website which is very
[22:58] very cool and easily formatted again
[23:00] something that I tried to do manually
[23:01] using AI but now you know we have
[23:03] everything in that nice absolute URL
[23:04] format um and then if they find any
[23:06] social media links I don't believe um
[23:09] there were more than Twitter but um if
[23:11] they find anything that's at their
[23:12] Twitter Instagram whatever then we have
[23:14] the link right over here it looks like
[23:15] they even give you the scraping time and
[23:17] if they scrape emails or phone numbers
[23:18] um they'll be there as well so I mean
[23:20] rapid AP is obviously fantastic this is
[23:21] a high throughput sort of thing and why
[23:23] don't we actually run through what this
[23:25] would look like if we were to run a curl
[23:26] request you see how it's automatically
[23:29] just formatting it as curl well that
[23:30] just means we just jump back here
[23:32] connect this to my HTTP request module
[23:35] click import curl paste it in like this
[23:38] import and it's actually going to go
[23:39] through and it's going to automatically
[23:41] map all these fields for me right query
[23:43] parameter URL left click. beautiful um
[23:45] API key x-raid API host here's the host
[23:49] here's the name of the API key here's
[23:51] everything we need well I can actually
[23:52] just recreate this request now inside of
[23:54] NN as opposed to being on rapid API and
[23:57] then I have all the data accessible to
[23:58] me here how cool is that so we can do
[24:01] this for any any major website really um
[24:03] you know there are a lot of specific
[24:06] bespoke scrapers obviously which um I
[24:07] don't know if you wanted to scrape uh
[24:09] let's go back to Discovery if you wanted
[24:11] to scrape like Instagram or something
[24:14] you could scrape um Instagram uh you
[24:16] could do like Facebook scraping you
[24:18] could scrape these large giants that are
[24:20] quite difficult to do So Meta ad Library
[24:22] Facebook ad scraper and depending on the
[24:24] plan that you're at it might be more
[24:25] cost- effective for you to sign up to
[24:27] some sort of monthly recurring thing
[24:28] rather than just pay two cents every
[24:29] single time you make one of these
[24:30] requests you just kind of got to do that
[24:32] determination yourself right like if
[24:33] you're scraping uh I don't know 50 every
[24:36] day or 100 every day or something might
[24:38] be a dollar or two a day which is
[24:39] reasonable but maybe if you want to
[24:41] scrape like 5,000 doing it the way that
[24:42] I was doing it a moment ago might might
[24:43] be infusible the next way to scrape
[24:45] websites in nadn is using the web
[24:47] scraper Chrome extension and then tying
[24:49] that to a cloud service that delivers
[24:51] the data that you just created using
[24:53] their no code tool um in nicely bundled
[24:55] formats it's called Cloud sync as of the
[24:57] time of this recording I think they
[24:58] changed the name a couple of times but
[25:00] um that's where we're at here is the
[25:02] name of the service web scraper here is
[25:05] their website essentially what happens
[25:07] is you install a little Chrome plugin
[25:08] which I'll show you guys how to do then
[25:10] you select the fields that you want
[25:11] scraped in various data formats and then
[25:14] what you do is it handles JavaScript
[25:16] sites Dynamic sites all that fun stuff
[25:19] and then you can um export that
[25:22] data as a cloud run to then send back
[25:28] sorry big sneeze to then send back to
[25:31] some API or some service um and then
[25:33] automatically do parsing and stuff like
[25:34] that so very cool I'm going to show you
[25:36] guys what that looks like um this is
[25:37] sort of a more customized way to build
[25:38] the stuff but I've seen a lot of people
[25:40] do this with naden um so we're going to
[25:42] run through what it looks like so first
[25:43] thing I'm going to want to do is I'm
[25:45] going to want to let's just go Cloud
[25:47] login or sorry um start free 7-Day trial
[25:51] as you can see you know there's a free
[25:52] browser extension here if you wanted to
[25:54] do uh I don't know like highs scale
[25:56] stuff you'd choose probably their
[25:58] project um endpoint where we Sorry
[26:01] project plan where we have 5,000 URL
[26:03] credits we can run a bunch of tasks in
[26:05] parallel we could scrape Dynamic sites
[26:07] JavaScript sites we have a bunch of
[26:08] different export options then we can
[26:09] also just connect it directly to all of
[26:11] these um what I'm going to do just
[26:12] because I want this to kind of work as a
[26:14] first go is I'm just going to sign up to
[26:15] a free Tri here beautiful just created
[26:17] my account just go left click give it a
[26:21] phone number we'll go left click. a
[26:24] we're going to go I don't know academic
[26:26] records needed per month we'll go 0 to
[26:28] th000 length of the project uh I don't
[26:30] know let's go two to 3
[26:31] months okay great so now we can import
[26:34] and run our own site map or we can use a
[26:36] premade Community sit map um what I'm
[26:38] going to do is I'm just going to import
[26:39] this we're then going to get the Chrome
[26:41] extension web
[26:43] scraper let me add that extension and
[26:45] it's going to download it do all that
[26:47] fun stuff beautiful so now we have it
[26:50] right over here I'm just going to pin it
[26:52] to my browser to make my life easier go
[26:54] to left click. a open up this puppy now
[26:57] there's a bunch of like tutorials and
[26:58] how to use this stuff um that's not that
[26:59] big of a deal but basically the thing
[27:01] you need is you need to hold command
[27:03] plus option plus I to open up your
[27:04] developer tools and you'll just find it
[27:05] on the in my case the far right so
[27:07] command option I that'll open up Dev
[27:10] tools you see all the way on the right
[27:11] hand side here I have a couple other
[27:12] things like make and and cookie editor
[27:14] but all the way on the right hand side
[27:15] here we have this web scraper thing um
[27:18] so we got what you're going to want to
[27:19] do first you're going to want to create
[27:20] a site map for the resource that you're
[27:21] going to want to scrape I'm just going
[27:22] to call it left click and I just want to
[27:24] scrape left click. okay once we have our
[27:27] sitemap if I just give a quick little
[27:28] click I can then add a new selector and
[27:30] the really cool thing about this web
[27:32] scraper is um if I just zoom out a
[27:34] little bit here uh what you can do is
[27:35] you can you can select the elements on
[27:37] the website that you want scraped so for
[27:39] instance it's a very quick and easy way
[27:40] to do this if you think about it is like
[27:42] just to show you guys an example
[27:43] structure data is uh sort of like an
[27:45] e-commerce application let's say you
[27:46] have like the title of a product and you
[27:47] have like I don't know the the
[27:49] description of a product so on my
[27:50] website really quick and easy way to do
[27:51] this is let's just call this
[27:53] products and it's a type text what I'm
[27:56] going to do is I'm going to click select
[27:58] then I'll just click on this I'll click
[28:01] on this as well and as you see it'll
[28:03] automatically find all of the headings
[28:06] that I'm looking for so that's products
[28:09] we are going to then click data I'm
[28:10] going to click done selecting data
[28:12] preview as you can see it only selected
[28:14] one of them the very first so what we're
[28:15] going to want to do is go multiple and
[28:17] now if I data preview we get all of the
[28:19] headings which is very cool so now we
[28:21] have a basically like a list of headings
[28:23] um from here I'm going to save this
[28:25] selector I'm add a new one let's go
[28:27] product
[28:28] descriptions and then going to select
[28:31] this this it'll select all of them I'll
[28:34] go multiple data preview just to make
[28:36] sure that it looks good I'm getting no
[28:37] data extracted here oh sorry I didn't
[28:39] actually select the um didn't actually
[28:41] finish it now we're getting product
[28:42] descriptions that's pretty cool um this
[28:44] is me doing this sort of like one at a
[28:46] time you can also um group The selectors
[28:49] there you go it's actually um offered to
[28:51] group it for me so we can uh group this
[28:54] into one object with products and then
[28:56] product descriptions so it's automatic
[28:58] group it now we have wrapper for
[28:59] products and products descriptions then
[29:01] we have products and product
[29:02] descriptions buried underneath we could
[29:04] go as far as we want with this but
[29:05] basically what I'm what I'm trying to
[29:06] show you guys is very simple and easy
[29:08] just drag your mouse over the specific
[29:10] thing you want if you select more than
[29:12] one it'll automatically find all of them
[29:13] on the website which is really cool okay
[29:15] great once we have this um what I can do
[29:17] is I can actually go export sitemap so
[29:20] now I have all of the code on the
[29:21] website that actually goes and finds it
[29:23] for me then I can paste this in here
[29:26] I'll just call this left click scraper
[29:28] and I'm going to import this to my cloud
[29:31] scraper uh I think I'm running into oh
[29:34] sorry I don't think we can do a space
[29:36] there my bad just call it left click and
[29:38] now what we can do is we can actually
[29:39] just like run a server instance that
[29:41] goes out and then scrapes this for us
[29:42] okay so I'm going to click scrape it
[29:45] looks like I need to verify my email so
[29:47] just make sure you do that before you
[29:48] try and get ahead of yourself like I
[29:52] was okay looks like we just verified the
[29:54] email let's head back over here refresh
[29:57] then scrape we've now scheduled a
[29:59] scraping job for this sitemap scheduling
[30:02] you know in their lingo just means that
[30:03] it's now part of their big long queue of
[30:05] thousands of other things that they're
[30:06] probably scraping through their server
[30:07] and that's fine okay I just gave this a
[30:09] refresh and as we see we have now
[30:10] finished said scraping job we have all
[30:12] of the data available to us using their
[30:14] UI but now that we've gone through this
[30:16] process of you know building out this
[30:18] this thing um how do we actually take
[30:20] that and then use it in our nadn flows
[30:22] so variety of ways um if you wanted to
[30:24] connect this let's say to specific
[30:25] service like Dropbox um Google
[30:28] you know dump anow or something Google
[30:30] Drive I'd recommend just doing it
[30:31] directly through their integration it's
[30:32] just a lot easier to get the data there
[30:34] and then you can just connect it to n
[30:36] and watch the data as it comes in or
[30:37] something you can also use the web
[30:39] scraper API uh this is pretty neat
[30:42] because you can you know that's what
[30:43] we're going to end up using it was
[30:45] pretty neat because you can uh like
[30:47] schedule jobs you can send jobs you can
[30:48] do basically everything just through the
[30:50] NN interface and then we can just
[30:52] retrieve the data afterwards which is
[30:53] pretty neat um this is basically what
[30:55] you end up getting you end up with
[30:57] scraping job ID status sitemap all this
[30:59] fun stuff and then we can set like a web
[31:02] hook URL where we we receive the request
[31:05] so um let me check we need a scraping
[31:07] for testing you need a scraping job that
[31:09] has already been finished I think our
[31:10] scraping job has already been finished
[31:12] I'm just going to go htps uh back to my
[31:15] n8n flow I'm actually going to build an
[31:18] n8n web hook give that a click I'm not
[31:22] going to have any authentication let me
[31:24] just turn all this off basically what we
[31:26] want is we we want to use this as our
[31:27] test
[31:29] event we're going to go back to the
[31:32] API paste this in
[31:35] save and I'm just going to want to give
[31:36] it a test endpoint here so
[31:39] test looks like um the push notification
[31:42] was failed the reason why is because
[31:44] it's saying this web Hook is not
[31:45] registered for post request did you mean
[31:47] to make a get request beautiful thank
[31:48] you naden we absolutely did so I'm going
[31:50] to stop listening change your HTTP HTTP
[31:53] method here to post there's basically
[31:54] two ways to call a website and this is
[31:55] one of them I'm going to listen for test
[31:57] events go back here and then
[31:59] retest awesome looks like we've now
[32:01] triggered the beginning of our workflow
[32:03] using this data let's see what sort of
[32:05] information was in
[32:06] it okay great we have the scraping job
[32:09] ID the status execution
[32:11] mode okay great so we basically have
[32:13] everything we need now to set up a flow
[32:16] where we can schedule something in this
[32:17] web scraper service that maybe monitors
[32:19] some I don't know list of e-commerce
[32:21] product or something every 12 hours and
[32:23] then we can set up a web hook in NN that
[32:24] will catch the notification get the
[32:27] update now we can do is we can ping um
[32:30] we can ping the web scraping API which
[32:32] I'll show you to set up in a second to
[32:34] request the data from that particular
[32:35] scraping run and from here we can take
[32:38] that data do whatever the heck we want
[32:39] with it but obviously let me show you an
[32:41] example of what the the actual data
[32:42] looks like so we just got the data from
[32:44] web hook let's set up an HTTP request to
[32:48] their API now where we basically get the
[32:50] ID of the thing and then we can call uh
[32:53] we can call that back so got my API
[32:55] token over here I'm going head over to
[32:56] their API documentation first okay and
[32:59] then what we want to do is download
[33:00] these scrape data in CSV format at least
[33:03] in my case I imagine most of you guys
[33:04] are going to add this to a spreadsheet
[33:05] or whatever um you can very easily do
[33:08] whatever you want there's also a Json
[33:09] format endpoint here um but let's just
[33:12] do CSV for Simplicity so I've already
[33:14] gone ahead and I've gotten the method
[33:16] which was a get request so I've added
[33:18] that up here the URL was this over here
[33:21] with the scraping job ID and then your
[33:23] API token there so what I've done is
[33:25] I've grabbed the API token and the
[33:27] scraping job ID I mean I hardcoded it in
[33:28] here just while I was doing the testing
[33:30] let's actually make this Dynamic now
[33:33] drag the scraping job ID right over here
[33:36] voila and then the API token if you guys
[33:38] remember back here on the API page you
[33:41] have your access to API token so just
[33:42] copy that
[33:43] over uh great and now if I run this I'm
[33:47] actually selecting that specific job
[33:49] then from here we have all the data that
[33:50] we just scraped as you can see there's
[33:51] like a uh the way that CSV Works
[33:54] actually let me just copy this over here
[33:55] I just wanted to give this to you guys
[33:56] as an example of a different data type
[33:58] but maybe some people here aren't really
[34:00] familiar with it basically the way that
[34:01] it works is if I just paste this into
[34:02] like a Google sheet you see how it looks
[34:04] like this what what you can do is if you
[34:05] just um split the text to columns you
[34:08] kind of see
[34:09] how kind of see how there's like these
[34:11] four pettings there's web scraper order
[34:13] web scraper startup products and product
[34:15] descriptions right I'm imagine scraping
[34:17] this for some lead genen applica sorry
[34:19] some some e-commerce application list of
[34:21] products here product descriptions maybe
[34:23] product prices maybe product whatever
[34:24] the heck you want um so yeah you can you
[34:27] can put in like a number of formats and
[34:29] I just wanted to give you guys an
[34:29] example what that looks like the next
[34:31] way to scrape websites in naden is using
[34:33] appify if you guys are no strangers to
[34:35] this channel you know that I do appify
[34:37] all the time and I talk about them all
[34:38] the time because I think that they're
[34:39] just a great service um they've now
[34:41] given me a 30% discount where anybody
[34:44] can use it for I was initially under the
[34:46] impression it was lifetime I think it's
[34:47] three months so you probably get 30% off
[34:48] your first three months just check the
[34:50] um description if you want that but Cent
[34:52] how appify is is it is a Marketplace
[34:54] very similar to Rapid API um although
[34:56] extraordinarily well Main ained and they
[34:58] also have a ton of guides set up to help
[35:00] you get you know up and running with
[35:02] scraping any sort of application so just
[35:05] as we had earlier we have Instagram
[35:06] scrapers we have Tik Tok scrapers we
[35:09] have email scrapers we have map scrapers
[35:12] Google Maps we could do I don't know
[35:15] Twitter
[35:15] scrapers uh medium scrapers right
[35:18] basically any any service out there that
[35:22] has this Dynamic aspect to it that's not
[35:24] a simple HTTP request you can make you
[35:25] could scrape it using ampify and then
[35:27] obviously you you have things too like
[35:28] just like basic website crawlers you can
[35:30] generate screenshots of sites I mean
[35:32] there's just there's so many things let
[35:33] me walk you guys through what it looks
[35:35] like now in my case I'm not actually
[35:37] going to sign up to appify because I
[35:38] have like 400 accounts but trust me when
[35:40] I say it is a very easy and simple
[35:41] process you go to app ay.com you go get
[35:45] started you put in your email and your
[35:47] password they'll give you $5 in free
[35:50] platform credit you don't need any
[35:51] credit card and you can just get up and
[35:53] running and start using this for
[35:54] yourself super easily then the second
[35:56] that you have all that you'll be Creed
[35:57] with this screen it is a console screen
[36:00] don't be concerned when you see this um
[36:03] you know this is super simple and and
[36:05] easy and and not a big deal this is one
[36:06] of my free accounts um so I just wanted
[36:08] to show you guys what you can do with a
[36:09] free account uh but from here what you
[36:12] do is you go to the store and as you can
[36:13] see I'm just dark mode all this is the
[36:15] same thing we were just looking at
[36:16] before and then um we're just going to
[36:17] run a test on the thing that we want to
[36:19] scrape okay so what I'm going to want to
[36:21] do is for the purposes of this I'm now
[36:23] going to do something different from
[36:25] what I was doing before like which was
[36:26] just left click over and over and over I
[36:27] think that kind of gets boring what I'm
[36:29] going to do is I'm going to scrape
[36:30] Instagram posts okay so what I'm going
[36:32] to do is I'm going to feed in a name
[36:34] nickf this is just my um
[36:38] Instagram uh which almost hit 10K in God
[36:41] like 15 days or something like that but
[36:42] I'm going to feed in my Instagram here
[36:44] and then I'm just going to grab like I
[36:45] don't know the last 10 posts okay save
[36:48] and start this is now going to run an
[36:49] actor actor is just their term for
[36:51] scraper which will go out it'll extract
[36:54] data from my Nick surve Instagram and as
[36:57] you can see will get a ton of fields
[36:58] caption owner full name owner Instagram
[37:00] URL comments count first comment likes
[37:02] count timestamp query tag we get
[37:05] everything from these guys which is
[37:06] really cool this might take you know 30
[37:08] 40 50 seconds we are spinning up a
[37:10] server in real time every time you do
[37:12] this as you see in bottom left hand
[37:13] corner there's a little memory tab which
[37:14] shows that we are legitimately running a
[37:16] server with one gigabyte of memory right
[37:18] now so generally my recommendation when
[37:19] you use appify is not to use it for
[37:21] oneoff requests like this feed in 5 to
[37:23] 10 15 20 Instagram Pages uh but you know
[37:26] I just got the back and voila we we have
[37:28] it it's right in front of us we have all
[37:29] of the data of that person's Instagram
[37:32] profile so you can see it's quite
[37:33] scalable in that way um so the question
[37:35] is obviously how do you get this in NN
[37:37] well appify has a really easy to use um
[37:39] API which I like
[37:40] doing all you have is if we wanted to
[37:44] get the uh let's see get data set items
[37:47] okay all I'm going to do is I'm just
[37:49] going to copy
[37:50] this go back here and then connect this
[37:53] to an HTTP request
[37:55] module as you could see we have this big
[37:58] long field here with my API appify API
[38:01] token and this specific data set that
[38:03] I'm looking for I'll show you how to get
[38:04] it dynamically but I just wanted to like
[38:06] allow you to see how to get data in
[38:07] naden really quickly now if we go to the
[38:10] schem of view we can see we legitimately
[38:12] we we already have all of the data that
[38:13] we we had from appify a second ago okay
[38:16] super easy and quick and simple to get
[38:17] up and running um we have the input URL
[38:19] field the ID field the type the short
[38:21] code caption now this is Instagram um
[38:25] every looks like we have some comments I
[38:27] don't have any style how do I create my
[38:29] man you just got to fake it till you
[38:31] make it I don't have any style either
[38:33] just some nerd in my mom's basement uh
[38:36] yeah so you you can scrape any resource
[38:38] you want here um obviously I was
[38:40] scraping an Instagram resource but like
[38:41] if you were scraping something else
[38:43] there'd be no change to this at all no
[38:45] change whatsoever now uh basically what
[38:47] we need in order to make this Dynamic
[38:49] basically make us able to run something
[38:51] in appify and then get it in NN so we
[38:54] need to set up an integration so just
[38:55] head over to this tab set up integration
[38:57] and then all you want to do is you just
[38:59] want to do web hook send an HTTP post
[39:02] web Hook when a specific actor event
[39:03] happens the actor event that we're going
[39:05] to want is basically when the run is
[39:07] succeeded the URL we're going to want to
[39:09] send this to if you think about it we
[39:11] just actually make another web hook
[39:13] request here web
[39:15] hook the URL we're going to want to send
[39:17] it to is going to be this test URL over
[39:21] here now I'm just going to delete all
[39:22] the header off stuff here because um it
[39:24] just uh complicates it especially for
[39:26] beginners um but we're going to copy
[39:28] this over head back over here paste in
[39:30] this URL and then let me see this is a
[39:32] post request I think I don't actually
[39:34] remember so we're going to have to
[39:35] double check I think it's a post
[39:37] request yeah and then what I'm going to
[39:39] do is I'm going to listen for a test
[39:40] event run the test web
[39:43] hook so we're listening we're making a
[39:45] get request okay so the fact that it
[39:47] hasn't connected yet probably tells me
[39:48] it's a post request so let's move over
[39:50] here move this down to post now let's
[39:52] listen to a test event let's run this
[39:55] puppy one more time so we just
[39:57] dispatched it and yeah the post request
[39:59] succeeded and what did we get we got
[40:00] tons of information we got a body with a
[40:02] user ID created at event data joke right
[40:05] looks like when you test something out
[40:06] they just send you a joke about how
[40:08] Chuck nurse can sketi a cow in two
[40:10] minutes have you ever heard of the word
[40:11] sketi before this moment I haven't I
[40:13] want to be known for my ability to sketi
[40:16] we'll go Instagram website
[40:19] scraper okay and now if we go back here
[40:22] right we're now listening for a test
[40:24] event so I'm going to listen for this
[40:26] test event I'm going to run the same
[40:27] scraper again maybe we'll make it five
[40:28] posts per profile just to make it a
[40:29] little
[40:30] faster and um once this is done what
[40:33] it's going to do is it's going to send a
[40:35] record of all the information we need to
[40:37] get the data over to Ann we're going to
[40:40] catch that information and then we're
[40:42] going to use it to query the the the
[40:44] database basically that it created for
[40:45] that particular Instagram run which will
[40:48] then enable us to do whatever the heck
[40:49] we want with it so it's now starting to
[40:51] crawl as we see here we had five
[40:53] requests so it should be able to do this
[40:54] in like the next 5 seconds or so okay
[40:56] and once that's done we now have an
[40:57] actor succeeded event um and then we
[41:00] have uh let me see the data that we want
[41:04] would be the default data set ID down
[41:06] over here so if we just go to that next
[41:08] HTTP request node what I can do is I can
[41:11] feed that in as a variable right
[41:14] here let going to a default data set
[41:18] ID drag that in between these two little
[41:23] lines and now we can test that step with
[41:25] actual live data now we have everything
[41:26] that we need
[41:27] so I don't know maybe now you want to
[41:29] feed this into Ai and you want to have
[41:30] ai tell you something about the last
[41:32] five posts tell you wow those last five
[41:34] posts were amazing Nick I loved the
[41:37] specifically the one on Korea and I just
[41:39] wanted to send you over some quick
[41:40] assets to help you out right you can now
[41:42] do super Dynamic and structured Outreach
[41:46] you could take that data and use it to
[41:48] like draft up your own post I mean the
[41:50] options are ultimately unlimited that's
[41:52] why I love appify so much the sixth way
[41:54] to scrape websites with NN is data for
[41:57] Co this is another thirdparty service
[41:59] but it's a very high quality one that's
[42:00] specifically geared towards search
[42:02] engine optimization requests you guys
[42:04] haven't seen data for SEO before it's
[42:06] basically this big API stack that allows
[42:08] you to do things like automatically
[42:10] query a service maybe some e-commerce
[42:12] website or some content website and then
[42:14] like extract things in nicely structured
[42:16] formatting um again specifically for SEO
[42:19] purposes tons of apis here as well I
[42:21] mean a lot of these services are now
[42:22] going towards like more Marketplace
[42:24] style stuff but just to give you guys an
[42:26] example you could like Google really
[42:28] quickly to scrape a big list of Google
[42:30] search results for a term and then you
[42:31] could like feed that into one of any of
[42:33] the other scrapers that we set up here
[42:34] to get data on stuff you could go Google
[42:36] Images Google Maps you could do Bing BYO
[42:39] YouTube Google's uh their own data set
[42:41] feature I don't really know what that is
[42:42] but I imagine it's pretty cool uh and
[42:44] then you can you can take this data and
[42:45] do really fun stuff with it so I'm just
[42:47] going to click try for free over here in
[42:48] the top right hand corner show you guys
[42:50] what that looks
[42:52] like and as you see here um I signed in
[42:55] to data for SEO to my own account looks
[42:57] like I have 38 million bajillion
[43:00] dollars um but obviously you'd have to
[43:03] go through the rig Rolla creating your
[43:04] own account so why don't actually just
[43:06] do that with you and then I'll just use
[43:07] that account that is 38 million
[43:08] bajillion dollars we'll click try for
[43:11] free we'll go Nikki
[43:15] Wiki uh let's use a different email I
[43:17] need a business email huh that's
[43:21] unfortunate okay I do agree to the terms
[43:23] of use
[43:25] absolutely uh bicycle
[43:28] is that a bicycle that's not a
[43:32] bicycle what does it mean when I can't
[43:35] answer these does it mean that I'm a
[43:38] robot if you look at some of my posts
[43:40] some of my comments people would
[43:42] absolutely say yes it means that that
[43:45] you're a robot um I don't know why
[43:47] people keep saying stuff like dude Nick
[43:49] nice AI Avatar bro but I'm it's not an
[43:52] AI Avatar it's not an AI Avatar at all
[43:55] it's actually just me okay anyway so I
[43:57] need to activate my account doesn't look
[43:59] like it allows you to feed in the code
[44:01] here so I'm just going to feed it in
[44:02] myself uh it's obviously you're getting
[44:04] a lot of spammers hence
[44:05] this um bicycle stuff I don't know why
[44:09] the code isn't working here let me just
[44:12] copy this link address paste it in here
[44:15] instead there you go okay great so now
[44:17] you can sign
[44:19] in and once you're in you got also um
[44:23] they're actually really big on on
[44:24] bicycles they're training um a model to
[44:26] convert all ads on planet Earth into
[44:28] bicycles they'll actually give you a
[44:29] dollar worth of API access uh credits
[44:32] which is pretty cool um I'm not going to
[44:34] do that I'm just going to go over to
[44:35] mine which is$ 38 million bajillion
[44:36] dollars with 99,999 estimated days to go
[44:40] um and yeah let's actually run through
[44:41] this the first thing that I recommend
[44:42] you do is go over to playground on the
[44:43] Le hand side there's all of their
[44:45] different API endpoints that you can
[44:47] call um what I'm going to do is I'll
[44:48] just go to serp for now just to show you
[44:50] that you could scrape Google with this
[44:51] pretty easily so maybe I'm in the UK and
[44:54] I want to scrape um let me see
[44:58] a keyword ni arrive okay then I'm going
[45:01] to send a request to this
[45:03] API there's there's a bunch of other
[45:05] terms here that are going to make more
[45:06] sense if you're a SEO person um but now
[45:09] we receive as output a structured object
[45:12] with a ton of stuff right we have um the
[45:15] first result here it's like an organic
[45:16] one with some big URL a bunch of chips
[45:20] um I'm I have like a Knowledge Graph
[45:22] profile which is cool apparently it
[45:23] finds it says I'm a freelance writer um
[45:27] you know we have a bun bunch of data
[45:28] here bunch of data you know you can use
[45:30] this to get URLs of specific things and
[45:32] then with the URLs you can then feed
[45:34] that into scrapers um that do more like
[45:36] I talked about earlier maybe appify or
[45:38] maybe rap API maybe fir crawl so a lot
[45:42] of options here to like create your own
[45:43] very complex
[45:45] flows you can do other stuff as well um
[45:48] you grab a bunch of keyword data so
[45:50] maybe you wanted to find a keyword and
[45:53] maybe again it's Nicks or location you
[45:55] want let's do United States that'll
[45:57] probably be
[45:58] better language um I'm just not going to
[46:01] select an language and then I'll do a
[46:03] request so now it's going to find us um
[46:06] a bunch of search volume related stuff
[46:08] so I don't actually know how many people
[46:10] are searching for me in 2025 apparently
[46:13] 390 is this per month H wonder if it's
[46:16] per month per day that's interesting uh
[46:20] I don't really know why they break it
[46:21] down by like the month date yeah looks
[46:24] like it's 390 per month so to the 390
[46:26] people that are Googling me who are you
[46:28] and what do you want I'm just kidding um
[46:31] you can do things like you could find
[46:32] back links so you could find links um
[46:35] for I believe you feed in a website URL
[46:38] and then it finds back links to that
[46:40] website so this is you technically now
[46:41] scraping a bunch of other websites
[46:43] looking for links to the specific
[46:45] resource that you have that's kind of
[46:49] neat it looks like that found it
[46:51] basically immediately which is really
[46:52] really
[46:54] cool and it looks like they're referring
[46:56] top level links that are Dooms BG bgs
[46:59] would be interesting I wonder where
[47:00] that's coming from um there's a Content
[47:03] generation API playground so you could
[47:05] you know feed in some text and then have
[47:07] it generate other stuff but I think
[47:08] we're kind of getting away from um the
[47:10] actual thing that matters which is the
[47:11] scraping of the uh scraping of the
[47:13] websites so yeah lots of stuff lots of
[47:16] stuff for sure now that's all good um
[47:18] but let's actually turn this into an API
[47:20] call if we head over to the API of do
[47:22] data for SEO so in my case docs. datafor
[47:24] seo.com V3 _ page SL contentor parsing
[47:30] live that's what I'm I'm curious about
[47:32] you'll see that we have a post request
[47:33] that we need to send to this URL um well
[47:36] I have a curl just like this which I can
[47:39] feed into um an API request that's what
[47:42] I'm going to do so I'm going to go back
[47:43] over here and I'm just going to import
[47:45] this curl
[47:47] import and it's going to go through and
[47:49] it's basically going to um parse out all
[47:52] these fields that I'm interested in with
[47:53] the URL which I'll go htps
[47:57] left click. AI um and then we have sort
[48:00] of like a gacha here that a lot of
[48:01] people don't understand this is the um
[48:03] authorization the authorization is a
[48:05] little bit different from most of the
[48:06] easy authorizations we've had so far we
[48:07] actually have to convert it um we have
[48:08] to go one one more step basically to
[48:10] make this work if I check out the let's
[48:15] see um
[48:17] authorization
[48:19] here what we need is we need to um get
[48:23] the login and then the P so this is your
[48:26] username and then your password then we
[48:28] have to Hash it or not hash it but we
[48:29] have to convert it into something called
[48:30] base 64 um this is just how they do
[48:33] their API key stuff I guess it's kind of
[48:35] annoying but it's just part and parcel
[48:37] of working with some apis you're just
[48:38] not always going to have it available to
[48:40] you really easily so I'm just going to
[48:42] go back to data for SEO and then I'm
[48:45] going to grab my credentials okay so
[48:47] what we need to do is we need to base 64
[48:50] encode the username and the password um
[48:52] I'm just going to leave that at what
[48:53] I've done is I've actually gone through
[48:54] and done it in this edit Fields node um
[48:57] basically what you need to do is you
[48:58] need to have your username or your login
[49:02] so maybe this is me searching Nix or
[49:04] have Reddit uh Nick left click. so that
[49:07] might be my username and then my
[49:09] password is What's called the API
[49:10] password you can find that really
[49:11] quickly and easily just by going over
[49:13] here to API access and then API password
[49:16] if you just signed up it'll be visible
[49:17] right here if it's been more than 24
[49:18] hours you actually have to send it by
[49:19] email but anyway so that's um that's
[49:21] where i' get the API password from uh
[49:24] and then once you feed it in over here
[49:26] where you're going to want to do is
[49:27] you're going to want to base 64 encode
[49:28] it like this they just require you to
[49:31] use these creds um or to operate with
[49:35] these creds as base 64 encoded versions
[49:36] Bas 64 is just a way to like translate
[49:38] into a slightly different number format
[49:40] so once you have that you would just
[49:41] feed in the variable right over
[49:43] here Ju Just as follows and then you can
[49:46] make a request to their API and receive
[49:47] data so uh it looks like I was doing
[49:50] their content parsing live you know what
[49:53] I wanted to do is I just wanted to
[49:55] call their endpoint which I think was
[49:58] their like instant
[50:02] Pages this one right over here so it's
[50:04] just V3 uh once you've sorted this out
[50:06] by the way the AP gets like
[50:07] extraordinarily easy to manage you just
[50:09] need to like figure out the
[50:09] authentication from there on out all
[50:11] you're literally doing is just swapping
[50:12] out the requests so you know if you
[50:14] wanted to do instant Pages all I'm doing
[50:16] is pumping that in there I just sent a
[50:19] request and now I receive a bunch of
[50:20] links with different headings and and so
[50:22] on and so forth that's easy the seventh
[50:24] way to scrape websites and Ed end is
[50:25] using a third party application called
[50:27] crawl Bas they're known for their
[50:29] rotating proxies which allow you to send
[50:31] very high volume um API requests so um
[50:33] it's very proxy driven this is their
[50:35] website so it's a scraping platform
[50:37] similar to Rapid API um and uh you know
[50:40] appify they support many of the major
[50:42] websites here and um the reason why
[50:44] they're so good at this is just because
[50:45] they you know as I mentioned they rotate
[50:47] the hell out of these proxies so we're
[50:49] just going to sign up to Tri it free
[50:51] I'll use my business email
[50:54] here and then continue with Emil
[50:57] email we got to add a phone number
[51:00] obviously we're going to do less than a
[51:01] thousand I'm a CTO I don't want to
[51:04] what's the animal right is that an
[51:06] animal yes it's an animal good God beep
[51:10] boop uh we're going to head over to my
[51:14] Gmail and receive this
[51:18] now so we need to confirm my account
[51:20] just going to copy this link address
[51:22] that I can do this in one
[51:23] page awesome we should be good to log in
[51:26] so that's what's
[51:27] happening we need to select the animal
[51:29] again just doesn't it doesn't believe
[51:31] really just doesn't
[51:34] believe okay great so now we have a
[51:36] crawling API smart proxy thing if you
[51:39] guys want to run like uh I don't know
[51:41] use in apps that have a proxy field
[51:43] specifically I'm just going to keep
[51:44] things simple we're doing this in n8n so
[51:46] we're going to go crawl base API we have
[51:47] a th000 free crawls remaining very first
[51:50] thing we're going to want to do is just
[51:51] click start crawling now just to get up
[51:52] and running with the
[51:54] API um and as you see here the these
[51:56] guys have probably one of the simplest
[51:57] apis possible all API URLs start with
[52:00] the folling base part click and then all
[52:02] you need to do in order to make an API
[52:04] call is run the following sort of line
[52:06] so this is a curl request obviously
[52:08] we're in n8n and one of the value
[52:11] valuable parts of NN is we can just
[52:12] import a COR request so well I'm going
[52:14] to import it as you can see here we have
[52:17] a token field then we just have the URL
[52:19] field of the place we want to crawl so
[52:21] I'm going to do left click. for now um I
[52:24] don't know if this token field was
[52:26] actually my real token I don't believe
[52:27] so maybe we'll give it a try maybe it's
[52:29] like a test token or
[52:31] something so I'm now running this and it
[52:34] looks like we just received a bunch of
[52:36] very spooky data I don't like the spooky
[52:38] data no spooky data for
[52:41] us um sometimes spooky data like
[52:47] this H this seems kind of weird to me
[52:49] actually just give me one second to make
[52:51] sure that's right we are receiving a
[52:53] data parameter back which is nice but
[52:56] yeah something about this is a little
[52:57] bit spooky um was it a get request or
[53:00] was it a post request no I guess it's a
[53:01] get request
[53:03] strange very very
[53:06] strange okay anyway they give you two
[53:09] types of tokens here um this is why I'm
[53:11] talking about it to begin with I'm also
[53:12] because I just used it before for a
[53:13] couple of applications and I found it
[53:15] very easy they give you a normal token
[53:16] and they give you a JavaScript um token
[53:19] as well so the reason why that's
[53:20] valuable is because if you're scraping
[53:22] one of these websites I talked about
[53:23] before where when you send a simple HTTP
[53:25] request nothing pops up like this is the
[53:27] this is the purpose of this you actually
[53:28] feed in a JavaScript token um when you
[53:31] use the JavaScript token it'll
[53:32] automatically launch a browser instance
[53:34] inside of um craw base for you so
[53:37] instead of you getting just like that
[53:39] empty thing back that I mentioned I'm
[53:40] you're actually going to get uh you're
[53:42] going to get like a JavaScript version
[53:44] of the website where somebody went on
[53:46] the website it loaded really briefly and
[53:48] then they grabbed the code
[53:50] afterwards so yeah we have some some API
[53:52] call stuff over here um this one's just
[53:54] using Amazon this is pretty interesting
[53:56] so I might actually give that a a go
[53:58] just to give you guys an example of said
[54:00] Amazon
[54:02] scrape uh let's just go
[54:09] www.amazon.com oh right Amazon might be
[54:11] JavaScript actually so maybe we give
[54:13] that a go no it looks like we um we got
[54:15] the data from from Amazon which is
[54:16] pretty cool if you feed that into the
[54:18] markdown converter like we had before
[54:20] it's going to feed in the HTML here pump
[54:23] it into a data
[54:24] key we've now converted this
[54:28] into uh this is very long let's go
[54:30] tabular we've now converted this into
[54:32] markdown which is cool and this is
[54:34] pretty long right obviously has all of
[54:36] the images and has all of the
[54:38] information on the site which is cool
[54:39] and then we can feed it into open AI
[54:41] like I did
[54:42] before where I message a model and I'm
[54:46] just going to copy um from my uh
[54:48] previous application here to make my
[54:49] life a little bit
[54:50] easier where the heck are you
[54:56] and then we're just going to feed in the
[54:58] code here and then because I didn't feed
[55:02] in this we should now run this we're
[55:06] going to grab data from the site and
[55:08] we're going to try and I mean you know
[55:09] we kind of all know what Amazon is and
[55:11] what it does right so I'm not expecting
[55:13] expecting anything spectacular but it's
[55:15] still going to go it's going to give me
[55:16] all of the text on this Amazon page and
[55:18] then I'm going to get a bunch of list of
[55:19] links basically absolute URLs ideally
[55:22] should play some Jeopardy
[55:25] music or going be able to play um Star
[55:28] Wars music that'd be kind of cool okay
[55:31] we now have a schema with all of the
[55:33] links on the page which is pretty cool
[55:34] we have the plain text website copy we
[55:37] have a on line summary uh you know plain
[55:39] text website copy is a lot longer than
[55:41] this obviously it's just shortening and
[55:42] truncating it for us but yeah very quick
[55:44] and easy way to use crawl base for this
[55:46] now the value in crawl base is not
[55:48] necessarily just to send them to static
[55:49] websites like I talked about it's to use
[55:51] like highly scalable scraping where
[55:55] you're scraping any applications
[55:58] consistently um as you see here the
[56:00] average API response time is between 4
[56:02] to 10 seconds so you you will receive
[56:03] results back pretty quick if you wanted
[56:05] to just send one request or 20 requests
[56:08] every second think about it like 20
[56:10] requests a second times 60 seconds a
[56:12] minute is 1,200 requests times 60
[56:14] minutes and an hour 72,000 requests
[56:16] right um sorry just jumping around the
[56:18] place here you can send 72,000 requests
[56:20] basically an hour which is crazy um and
[56:23] you can do so as quickly and as easily
[56:24] as just like adding an API call like
[56:26] and then it'll automatically distinguish
[56:28] between like a a plain text thing or a
[56:30] JavaScript thing okay the eighth way to
[56:31] scrape data in nadn specifically website
[56:34] resources is octop parse octoparse is
[56:37] very similar to some of the other
[56:38] services that we've talked about um it
[56:40] is a web scraping tool that actually
[56:41] gives you quote unquote free web
[56:43] crawlers and I'm just a fan of their ux
[56:44] I think it's very clean I think the way
[56:46] that they have their signup flow and
[56:47] stuff's really easy so if you made it to
[56:49] this part of the uh tutorial and you
[56:50] have yet to sign up to one of these
[56:52] Services give octoparse um give
[56:54] octoparse your thoughts
[56:56] let's double
[56:58] check that I haven't actually created an
[57:00] account using this no I haven't
[57:01] fantastic so I should be able to jump
[57:03] through and show you guys what this
[57:05] looks like we have a verification code
[57:07] I'm going to paste in if you're not
[57:09] familiar with jumping around and stuff
[57:11] like this um or if you're wondering how
[57:13] I'm jumping around I'm just using a
[57:14] bunch of website
[57:19] hotkeys okay great account is now ready
[57:22] so we can start a free premium trial if
[57:24] you want I think you're going to have to
[57:25] add a card um I don't know if I have
[57:26] enough credits to actually do anything
[57:29] but if I'm not then I'll start that
[57:30] trial in a second what you're going to
[57:32] have to do in order to make this work is
[57:33] you're going to want to have to download
[57:34] you're going to want to download the
[57:36] octoparse desktop app so let's give it a
[57:39] quick and easy go just going to drag
[57:42] this puppy um if you are using something
[57:45] that is not Mac OS you will not have
[57:47] this strange drag and drop feature here
[57:50] once that is done you will have octo
[57:51] parse accessible just open that up yes I
[57:54] want to open this thank you
[57:57] and the cool thing about
[57:59] octoparse um kind of relative to what
[58:03] else you know like the other scraping
[58:05] applications I talked about is this is
[58:07] just running in a desktop app um like
[58:09] kind of in in your
[58:10] computer so like it's cool because it's
[58:13] just easy to get up and running with um
[58:14] and it's also local as opposed to a lot
[58:16] of these other ones which are not so I'm
[58:19] going to Auto log in on my desktop app
[58:21] remember my password beautiful the
[58:22] simplest and easiest way to scrape a s a
[58:24] service is just to pump in the the URL
[58:26] here then click Start and basically
[58:28] what'll happen is um it'll actually
[58:30] launch like an instance of your browser
[58:31] here with this little tool that allow
[58:33] you similarly the web scraping Chrome
[58:35] extension select the elements on the
[58:36] page you want scraped so I don't know
[58:39] maybe I want these logos scraped the
[58:41] second that I tapped one you'll see it
[58:42] automatically found six similar elements
[58:45] so now I'm actually like scraping all of
[58:47] this stuff okay now we have access to
[58:50] this sort of drag and drop or um
[58:52] selector thing similar to what we had
[58:55] before if you click on one of these
[58:56] you'll see it allow you to select all
[58:58] similar
[58:59] Elements which is pretty sweet and then
[59:02] um you can also do things like click
[59:04] elements and so on and so forth extract
[59:06] the text Data here um you can also tie
[59:09] that to other things right so as you see
[59:12] I'm now mapping each of these very
[59:13] similarly to how I was doing before
[59:15] between the first field which is the
[59:17] title of the product and then the second
[59:18] field which is like the field to uh so
[59:21] that's pretty sweet we could do the same
[59:22] thing with a number of things you could
[59:23] extract like the headings and then the
[59:25] values and so on and so on and so forth
[59:27] but I'll kind of leave it there um so
[59:29] once you're done selecting all the
[59:30] elements that you want all you do is you
[59:31] click run and you have a choice between
[59:33] running it on your device versus running
[59:35] it on the cloud so um on the cloud is
[59:38] API supported that's how you're going to
[59:39] get stuff in NM but I just want you guys
[59:40] to know that you can also just run it
[59:41] here you could run it here load up the
[59:43] URL scrape all the things that you want
[59:45] on the specific page you're feeding in
[59:46] and then you can be done with it so I
[59:48] just selected run in the cloud it's now
[59:49] going to open up said Cloud instances as
[59:51] we could see we have this little field
[59:53] where it's running and extracting the
[59:54] data we're now done so I can export this
[59:58] data locally um but I could also do a
[60:00] lot of other stuff which we'll show you
[60:01] in a second so um you can dump this
[60:03] automatically to Google Sheets you could
[60:05] do zapier to connect to Google Sheets do
[60:07] like some sort of web Hook connection
[60:08] export to cloud storage uh similar stuff
[60:10] to the the web scraping Chrome extension
[60:13] um but for now let's just export this as
[60:16] Json give ourselves a little ad Json
[60:19] file here thank
[60:21] you and yeah now we have it locally now
[60:23] in order to connect with the octop par
[60:24] CPI what you're going to have to do is
[60:26] first you get up to request an access
[60:28] token the way that you do this is you
[60:29] send a post request to this URL here and
[60:33] the way that you format it is you need
[60:34] to send your username your password and
[60:38] then have the grantor type as password
[60:40] okay now password obviously just put in
[60:42] whatever your password is don't store it
[60:44] in PL text like I'm doing um with my
[60:45] hypothetical password put it somewhere
[60:47] else and then grab that data and then
[60:49] use it um but the the output of this is
[60:51] we have this big long access token
[60:52] variable which is great after that if I
[60:55] just go back to their API here um once
[60:57] we're here we can actually extract the
[60:58] data that we need so basically the thing
[61:01] that you're going to want is you're
[61:02] going to want um get data by Offset you
[61:04] can also use get non-exported data which
[61:06] is interesting so I think this just like
[61:07] dumps all of the data as not exported um
[61:10] and then sends that over to you I
[61:12] believe but anyway you could also get
[61:14] the data by offset so if I go a get
[61:17] request to open api. octop course.com SL
[61:20] all and then I just send a header with
[61:21] the URL parameter this is a get request
[61:25] uh we're going to send a header with the
[61:27] token so
[61:29] authorization
[61:30] Bearer and then feed in the access token
[61:33] here just make sure that this is just
[61:35] one space no it's
[61:37] two if I feed this in um it's saying
[61:41] that it's a bad request let me just
[61:42] triple check why I think we need three
[61:46] Fields yeah I think we need three Fields
[61:49] actually my bad um we need uh this get
[61:53] request then we need the authorization
[61:56] header like I talked about then we need
[61:58] three Fields task ID yeah right
[62:00] obviously we need to feed in the task ID
[62:01] so you need task ID offset or size so um
[62:05] we'll feed this in as query parameters
[62:07] here so send query parameters the first
[62:10] value was task ID second one was
[62:14] offset and uh offset is no Capital the
[62:18] third was size offset's going to be zero
[62:20] size going to be I don't know let's just
[62:22] do 1,000 and what we need now is we need
[62:24] the task ID of the specific run that we
[62:26] just finished oh in order to get the
[62:27] task list you head over to task list top
[62:29] right hand corner here task ID API so we
[62:32] now have access to this so if we go back
[62:35] to our NN instance we could feed that in
[62:37] here by test the step you'll see that we
[62:40] now have all the data that we just asked
[62:42] for earlier so a variety of ways to do
[62:44] this um in practice like octop par
[62:45] allows you to schedule runs you could
[62:47] schedule them um using their you know
[62:49] whatever it is uh uh like cloud service
[62:52] you could use it to scrape I don't know
[62:54] Twitter uh they have a variety of like
[62:56] other scrapers um that you can check out
[62:58] just heading over
[63:00] to this new here uh if we just go sorry
[63:03] go down to templates um there's a
[63:05] variety of other ways to scrape Google
[63:06] job scraper glass door scraper super
[63:08] Pages scraper you could schedule these
[63:10] right and then what you can do in na is
[63:11] you can just query it once a day grab
[63:13] all the data like I showed you how to do
[63:14] a moment ago dump that into some sheet
[63:17] octoparse is pretty cool it's like more
[63:18] of like an industrial Enterprise level
[63:20] application um to be honest so there
[63:23] might be some gotas if you're not super
[63:24] familiar with working with like desktop
[63:27] apps and stuff but I I like the idea
[63:28] that you can also just scrape locally
[63:30] which is pretty sweet and the last of
[63:31] our nine best ways to scrape websites in
[63:33] nadn is browserless now browserless runs
[63:36] a headless Chrome instance in the cloud
[63:38] this stuff is great for dynamic or heavy
[63:40] JavaScript websites if you've never used
[63:42] browser list before the cool part about
[63:45] browser list is allows you to actually
[63:46] bypass captas which is a big issue that
[63:48] a lot of people have um so I'm going to
[63:50] click try it for free I'm going to enter
[63:52] my email address over here verify I need
[63:56] to submit a code so let's head back over
[63:58] here thank you thank you thank you thank
[64:02] you we have a ton of free trial signups
[64:07] obviously I don't have a promo code or
[64:09] anything don't have a company name I'm
[64:11] just going to enter a password I'm using
[64:13] this to get past uh to avoid setting up
[64:15] a puppeteer and playright server sure
[64:17] I'm going to click complete we're now
[64:19] going to have a th credits inside of
[64:21] browser list which is pretty sweet um
[64:23] and we'll get a we'll get a full plan
[64:25] eventually we now have an API token so I
[64:27] can figure out how all of the stuff
[64:28] works here I'm just going to dive right
[64:29] into the API I can figure out how all of
[64:31] the API stuff works using their API docs
[64:33] which are fantastic by the way um and we
[64:35] don't want to do any of this stuff we
[64:37] just want to do HTTP apis brow list API
[64:39] index okay great so here's where we're
[64:41] at um if you want to send and receive a
[64:42] request what you need to do is uh you
[64:46] send a request to one of these endpoints
[64:48] content unblock download function PDF
[64:53] screenshot scrape or performance
[64:56] what we want for the purpose of this is
[64:58] just uh let's do content okay this is
[65:01] the request right over here so I'm just
[65:03] going to paste my API token up here copy
[65:05] this
[65:06] request feed it into nadn in the HTTP
[65:09] request module as per
[65:11] usual nice quick and easy I'm going to
[65:14] grab my API token and where it says your
[65:16] API token here I'm going to feed that in
[65:19] what I want as a website is just left
[65:21] click. a I'm going to run test step we
[65:24] are now quering the pi and in seconds we
[65:27] have access to the data same thing that
[65:30] we had before but now we're using a pass
[65:33] through and browser list is a great pass
[65:35] through um because you know uh they they
[65:38] allow you to scrape things that go far
[65:40] beyond the usual static site thing so
[65:44] like honestly and I'm just leaving this
[65:46] as a secret and sort of a little I guess
[65:48] Easter egg for people that have made it
[65:49] this far in the video like my go-to when
[65:51] scraping websites is as I mentioned do
[65:54] that HTTP request trans forg that works
[65:55] then do something like fir C.D but if
[65:57] that doesn't work I I do something like
[65:59] browserless that has all of this stuff
[66:01] built in um and I especially use browser
[66:05] list anytime that there's some sort of
[66:07] you know application where I'm just
[66:08] going to save this so I can make all my
[66:10] HTP requests really easy um especially
[66:12] when you know there's issues with captas
[66:14] and and accessing resources and stuff
[66:16] check this out not only can you do um
[66:19] the actual scrape you can do a
[66:21] screenshot of the page as well and
[66:22] because I've entered my token up here
[66:24] the requests that I'm going to setting
[66:25] up are as simple as importing the
[66:28] curl then clicking test step so
[66:31] straightforward we now have a file which
[66:34] is the screenshot now I used example
[66:36] domain there let's go left
[66:39] click. run this test now you can see
[66:42] we've actually like received a
[66:43] screenshot of the of the website
[66:47] view very sexy and my website's pretty
[66:49] long so keep in
[66:51] mind um and yeah you know obviously a
[66:53] lot you could do with that you can
[66:54] download the site you can turn the site
[66:56] into a PDF so um that's pretty neat I
[66:59] don't think I've actually used this one
[67:00] before but for the purposes of this
[67:02] demonstration why don't we give it a try
[67:04] we'll go over here import the curl paste
[67:07] it in voila the website I'm going to do
[67:10] is left click. aai going to test this
[67:14] step so now there servers doing a couple
[67:16] things like I'm scraping the site then
[67:18] converting it all into PDF format um
[67:20] probably screenshotting a bunch of stuff
[67:21] too if I view this now we now have my my
[67:24] file looks like it didn't capture all of
[67:26] the color aspects um that might just be
[67:29] difficult or whatever but I still have
[67:30] like a PDF of the site which is pretty
[67:31] neat um and yeah let you guys kind of
[67:34] screw around with this on your own but
[67:35] there are a variety of cool applications
[67:36] you can use browless for all right I
[67:39] hope you guys appreciated the nine best
[67:40] ways to scrape websites in nadn as you
[67:42] guys could see it's a combination of on
[67:45] platform scraping using the HTTP request
[67:47] module a lot of like API documentation
[67:49] stuff like that if you want to get good
[67:51] at this I'm releasing a master class on
[67:52] API stuff um uh as part of my next na
[67:56] tutorial video uh and then you know
[67:58] navigating this and then and then taking
[68:00] the data from these services and using
[68:01] them to do something that you want to do
[68:03] like artificial intelligence to give you
[68:04] a summary of the site or generate ice
[68:07] breakers for you or do something else um
[68:09] whether you're using a local application
[68:11] like octop parse or maybe the web
[68:13] scraping CH uh Chrome extension or using
[68:16] something like firra browserless appify
[68:19] rapid API and so on and so forth um you
[68:21] now have everything that you need in
[68:22] order to scrape static sites Dynamic
[68:24] sites super Js heavy websites and even
[68:26] social media websites like Tik Tok
[68:28] Twitter and Instagram thanks so much for
[68:29] making it to this point in the video if
[68:31] you have any suggestions for future
[68:32] content drop them down below more than
[68:34] happy to take your idea and run with it
[68:36] assuming it's something that I haven't
[68:37] done before and then if you guys could
[68:39] do me a really big solid like subscribe
[68:41] do all that fun YouTube stuff and I'll
[68:42] catch you on the next video thank you
[68:43] very much

Guidelines:
1. Start with a compelling hook
2. Extract 3-5 key insights or takeaways
3. Include relevant statistics or data points if available
4. End with a thought-provoking question or call-to-action
5. Add appropriate hashtags based on the content
6. Keep the total length between 1000-1300 characters
7. Format for LinkedIn's style (use line breaks effectively)
8. Include the YouTube video link at the end

Video Link: https://youtu.be/y-eEbmNeFZo

==================================================

--- TEMPLATE PROMPT ---

You are a professional content marketer specializing in LinkedIn content. Create a structured, template-based LinkedIn post from this YouTube video content.
Use a professional template format that's proven to drive engagement.

Video Title: The 9 Best Ways to Scrape Any Website in N8N
Channel: Nick Saraev
Duration: 1:08:45
Views: 1,230
Likes: 88
Description: Join Maker School & get your first automation customer ‚§µÔ∏è
https://www.skool.com/makerschool/about

Summary ‚§µÔ∏è
Here I show you the 9 best ways to scrape websites using N8N.

First, I'll cover the basics of handling static and dynamic sites (and the differences between them. I'll then show you how to manage direct HTTP requests in N8N before demoing tools like Firecrawl, RapidAPI, and Browserless. Plus, I'll explore options like the Web Scraper Chrome Extension and Apify for advanced tasks. Let me know if q's!

My software, tools, & deals (some give me kickbacks‚Äîthank you!)
üöÄ Instantly: https://link.nicksaraev.com/instantly-short
üìß Anymailfinder: https://link.nicksaraev.com/amf-short
ü§ñ Apify: https://console.apify.com/sign-up (30% off with code 30NICKSARAEV)
üßëüèΩ‚Äçüíª n8n: https://n8n.partnerlinks.io/h372ujv8cw80
üìà Rize: https://link.nicksaraev.com/rize-short (25% off with promo code NICK)

Follow me on other platforms üòà
üì∏ Instagram: https://www.instagram.com/nick_saraev
üïäÔ∏è Twitter/X: https://twitter.com/nicksaraev
ü§ô Blog: https://nicksaraev.com

Why watch?
If this is your first view‚Äîhi, I‚Äôm Nick! TLDR: I spent six years building automated businesses with Make.com (most notably 1SecondCopy, a content company that hit 7 figures). Today a lot of people talk about automation, but I‚Äôve noticed that very few have practical, real world success making money with it. So this channel is me chiming in and showing you what *real* systems that make *real* revenue look like.

Hopefully I can help you improve your business, and in doing so, the rest of your life üôè

Like, subscribe, and leave me a comment if you have a specific request! Thanks.

Chapters
00:00:040Introduction to Web Scraping in N8N
00:00:42 Understanding Static vs Dynamic Sites
00:02:44 First Method: HTTP Requests
00:05:44 Converting HTML to Markdown
00:09:51 Using OpenAI for Data Processing
00:10:17 Second Method: Firecrawl Service
00:11:25 Signing Up for Firecrawl
00:14:10 Third Method: Rapid API Marketplace
00:19:24 Fourth Method: Web Scraper Chrome Extension
00:25:36 Fifth Method: Appify for Scraping
00:34:30 Sixth Method: Data for SEO API
00:41:18 Seventh Method: Crawlbase for High Volume Requests
00:48:51 Eighth Method: Octoparse for Local Scraping
01:03:30 Ninth Method: Browserless for Dynamic Sites
01:08:20 Conclusion and Future Content Suggestions
Tags: automation, make.com, content creation, ai content, google sheets, chatgpt, wordpress, openai, blogging, integromat, make, automating, automate, gpt-4, gpt, openai api, indie hacking, small business, $20K/mo, cold email, make money online, make.com for people who want to make real money, make.com money, AIAA, ai automation, ai automation agency, ai automation guide, n8n scraping, n8n how to scrape, n8n scraper, n8n scrape, n8n scraping setup, n8n scrapers, n8n ai scraper, firecrawl

Transcription:
[00:00] hey Nick here today I'm going to show
[00:01] you the nine best ways to scrape any
[00:03] website in nadn you're going to be able
[00:05] to scrape static sites Dynamic sites
[00:07] JavaScript social media whatever the
[00:09] heck you want by the end of the video
[00:10] you'll know how to do it I scaled my
[00:12] automation agency to 72k a month using
[00:14] no code tools like make and nadn and
[00:16] scraping was a big part of that so this
[00:18] video is just going to give you all the
[00:18] sauce you're going to learn everything
[00:19] you need to scrape websites like that on
[00:21] your own let's get into it all right I'm
[00:23] going to jump into NN in a minute and
[00:24] actually build these alongside you and
[00:25] one other thing I'm going to do is I'm
[00:26] actually going to sign up to all the
[00:27] services in front of you walk you
[00:29] through the Authentication and the
[00:30] onboarding flows and get your API keys
[00:32] and stuff like that but just before I do
[00:34] want to explain very quickly the
[00:36] difference between a static site and
[00:38] then a dynamic site because if you don't
[00:40] know this um scraping just gets a lot
[00:42] harder and so we're just going to cover
[00:43] this in like 30 seconds and we can move
[00:45] on so basically um if this is you okay
[00:47] you're just this wonderful smiley person
[00:50] and you want to access a static website
[00:52] what you're doing is you're sending a
[00:54] request over basically to just like some
[00:56] document you know think about this as
[00:58] just like a piece of paper on a cupboard
[00:59] and there's a bunch of text on this
[01:01] piece of paper and what you do is you
[01:02] say hey can I have this piece of paper
[01:04] and then the piece of paper just comes
[01:06] back to you with all of the information
[01:07] inside of the piece of paper okay this
[01:09] is a very simplified version of what's
[01:11] actually going on but static sites are
[01:13] by far the easiest thing to scrape and
[01:15] so um this is where you know a lot of
[01:17] people think all websites are are at and
[01:20] then they kind of confuse it with this
[01:21] next step which is dynamic a dynamic
[01:23] site essentially is not like that at all
[01:26] basically what you're doing is you're
[01:27] sending a request to a piece of paper
[01:30] but the piece of paper has nothing on it
[01:32] okay what happens is this piece of paper
[01:34] then sends a request to some other dude
[01:36] which I guess in this case is just a
[01:37] server really who will then he has a
[01:40] trusty pen in his hand and he'll
[01:41] actually write all of the stuff on said
[01:44] piece of paper and then you'll get the
[01:46] piece of paper back so um there's
[01:48] actually that intermediate step okay
[01:50] where basically you are pinging some
[01:51] sort of uh you know domain name or
[01:53] whatever then that domain name shoots
[01:55] some code over forces a server to
[01:57] generate all of the contents on that
[01:58] domain and then you get it
[02:00] this is obviously kind of a two-step
[02:02] process and then this is a three-step
[02:04] process so if you just understand that
[02:08] um you know when you scrape a dynamic
[02:09] resource what you're really doing is
[02:10] you're sending a request to a page which
[02:12] sends a request back to another server
[02:14] which then fills your thing this element
[02:16] eliminates 99% of the confusion because
[02:18] most of the time like scraping issues
[02:19] are hey I just ping this page but I got
[02:21] nothing back you know the HTTP request
[02:23] or or or whatever I I sent you know it
[02:26] was fine but for some weird reason
[02:27] there's nothing on this page of of any
[02:29] substance well if that was the case for
[02:31] you it was most likely um because this
[02:32] was empty before you sent it over um
[02:35] whereas you know simple scraped static
[02:37] resources tend to just like give you
[02:38] what you want really quickly and it's
[02:39] really easy all right so hopefully we at
[02:42] least understand that there's that
[02:43] difference between static and dynamic
[02:44] sites here um I'm not going to go into
[02:46] it more than that we're actually just
[02:47] going to dive in with both feet start
[02:49] doing a little bit of scraping and then
[02:50] we'll kind of see where we land I find
[02:51] the best way to do this stuff is just by
[02:53] example and and you know being practical
[02:54] about it so the first major way to
[02:57] scrape websites in NN is using direct h
[02:59] HTTP requests this is also what I like
[03:02] to think of as the Magic in scraping
[03:05] itself what we're going to do is we're
[03:07] going to use a node called the HTTP
[03:08] request node to send a get request to
[03:10] the website we want this is going to
[03:12] work with static websites and non
[03:14] JavaScript resources so let me give you
[03:16] guys a website that I'm going to be
[03:17] scraping here this is my own site it's
[03:19] called left click I'm about to do a
[03:20] redesign um but this is a static
[03:22] resource I know this because I built the
[03:24] site you know I I did it in code and
[03:26] basically all this is is just a document
[03:27] somewhere on my or on on a server some
[03:29] more so what I'm going to want to do is
[03:31] and I'm just going to pretend that I
[03:32] haven't done any of this so uh we're
[03:34] just going to go HTTP
[03:37] request HTTP request node looks like
[03:39] this we have a method field a URL field
[03:42] authentication field query parameters
[03:44] headers body and then some options down
[03:46] here as well all I'm going to do is I'm
[03:48] just going to paste in the website that
[03:50] I want to visit okay then I'm just going
[03:52] to test the step it's that easy now the
[03:55] response from this on the right hand
[03:56] side see all this code over here this is
[03:59] what's called HTML if you're unfamiliar
[04:01] and HTML is basically just the like it's
[04:04] it's the code behind the site so if I
[04:07] were to zoom in over here you see where
[04:09] it says I don't know let's let's go to
[04:10] my website let's just find a little bit
[04:11] of little bit of texture build hands off
[04:14] growth systems okay if I just command F
[04:17] and paste this in we actually have that
[04:19] text buried somewhere in this big long
[04:21] HTML string right so all that this HTML
[04:25] is is this is the code that is sent to
[04:28] my browser which is Google Chrome in
[04:30] this case then my browser takes the code
[04:32] and it just renders it into this
[04:34] beautiful looking thing well beautiful
[04:35] is a subjective State I would say but
[04:38] this uh wonderful looking thing in front
[04:39] of us which is this website with like
[04:41] sizing and the tabs and the divs and all
[04:44] that fun stuff okay so basically what
[04:47] I'm trying to say is everything over
[04:48] here on the right hand side this is the
[04:50] entire site we can do anything we want
[04:52] with this information um and we can
[04:54] carry this information forward to to do
[04:56] any one of our any one of many flows so
[04:59] in my case right looking at a bunch of
[05:00] code isn't really very pretty so one big
[05:02] thing that you'll find in the vast
[05:03] majority of modern um scraping
[05:05] applications is you'll find that they'll
[05:06] take that HTML which we saw earlier and
[05:08] they'll convert it to something called
[05:09] markdown okay so um this is a markdown
[05:13] node we have a mode of HTML to markdown
[05:16] and all I'm going to do is I'm going to
[05:18] grab that data and I'm going stick it in
[05:20] the HTML section of the HTML to markdown
[05:22] converter what do you think is going to
[05:24] happen when I test this well we're going
[05:25] to convert this from this big long ugly
[05:27] super dense uh thing with a much these
[05:30] like greater than and less than symbols
[05:31] and we're just going to convert it into
[05:33] something a little bit shorter a little
[05:34] bit simpler This Is Us just manipulating
[05:36] file formats by the way and I find that
[05:38] manipulating file formats is a big part
[05:39] of what makes a good scraper a good
[05:41] scraper so now we have something in
[05:43] what's called markdown format what's the
[05:45] value there well markdown format does
[05:46] two things for us one it's much easier
[05:48] to parse parse just means we can extract
[05:50] different sections of the text we want
[05:52] we can structure it in some sort of
[05:54] other data format um and then in my case
[05:56] because I love using AI for everything
[05:58] it's much easier and shorter for us to
[06:00] use with AI so I'm going to give you
[06:01] guys a very simple example where we take
[06:03] this text from the static resource and
[06:05] then we just use um AI to tell us
[06:08] something about it so I'll go down to
[06:09] open Ai and then what I'm going to do is
[06:11] I'll do the message a model just have to
[06:14] connect my credential here I'm assuming
[06:16] that you've already connected a
[06:16] credential if not you're going to have
[06:18] to go to opena website when you do the
[06:20] connection um and grab your API key and
[06:22] paste it in there's some instructions
[06:24] that allow you to do so right over
[06:26] here uh what I'm going to do is I'm
[06:28] going to grab the G PT 40 Mini model
[06:31] that's just the uh I want to say most
[06:33] cost effective one as of the time of
[06:34] this recording and then what I'm going
[06:36] to do is I'm going to add three prompt
[06:37] I'm going to add a system prompt first
[06:39] I'll say you are a helpful intelligent
[06:42] web scraping
[06:43] assistant then I'm going to add a user
[06:46] prompt and I'll say your task is to take
[06:48] the
[06:49] raw
[06:51] markdown of a website and convert it
[06:54] into structured data use the following
[06:57] format and then I'm going to give it an
[06:59] example of what I want in what's called
[07:00] Json JavaScript object notation format
[07:02] so the very first thing I'm going to do
[07:04] is I'm going to have it just pull out
[07:05] all the links on the website because I
[07:07] find that that's a very common scraping
[07:08] application so I go links and then I'm
[07:10] just going to show an example of un
[07:13] array of we'll go absolute URLs this is
[07:19] very important that they're absolute
[07:20] URLs any thing that we're going to build
[07:22] after this is going to be making use of
[07:23] the absolute URLs not the relative URLs
[07:25] if you're unfamiliar with what that
[07:26] means if we Zoom way in here you see how
[07:28] there's this B uh SL left click log.png
[07:31] this is what's called a relative URL if
[07:33] you were to copy this and paste this
[07:35] into here this wouldn't actually do
[07:37] anything for us right uh what we what we
[07:39] want is we want this instead we want
[07:41] left click aka the root of the domain
[07:43] and then um left click _ logogram and
[07:46] that's how we get to the actual file
[07:47] asset so uh if we go back over
[07:50] here so if we go back over here um yeah
[07:53] you know basically we want a link array
[07:56] of absolute URLs and then I'm just going
[07:58] to want main text website copy this is
[08:02] going to be a long string containing all
[08:05] of the website
[08:06] copy containing just the text of the
[08:09] site no
[08:11] formatting and then why don't we do one
[08:13] more thing why don't we have like a
[08:16] summarized or summary let's do one line
[08:19] summary just to show you guys you can
[08:21] also use AI to do other cool stuff you
[08:23] could take this oneline summary and feed
[08:25] it into some big sequence you could have
[08:26] ai write an icebreaker for an email you
[08:29] could do a things with this but I'll say
[08:30] on line summary um brief summarization
[08:34] of what the site is and
[08:37] how what the site is about let's do that
[08:42] okay so this is our example I'm going to
[08:45] say your website URL
[08:48] is left click URL for the relative to
[08:53] Absolute
[08:54] conversions is left click. and then the
[08:56] final thing is I'm going to add one more
[08:58] user prompt I'm just going to draw drag
[08:59] all of that markdown data in here then
[09:01] I'm going to click output content as
[09:03] Json I'm going to test the step I'm
[09:06] going to take a sip of my coffee while
[09:07] this puppy processes and we now have our
[09:09] output on the right hand side if we go
[09:11] to schema view what you can see is we've
[09:14] now
[09:15] generated basically an array of links on
[09:17] the rightand side which contains every
[09:19] link on this website very cool looks
[09:21] like the vast majority of these are type
[09:23] form links for some reason don't really
[09:24] know what's about that oh right it's
[09:26] because that's basically the only thing
[09:27] on my website it's just a one P with a
[09:29] bunch of different links to time for
[09:31] that's funny um anyway you could
[09:33] obviously just get it to Output one link
[09:35] or tell it like make sure all the links
[09:36] are unique or something um and then we
[09:38] have a big chunk of plain text website
[09:40] copy right then we have a oneline
[09:41] summary of the site so this is a very
[09:44] simple example of scraping we're
[09:46] scraping a static resource obviously but
[09:48] when I build scrapers for clients or for
[09:50] my own business this is always my first
[09:52] pass I will always just make a basic
[09:54] HTTP request to the resource that I'm
[09:56] looking at because if I can make that
[09:58] http request work whether it's a get
[10:00] request or whatever the the the rest of
[10:03] my life building scraper building the
[10:05] scraper is so easy I just take the data
[10:08] I process it usually using AI or some
[10:09] very cheap Tok cheap per token thing and
[10:12] then voila you know like we've basically
[10:14] built out a scraper in this case and
[10:15] it's only taken us what three nodes
[10:17] right so that's number one the second
[10:19] way to scrape websites in NN is using a
[10:21] third party service called fir crawl and
[10:22] making an HTTP request to it I'm using
[10:25] something called their extract endpoint
[10:27] but just to make a long story short fire
[10:29] craw is a very simple but High uh
[10:32] bandwidth service that turns websites
[10:35] into large language model ready data and
[10:37] basically you know how earlier we had to
[10:38] do HTTP request and then we had to
[10:40] convert all that stuff into markdown and
[10:41] then we had to you know manipulate that
[10:43] markdown what this does is it just does
[10:45] a lot of that stuff for you it'll
[10:46] actually allow you to go scrape and then
[10:48] it will automatically convert text into
[10:49] markdown for you um so that you can do
[10:52] whatever the heck you want they turn it
[10:53] into structure data using Ai and and so
[10:54] on and so on and so forth so if I were
[10:56] to do the same thing that I just did
[10:58] earlier
[10:59] with my own
[11:00] website then I were to you know run an
[11:02] example of this what it would go do is
[11:05] it would basically spin up a server for
[11:07] me and that would actually go and
[11:08] generate markdown of the same format um
[11:10] the only difference here is it's
[11:11] actually generated new lines between
[11:13] sections of text how beautiful um and
[11:15] then now you know we have basically the
[11:17] same thing you also get it in Json which
[11:19] is pretty cool um and you know you can
[11:20] slot this into any workflow this is
[11:22] basically like the simple and easy way
[11:23] of getting started um what we're going
[11:26] to be showing you today is the extract
[11:28] endpoint which allows you to extract
[11:30] data just using a natural language
[11:32] prompt which is pretty cool and from
[11:34] here we're going to be able to take any
[11:35] URL and just turn it into structure data
[11:37] but we're not actually going to have to
[11:37] know how to parse we're not going to
[11:38] have to know any code we're not going to
[11:39] have to know any of that stuff so let me
[11:41] actually run through the signup process
[11:42] with you guys go to fire. Dev here just
[11:47] going to open this up in an incognito to
[11:48] show you guys what this looks like all
[11:49] you do is you just go sign up I'm going
[11:53] to add a password we're then going to
[11:55] have to validate this one
[11:58] oh guess these guys are Mega
[12:01] secure so now I'm going to go back to my
[12:03] email
[12:04] address I'm going to count this up and
[12:09] we have a call back here I just need to
[12:10] paste this URL and put it in here that's
[12:12] just because I you know I'm doing this
[12:14] in an incognito tab normally when you do
[12:16] this you're not going to have that step
[12:17] okay great now we're inside a fir craw
[12:20] they give you I think something like 500
[12:22] um free credits something of that nature
[12:24] anyway so what I'm going to do is I'm
[12:26] going to go through and just like give
[12:28] give this extracting point just a basic
[12:30] natural language query so um let's go
[12:34] from the homepage at left
[12:37] click. I want to
[12:40] extract a oneline summary of the website
[12:45] let's do all of the text on the
[12:50] website all of the copy on the website
[12:53] in plain text let's do a oneline summary
[12:56] of the website a oneline Icebreaker I
[12:58] can use as the first line of of an of a
[13:01] cold email to the
[13:05] owner and uh the company name and a list
[13:11] of the services they provide let's do
[13:14] that this is a lot of requests we're
[13:17] asking it to do like seven or eight
[13:19] things but all I need to do in order to
[13:21] make this work is I click generate
[13:22] parameters it's going to basically now
[13:24] generate me a big object with a bunch of
[13:26] things so copy summary Icebreaker
[13:28] company name and now I can actually go
[13:30] and I can run this okay this is the URL
[13:33] it just parsed as well let's give it a
[13:35] run what it's doing now is it's scraping
[13:38] the pages using their high throughput
[13:39] server I just love this thing like I'm
[13:42] not sponsored by fire crawl or anything
[13:43] like that but I love their uh I don't
[13:45] know I just love the design I love this
[13:47] little like burning Ember or whatever
[13:49] the heck you want to call it I love how
[13:51] simple they've tried to make everything
[13:52] it's it's great honestly okay awesome
[13:55] and now you guys see we have basically a
[13:57] big array with a bunch of sub objects we
[14:00] have a summary like I asked for a list
[14:02] of services looks like we even have
[14:04] links to the specific places oh okay
[14:06] links from the resource we have an
[14:08] icebreaker and then we have the company
[14:10] name as well so we can do a lot with
[14:11] this right but right now this is just um
[14:13] this is just on on a website how do we
[14:14] actually bring this in naden uh well
[14:16] it's pretty simple as you see there's an
[14:17] integrate Now button you can either get
[14:19] code or you can use it in zap here
[14:21] basically what we're going to want to do
[14:22] is we're going to want to run a request
[14:24] to um their endpoint and then we're
[14:26] going to want to turn that into
[14:29] basically our HTTP request let me show
[14:30] you what that looks like I'm just going
[14:33] to do all of this stuff in curl so if we
[14:35] go to curl as you can see what we need
[14:38] to do is we need to format a request
[14:40] that looks something like this but we
[14:41] need to make sure it's using the extract
[14:43] endpoint okay so I'm going to go down to
[14:45] extract and then now I have this big
[14:47] long beautiful string what I'm going to
[14:50] do is I'm going to copy
[14:51] this I'm going to go back to my NN
[14:55] instance which is right over here and
[14:56] then what I need to do is just open up
[14:58] an HTTP request module and then click
[14:59] import curl just paste all the stuff
[15:01] inside now this is an example request
[15:03] but that's okay we can actually use that
[15:04] example request to very quickly and
[15:05] easily format our our real request okay
[15:08] so we're sending a bunch of headers this
[15:09] is the endpoint that we're calling api.
[15:11] fire. dv1 extract so basically what
[15:14] we're doing now is we're like we're
[15:15] sending a request to fir craw which will
[15:16] then send a request to the website right
[15:18] so kind of a kind of a middleman and
[15:20] then all I'm going to do so if I go back
[15:22] to my example we have an API key here
[15:24] which we're going to need so I'm going
[15:25] to go here and then paste in an API key
[15:28] so that's how that work works right
[15:29] authorization is going to be the name
[15:30] value is going to be bear with a capital
[15:32] b space and then the API key and then we
[15:35] also have a body that we need to uh
[15:37] adjust or edit and this body is where
[15:39] we're going to put the links that we
[15:40] want to actually have scraped with the
[15:41] extract end point so what I'm going to
[15:44] do is I'm going to delete most of these
[15:46] I'll go back to my left click. a just
[15:48] like this the prompts um because you
[15:50] know I was just using their playground
[15:51] before we're actually going to need to
[15:52] convert this into a request for my
[15:56] service so I'm just going to paste The
[15:57] Prompt in here voila
[15:59] and now we need to put together what's
[16:00] called a schema where we have the
[16:02] objects that we asked for so in my case
[16:04] we asked for copy right so I'm going to
[16:06] go Copy Type string then summary so
[16:09] we're going to go summary type string
[16:12] then Icebreaker it's going to be
[16:14] Icebreaker type string then guess what
[16:16] we have last but not least company name
[16:18] which is going to be type string we're
[16:21] also going to want to make these fields
[16:22] required like uh you know you can set it
[16:24] up so they're not actually required when
[16:25] you do a request a fire call I'm I'm
[16:27] going to make it so they're required so
[16:28] I'll go copy
[16:29] summary Icebreaker and then company name
[16:32] actually you know what maybe I'll leave
[16:34] company name as unrequired if you think
[16:36] about it logically maybe not all the
[16:38] websites we're going to be scraping
[16:39] using this service are going to have the
[16:40] company names visible on the website I
[16:42] don't know but maybe so maybe I'll
[16:44] actually leave that as off okay great so
[16:46] now we have the API request formatted
[16:48] correctly um all we need to do at this
[16:50] point is just click test step it looks
[16:52] like we're getting a Json breaking um
[16:55] error and I think that's because I have
[16:56] this last comma and I'm just going to
[16:58] check to see if there are any commas in
[16:59] Jason you can't actually have the last
[17:00] element in an array have a comma on it
[17:03] so I think that's okay let me test it
[17:05] again all right so as you can see we
[17:06] just received an ID we've got a success
[17:08] and then we have a URL Trace array which
[17:10] is empty um if you think about this
[17:12] logically we don't actually get all the
[17:14] data that we send immediately because we
[17:16] need fir crawl to whip up the scraper
[17:18] you know do things to the data we could
[17:20] be feeding in 50 URLs here right so
[17:22] instead of just having the data
[17:23] available to us right now immediately
[17:25] what we need to do is we need to
[17:25] basically wait a little while wait until
[17:27] it's done and we need to Ping it and the
[17:29] reason why they've given us this ID
[17:31] parameter so that we could do the
[17:32] pinging so the way that you do this is
[17:34] you'd have to send a second HTTP
[17:36] request using this
[17:38] structure so the good news is we could
[17:41] just copy this
[17:42] over and then we can
[17:45] add a second um HTTP request I don't
[17:49] know where that went but I guess I'm
[17:50] just going to create it over here I'm
[17:53] going to import the curl to this request
[17:55] just like
[17:56] that then keep in mind that we just need
[17:58] to add our API key again because the
[18:00] previous node had it but this one
[18:01] doesn't so just going to go over here
[18:03] I'm going to copy this
[18:05] puppy go back over here I'm going to
[18:08] paste this in now technically what this
[18:10] is called is this is called polling um
[18:12] polling uh is where you know you're
[18:15] you're you're attempting to request a
[18:17] resource that you don't know whether or
[18:18] not is ready and there's a fair amount
[18:20] of logic that I'd recommend like putting
[18:22] into a polling flow where like when you
[18:24] try it and if it doesn't work basically
[18:25] you wait a certain amount of time and
[18:26] you retry again for the purpose of this
[18:28] video I'm not going to put all that
[18:29] stuff inside but um what I'm going to do
[18:31] is just set up this request I'm going to
[18:33] give this puppy a test let's just feed
[18:36] that in on the back end we got to put
[18:37] the extract ID right right over here
[18:40] where it said extract ID then I'm just
[18:41] going to give this a test uh looks like
[18:44] I've issued a malformed request we just
[18:46] have to make sure that everything here
[18:48] is okay specify body let me just make
[18:50] sure there's nothing else in here it was
[18:51] a get request this is a get cool we're
[18:54] not going to send a body
[18:56] then awesome and now we have all of the
[18:58] data available to us automate your
[19:01] business in the copy field summary field
[19:02] left clicks an ad performance
[19:04] optimization agency Icebreaker hi Nick I
[19:06] came across left click I'm impressed by
[19:07] you help B2B Founders scale their
[19:08] business automation keep in mind I never
[19:11] gave it my name it went it found my name
[19:12] on the website uh and then company name
[19:14] left click so quick and easy way uh
[19:17] you're going to have access to this
[19:18] template obviously without my API key in
[19:19] it um and feel free to you know use fir
[19:21] craw go nuts check out their
[19:22] documentation build out as complex a
[19:24] scraping flow as need be the third way
[19:25] to scrape websites in nadn is using
[19:28] rapid API for those of you that are
[19:29] unfamiliar rapid API is basically a
[19:31] giant Marketplace of third party
[19:33] scrapers similar to appify which I'll
[19:34] cover in a moment but instead of looking
[19:36] for um you know building out your own
[19:38] scraper for a resource let's say you're
[19:40] wanting to scrape Instagram or something
[19:41] that's not a simple static site what you
[19:43] can do is you could just get a scraper
[19:44] that somebody's already developed that
[19:46] does specifically that using proxies and
[19:48] all that tough stuff that I tend to
[19:50] abstract away um and then you just
[19:52] request uh to Rapid API which
[19:55] automatically handles the API request to
[19:56] the other thing that they want and then
[19:57] they format it and send it all back to
[19:59] and then you know you have beautiful um
[20:01] data that you could use for basically
[20:02] anything so this is what rapid API looks
[20:04] like it's basically a big Marketplace I
[20:05] just pumped in a search for website over
[20:08] here and we see 2,97 results to give you
[20:11] guys some context you can do everything
[20:12] from you know scraping social data like
[20:15] emails phone numbers and stuff like that
[20:17] from a website you could ping the ah
[20:19] refs SEO API you could find uh I don't
[20:23] know like unofficial medium data that
[20:25] they don't necessarily allow people to
[20:26] do so this is just a quick and easy way
[20:28] to I guess do a first pass after you've
[20:31] run through fir crawl maybe that doesn't
[20:32] work after you've run through HTTP
[20:33] request that doesn't work um just do a
[20:35] first pass look for something that
[20:36] scrapes the exact resource you're
[20:37] looking for and then take it from there
[20:39] so obviously for the purpose of this I'm
[20:40] just going to use the website to scraper
[20:41] API which is sort of just like a wrapper
[20:44] around what we're doing right now in
[20:45] nadn um but this website scraper API
[20:48] allows you to scrape some more Dynamic
[20:49] data um now I'm not signed up to this so
[20:51] I'm going to have to go through the
[20:52] signup process and I'm going to show you
[20:53] guys what that looks like um but yeah
[20:55] we're going to we're going to run
[20:55] through an API request to Rapid API
[20:57] which is going to make this a lot easier
[21:00] just going to put in all of my
[21:02] information
[21:04] here and then I'm going to do the
[21:07] classic email verification okay just
[21:11] copy this puppy over no thank you rise I
[21:13] use a time management app called rise
[21:16] and every time I go on my Gmail I set my
[21:18] Gmail up as like a definitely do
[21:22] not uh do during your workday let's just
[21:25] call it personal projects they don't ask
[21:26] me all these questions my goal today is
[21:28] to browse available apis awesome so
[21:31] that's their onboarding I think we're
[21:32] going to have to like pay a little bit
[21:33] of money or something like that which
[21:34] I'll sort out in a moment um but the
[21:36] scraper that I want is I just want the
[21:37] website one right so I'm going to type
[21:38] website in
[21:39] here uh I'm going to look
[21:42] for wherever it was earlier website
[21:44] scraper API and now check this out what
[21:47] we have is we have the app which is the
[21:49] name of the specific API that we're
[21:51] requesting we have an x-raid api-key and
[21:55] this is the API key we're going to use
[21:56] to make the request then we have the
[21:58] request URL which is basically what
[22:00] we're pinging and what we can do here is
[22:01] we can feed in the parameters okay what
[22:03] website we want to we want to scrape and
[22:05] then we can actually just like give it
[22:06] give it a run so I'm going to have to
[22:08] subscribe to this in order to test it uh
[22:10] I'm just going to go to the um basic
[22:12] plan and I'm going to pay money per
[22:14] month that probably seems the simplest
[22:15] way to do so okay and I just ran through
[22:17] the payment let's actually head over
[22:18] here and let's just run a test using my
[22:20] website URL we're going to test this
[22:22] endpoint now and now this actually going
[22:24] to go through Rapid API it's going to
[22:25] spin up the server and then it's going
[22:27] to send it and what we see here is we
[22:29] have multiple fields that Rapid apis or
[22:31] this particular scraper gives us let me
[22:32] just make this easier for you all to see
[22:34] we have a text content field with all of
[22:35] the content of the website which is cool
[22:37] this is basically what I did earlier um
[22:39] but instead of me having to formulate
[22:40] this request try and parse it and try
[22:42] and use AI tokens what I did is I sent
[22:43] the request to uh rapid API and did it
[22:45] all for me then we also have an HTML
[22:47] content
[22:48] field I think we have one more here
[22:51] scroll all the way down to the bottom as
[22:53] you can see there is a ton of HTML um
[22:55] and then we also have a list of all of
[22:56] the images on the website which is very
[22:58] very cool and easily formatted again
[23:00] something that I tried to do manually
[23:01] using AI but now you know we have
[23:03] everything in that nice absolute URL
[23:04] format um and then if they find any
[23:06] social media links I don't believe um
[23:09] there were more than Twitter but um if
[23:11] they find anything that's at their
[23:12] Twitter Instagram whatever then we have
[23:14] the link right over here it looks like
[23:15] they even give you the scraping time and
[23:17] if they scrape emails or phone numbers
[23:18] um they'll be there as well so I mean
[23:20] rapid AP is obviously fantastic this is
[23:21] a high throughput sort of thing and why
[23:23] don't we actually run through what this
[23:25] would look like if we were to run a curl
[23:26] request you see how it's automatically
[23:29] just formatting it as curl well that
[23:30] just means we just jump back here
[23:32] connect this to my HTTP request module
[23:35] click import curl paste it in like this
[23:38] import and it's actually going to go
[23:39] through and it's going to automatically
[23:41] map all these fields for me right query
[23:43] parameter URL left click. beautiful um
[23:45] API key x-raid API host here's the host
[23:49] here's the name of the API key here's
[23:51] everything we need well I can actually
[23:52] just recreate this request now inside of
[23:54] NN as opposed to being on rapid API and
[23:57] then I have all the data accessible to
[23:58] me here how cool is that so we can do
[24:01] this for any any major website really um
[24:03] you know there are a lot of specific
[24:06] bespoke scrapers obviously which um I
[24:07] don't know if you wanted to scrape uh
[24:09] let's go back to Discovery if you wanted
[24:11] to scrape like Instagram or something
[24:14] you could scrape um Instagram uh you
[24:16] could do like Facebook scraping you
[24:18] could scrape these large giants that are
[24:20] quite difficult to do So Meta ad Library
[24:22] Facebook ad scraper and depending on the
[24:24] plan that you're at it might be more
[24:25] cost- effective for you to sign up to
[24:27] some sort of monthly recurring thing
[24:28] rather than just pay two cents every
[24:29] single time you make one of these
[24:30] requests you just kind of got to do that
[24:32] determination yourself right like if
[24:33] you're scraping uh I don't know 50 every
[24:36] day or 100 every day or something might
[24:38] be a dollar or two a day which is
[24:39] reasonable but maybe if you want to
[24:41] scrape like 5,000 doing it the way that
[24:42] I was doing it a moment ago might might
[24:43] be infusible the next way to scrape
[24:45] websites in nadn is using the web
[24:47] scraper Chrome extension and then tying
[24:49] that to a cloud service that delivers
[24:51] the data that you just created using
[24:53] their no code tool um in nicely bundled
[24:55] formats it's called Cloud sync as of the
[24:57] time of this recording I think they
[24:58] changed the name a couple of times but
[25:00] um that's where we're at here is the
[25:02] name of the service web scraper here is
[25:05] their website essentially what happens
[25:07] is you install a little Chrome plugin
[25:08] which I'll show you guys how to do then
[25:10] you select the fields that you want
[25:11] scraped in various data formats and then
[25:14] what you do is it handles JavaScript
[25:16] sites Dynamic sites all that fun stuff
[25:19] and then you can um export that
[25:22] data as a cloud run to then send back
[25:28] sorry big sneeze to then send back to
[25:31] some API or some service um and then
[25:33] automatically do parsing and stuff like
[25:34] that so very cool I'm going to show you
[25:36] guys what that looks like um this is
[25:37] sort of a more customized way to build
[25:38] the stuff but I've seen a lot of people
[25:40] do this with naden um so we're going to
[25:42] run through what it looks like so first
[25:43] thing I'm going to want to do is I'm
[25:45] going to want to let's just go Cloud
[25:47] login or sorry um start free 7-Day trial
[25:51] as you can see you know there's a free
[25:52] browser extension here if you wanted to
[25:54] do uh I don't know like highs scale
[25:56] stuff you'd choose probably their
[25:58] project um endpoint where we Sorry
[26:01] project plan where we have 5,000 URL
[26:03] credits we can run a bunch of tasks in
[26:05] parallel we could scrape Dynamic sites
[26:07] JavaScript sites we have a bunch of
[26:08] different export options then we can
[26:09] also just connect it directly to all of
[26:11] these um what I'm going to do just
[26:12] because I want this to kind of work as a
[26:14] first go is I'm just going to sign up to
[26:15] a free Tri here beautiful just created
[26:17] my account just go left click give it a
[26:21] phone number we'll go left click. a
[26:24] we're going to go I don't know academic
[26:26] records needed per month we'll go 0 to
[26:28] th000 length of the project uh I don't
[26:30] know let's go two to 3
[26:31] months okay great so now we can import
[26:34] and run our own site map or we can use a
[26:36] premade Community sit map um what I'm
[26:38] going to do is I'm just going to import
[26:39] this we're then going to get the Chrome
[26:41] extension web
[26:43] scraper let me add that extension and
[26:45] it's going to download it do all that
[26:47] fun stuff beautiful so now we have it
[26:50] right over here I'm just going to pin it
[26:52] to my browser to make my life easier go
[26:54] to left click. a open up this puppy now
[26:57] there's a bunch of like tutorials and
[26:58] how to use this stuff um that's not that
[26:59] big of a deal but basically the thing
[27:01] you need is you need to hold command
[27:03] plus option plus I to open up your
[27:04] developer tools and you'll just find it
[27:05] on the in my case the far right so
[27:07] command option I that'll open up Dev
[27:10] tools you see all the way on the right
[27:11] hand side here I have a couple other
[27:12] things like make and and cookie editor
[27:14] but all the way on the right hand side
[27:15] here we have this web scraper thing um
[27:18] so we got what you're going to want to
[27:19] do first you're going to want to create
[27:20] a site map for the resource that you're
[27:21] going to want to scrape I'm just going
[27:22] to call it left click and I just want to
[27:24] scrape left click. okay once we have our
[27:27] sitemap if I just give a quick little
[27:28] click I can then add a new selector and
[27:30] the really cool thing about this web
[27:32] scraper is um if I just zoom out a
[27:34] little bit here uh what you can do is
[27:35] you can you can select the elements on
[27:37] the website that you want scraped so for
[27:39] instance it's a very quick and easy way
[27:40] to do this if you think about it is like
[27:42] just to show you guys an example
[27:43] structure data is uh sort of like an
[27:45] e-commerce application let's say you
[27:46] have like the title of a product and you
[27:47] have like I don't know the the
[27:49] description of a product so on my
[27:50] website really quick and easy way to do
[27:51] this is let's just call this
[27:53] products and it's a type text what I'm
[27:56] going to do is I'm going to click select
[27:58] then I'll just click on this I'll click
[28:01] on this as well and as you see it'll
[28:03] automatically find all of the headings
[28:06] that I'm looking for so that's products
[28:09] we are going to then click data I'm
[28:10] going to click done selecting data
[28:12] preview as you can see it only selected
[28:14] one of them the very first so what we're
[28:15] going to want to do is go multiple and
[28:17] now if I data preview we get all of the
[28:19] headings which is very cool so now we
[28:21] have a basically like a list of headings
[28:23] um from here I'm going to save this
[28:25] selector I'm add a new one let's go
[28:27] product
[28:28] descriptions and then going to select
[28:31] this this it'll select all of them I'll
[28:34] go multiple data preview just to make
[28:36] sure that it looks good I'm getting no
[28:37] data extracted here oh sorry I didn't
[28:39] actually select the um didn't actually
[28:41] finish it now we're getting product
[28:42] descriptions that's pretty cool um this
[28:44] is me doing this sort of like one at a
[28:46] time you can also um group The selectors
[28:49] there you go it's actually um offered to
[28:51] group it for me so we can uh group this
[28:54] into one object with products and then
[28:56] product descriptions so it's automatic
[28:58] group it now we have wrapper for
[28:59] products and products descriptions then
[29:01] we have products and product
[29:02] descriptions buried underneath we could
[29:04] go as far as we want with this but
[29:05] basically what I'm what I'm trying to
[29:06] show you guys is very simple and easy
[29:08] just drag your mouse over the specific
[29:10] thing you want if you select more than
[29:12] one it'll automatically find all of them
[29:13] on the website which is really cool okay
[29:15] great once we have this um what I can do
[29:17] is I can actually go export sitemap so
[29:20] now I have all of the code on the
[29:21] website that actually goes and finds it
[29:23] for me then I can paste this in here
[29:26] I'll just call this left click scraper
[29:28] and I'm going to import this to my cloud
[29:31] scraper uh I think I'm running into oh
[29:34] sorry I don't think we can do a space
[29:36] there my bad just call it left click and
[29:38] now what we can do is we can actually
[29:39] just like run a server instance that
[29:41] goes out and then scrapes this for us
[29:42] okay so I'm going to click scrape it
[29:45] looks like I need to verify my email so
[29:47] just make sure you do that before you
[29:48] try and get ahead of yourself like I
[29:52] was okay looks like we just verified the
[29:54] email let's head back over here refresh
[29:57] then scrape we've now scheduled a
[29:59] scraping job for this sitemap scheduling
[30:02] you know in their lingo just means that
[30:03] it's now part of their big long queue of
[30:05] thousands of other things that they're
[30:06] probably scraping through their server
[30:07] and that's fine okay I just gave this a
[30:09] refresh and as we see we have now
[30:10] finished said scraping job we have all
[30:12] of the data available to us using their
[30:14] UI but now that we've gone through this
[30:16] process of you know building out this
[30:18] this thing um how do we actually take
[30:20] that and then use it in our nadn flows
[30:22] so variety of ways um if you wanted to
[30:24] connect this let's say to specific
[30:25] service like Dropbox um Google
[30:28] you know dump anow or something Google
[30:30] Drive I'd recommend just doing it
[30:31] directly through their integration it's
[30:32] just a lot easier to get the data there
[30:34] and then you can just connect it to n
[30:36] and watch the data as it comes in or
[30:37] something you can also use the web
[30:39] scraper API uh this is pretty neat
[30:42] because you can you know that's what
[30:43] we're going to end up using it was
[30:45] pretty neat because you can uh like
[30:47] schedule jobs you can send jobs you can
[30:48] do basically everything just through the
[30:50] NN interface and then we can just
[30:52] retrieve the data afterwards which is
[30:53] pretty neat um this is basically what
[30:55] you end up getting you end up with
[30:57] scraping job ID status sitemap all this
[30:59] fun stuff and then we can set like a web
[31:02] hook URL where we we receive the request
[31:05] so um let me check we need a scraping
[31:07] for testing you need a scraping job that
[31:09] has already been finished I think our
[31:10] scraping job has already been finished
[31:12] I'm just going to go htps uh back to my
[31:15] n8n flow I'm actually going to build an
[31:18] n8n web hook give that a click I'm not
[31:22] going to have any authentication let me
[31:24] just turn all this off basically what we
[31:26] want is we we want to use this as our
[31:27] test
[31:29] event we're going to go back to the
[31:32] API paste this in
[31:35] save and I'm just going to want to give
[31:36] it a test endpoint here so
[31:39] test looks like um the push notification
[31:42] was failed the reason why is because
[31:44] it's saying this web Hook is not
[31:45] registered for post request did you mean
[31:47] to make a get request beautiful thank
[31:48] you naden we absolutely did so I'm going
[31:50] to stop listening change your HTTP HTTP
[31:53] method here to post there's basically
[31:54] two ways to call a website and this is
[31:55] one of them I'm going to listen for test
[31:57] events go back here and then
[31:59] retest awesome looks like we've now
[32:01] triggered the beginning of our workflow
[32:03] using this data let's see what sort of
[32:05] information was in
[32:06] it okay great we have the scraping job
[32:09] ID the status execution
[32:11] mode okay great so we basically have
[32:13] everything we need now to set up a flow
[32:16] where we can schedule something in this
[32:17] web scraper service that maybe monitors
[32:19] some I don't know list of e-commerce
[32:21] product or something every 12 hours and
[32:23] then we can set up a web hook in NN that
[32:24] will catch the notification get the
[32:27] update now we can do is we can ping um
[32:30] we can ping the web scraping API which
[32:32] I'll show you to set up in a second to
[32:34] request the data from that particular
[32:35] scraping run and from here we can take
[32:38] that data do whatever the heck we want
[32:39] with it but obviously let me show you an
[32:41] example of what the the actual data
[32:42] looks like so we just got the data from
[32:44] web hook let's set up an HTTP request to
[32:48] their API now where we basically get the
[32:50] ID of the thing and then we can call uh
[32:53] we can call that back so got my API
[32:55] token over here I'm going head over to
[32:56] their API documentation first okay and
[32:59] then what we want to do is download
[33:00] these scrape data in CSV format at least
[33:03] in my case I imagine most of you guys
[33:04] are going to add this to a spreadsheet
[33:05] or whatever um you can very easily do
[33:08] whatever you want there's also a Json
[33:09] format endpoint here um but let's just
[33:12] do CSV for Simplicity so I've already
[33:14] gone ahead and I've gotten the method
[33:16] which was a get request so I've added
[33:18] that up here the URL was this over here
[33:21] with the scraping job ID and then your
[33:23] API token there so what I've done is
[33:25] I've grabbed the API token and the
[33:27] scraping job ID I mean I hardcoded it in
[33:28] here just while I was doing the testing
[33:30] let's actually make this Dynamic now
[33:33] drag the scraping job ID right over here
[33:36] voila and then the API token if you guys
[33:38] remember back here on the API page you
[33:41] have your access to API token so just
[33:42] copy that
[33:43] over uh great and now if I run this I'm
[33:47] actually selecting that specific job
[33:49] then from here we have all the data that
[33:50] we just scraped as you can see there's
[33:51] like a uh the way that CSV Works
[33:54] actually let me just copy this over here
[33:55] I just wanted to give this to you guys
[33:56] as an example of a different data type
[33:58] but maybe some people here aren't really
[34:00] familiar with it basically the way that
[34:01] it works is if I just paste this into
[34:02] like a Google sheet you see how it looks
[34:04] like this what what you can do is if you
[34:05] just um split the text to columns you
[34:08] kind of see
[34:09] how kind of see how there's like these
[34:11] four pettings there's web scraper order
[34:13] web scraper startup products and product
[34:15] descriptions right I'm imagine scraping
[34:17] this for some lead genen applica sorry
[34:19] some some e-commerce application list of
[34:21] products here product descriptions maybe
[34:23] product prices maybe product whatever
[34:24] the heck you want um so yeah you can you
[34:27] can put in like a number of formats and
[34:29] I just wanted to give you guys an
[34:29] example what that looks like the next
[34:31] way to scrape websites in naden is using
[34:33] appify if you guys are no strangers to
[34:35] this channel you know that I do appify
[34:37] all the time and I talk about them all
[34:38] the time because I think that they're
[34:39] just a great service um they've now
[34:41] given me a 30% discount where anybody
[34:44] can use it for I was initially under the
[34:46] impression it was lifetime I think it's
[34:47] three months so you probably get 30% off
[34:48] your first three months just check the
[34:50] um description if you want that but Cent
[34:52] how appify is is it is a Marketplace
[34:54] very similar to Rapid API um although
[34:56] extraordinarily well Main ained and they
[34:58] also have a ton of guides set up to help
[35:00] you get you know up and running with
[35:02] scraping any sort of application so just
[35:05] as we had earlier we have Instagram
[35:06] scrapers we have Tik Tok scrapers we
[35:09] have email scrapers we have map scrapers
[35:12] Google Maps we could do I don't know
[35:15] Twitter
[35:15] scrapers uh medium scrapers right
[35:18] basically any any service out there that
[35:22] has this Dynamic aspect to it that's not
[35:24] a simple HTTP request you can make you
[35:25] could scrape it using ampify and then
[35:27] obviously you you have things too like
[35:28] just like basic website crawlers you can
[35:30] generate screenshots of sites I mean
[35:32] there's just there's so many things let
[35:33] me walk you guys through what it looks
[35:35] like now in my case I'm not actually
[35:37] going to sign up to appify because I
[35:38] have like 400 accounts but trust me when
[35:40] I say it is a very easy and simple
[35:41] process you go to app ay.com you go get
[35:45] started you put in your email and your
[35:47] password they'll give you $5 in free
[35:50] platform credit you don't need any
[35:51] credit card and you can just get up and
[35:53] running and start using this for
[35:54] yourself super easily then the second
[35:56] that you have all that you'll be Creed
[35:57] with this screen it is a console screen
[36:00] don't be concerned when you see this um
[36:03] you know this is super simple and and
[36:05] easy and and not a big deal this is one
[36:06] of my free accounts um so I just wanted
[36:08] to show you guys what you can do with a
[36:09] free account uh but from here what you
[36:12] do is you go to the store and as you can
[36:13] see I'm just dark mode all this is the
[36:15] same thing we were just looking at
[36:16] before and then um we're just going to
[36:17] run a test on the thing that we want to
[36:19] scrape okay so what I'm going to want to
[36:21] do is for the purposes of this I'm now
[36:23] going to do something different from
[36:25] what I was doing before like which was
[36:26] just left click over and over and over I
[36:27] think that kind of gets boring what I'm
[36:29] going to do is I'm going to scrape
[36:30] Instagram posts okay so what I'm going
[36:32] to do is I'm going to feed in a name
[36:34] nickf this is just my um
[36:38] Instagram uh which almost hit 10K in God
[36:41] like 15 days or something like that but
[36:42] I'm going to feed in my Instagram here
[36:44] and then I'm just going to grab like I
[36:45] don't know the last 10 posts okay save
[36:48] and start this is now going to run an
[36:49] actor actor is just their term for
[36:51] scraper which will go out it'll extract
[36:54] data from my Nick surve Instagram and as
[36:57] you can see will get a ton of fields
[36:58] caption owner full name owner Instagram
[37:00] URL comments count first comment likes
[37:02] count timestamp query tag we get
[37:05] everything from these guys which is
[37:06] really cool this might take you know 30
[37:08] 40 50 seconds we are spinning up a
[37:10] server in real time every time you do
[37:12] this as you see in bottom left hand
[37:13] corner there's a little memory tab which
[37:14] shows that we are legitimately running a
[37:16] server with one gigabyte of memory right
[37:18] now so generally my recommendation when
[37:19] you use appify is not to use it for
[37:21] oneoff requests like this feed in 5 to
[37:23] 10 15 20 Instagram Pages uh but you know
[37:26] I just got the back and voila we we have
[37:28] it it's right in front of us we have all
[37:29] of the data of that person's Instagram
[37:32] profile so you can see it's quite
[37:33] scalable in that way um so the question
[37:35] is obviously how do you get this in NN
[37:37] well appify has a really easy to use um
[37:39] API which I like
[37:40] doing all you have is if we wanted to
[37:44] get the uh let's see get data set items
[37:47] okay all I'm going to do is I'm just
[37:49] going to copy
[37:50] this go back here and then connect this
[37:53] to an HTTP request
[37:55] module as you could see we have this big
[37:58] long field here with my API appify API
[38:01] token and this specific data set that
[38:03] I'm looking for I'll show you how to get
[38:04] it dynamically but I just wanted to like
[38:06] allow you to see how to get data in
[38:07] naden really quickly now if we go to the
[38:10] schem of view we can see we legitimately
[38:12] we we already have all of the data that
[38:13] we we had from appify a second ago okay
[38:16] super easy and quick and simple to get
[38:17] up and running um we have the input URL
[38:19] field the ID field the type the short
[38:21] code caption now this is Instagram um
[38:25] every looks like we have some comments I
[38:27] don't have any style how do I create my
[38:29] man you just got to fake it till you
[38:31] make it I don't have any style either
[38:33] just some nerd in my mom's basement uh
[38:36] yeah so you you can scrape any resource
[38:38] you want here um obviously I was
[38:40] scraping an Instagram resource but like
[38:41] if you were scraping something else
[38:43] there'd be no change to this at all no
[38:45] change whatsoever now uh basically what
[38:47] we need in order to make this Dynamic
[38:49] basically make us able to run something
[38:51] in appify and then get it in NN so we
[38:54] need to set up an integration so just
[38:55] head over to this tab set up integration
[38:57] and then all you want to do is you just
[38:59] want to do web hook send an HTTP post
[39:02] web Hook when a specific actor event
[39:03] happens the actor event that we're going
[39:05] to want is basically when the run is
[39:07] succeeded the URL we're going to want to
[39:09] send this to if you think about it we
[39:11] just actually make another web hook
[39:13] request here web
[39:15] hook the URL we're going to want to send
[39:17] it to is going to be this test URL over
[39:21] here now I'm just going to delete all
[39:22] the header off stuff here because um it
[39:24] just uh complicates it especially for
[39:26] beginners um but we're going to copy
[39:28] this over head back over here paste in
[39:30] this URL and then let me see this is a
[39:32] post request I think I don't actually
[39:34] remember so we're going to have to
[39:35] double check I think it's a post
[39:37] request yeah and then what I'm going to
[39:39] do is I'm going to listen for a test
[39:40] event run the test web
[39:43] hook so we're listening we're making a
[39:45] get request okay so the fact that it
[39:47] hasn't connected yet probably tells me
[39:48] it's a post request so let's move over
[39:50] here move this down to post now let's
[39:52] listen to a test event let's run this
[39:55] puppy one more time so we just
[39:57] dispatched it and yeah the post request
[39:59] succeeded and what did we get we got
[40:00] tons of information we got a body with a
[40:02] user ID created at event data joke right
[40:05] looks like when you test something out
[40:06] they just send you a joke about how
[40:08] Chuck nurse can sketi a cow in two
[40:10] minutes have you ever heard of the word
[40:11] sketi before this moment I haven't I
[40:13] want to be known for my ability to sketi
[40:16] we'll go Instagram website
[40:19] scraper okay and now if we go back here
[40:22] right we're now listening for a test
[40:24] event so I'm going to listen for this
[40:26] test event I'm going to run the same
[40:27] scraper again maybe we'll make it five
[40:28] posts per profile just to make it a
[40:29] little
[40:30] faster and um once this is done what
[40:33] it's going to do is it's going to send a
[40:35] record of all the information we need to
[40:37] get the data over to Ann we're going to
[40:40] catch that information and then we're
[40:42] going to use it to query the the the
[40:44] database basically that it created for
[40:45] that particular Instagram run which will
[40:48] then enable us to do whatever the heck
[40:49] we want with it so it's now starting to
[40:51] crawl as we see here we had five
[40:53] requests so it should be able to do this
[40:54] in like the next 5 seconds or so okay
[40:56] and once that's done we now have an
[40:57] actor succeeded event um and then we
[41:00] have uh let me see the data that we want
[41:04] would be the default data set ID down
[41:06] over here so if we just go to that next
[41:08] HTTP request node what I can do is I can
[41:11] feed that in as a variable right
[41:14] here let going to a default data set
[41:18] ID drag that in between these two little
[41:23] lines and now we can test that step with
[41:25] actual live data now we have everything
[41:26] that we need
[41:27] so I don't know maybe now you want to
[41:29] feed this into Ai and you want to have
[41:30] ai tell you something about the last
[41:32] five posts tell you wow those last five
[41:34] posts were amazing Nick I loved the
[41:37] specifically the one on Korea and I just
[41:39] wanted to send you over some quick
[41:40] assets to help you out right you can now
[41:42] do super Dynamic and structured Outreach
[41:46] you could take that data and use it to
[41:48] like draft up your own post I mean the
[41:50] options are ultimately unlimited that's
[41:52] why I love appify so much the sixth way
[41:54] to scrape websites with NN is data for
[41:57] Co this is another thirdparty service
[41:59] but it's a very high quality one that's
[42:00] specifically geared towards search
[42:02] engine optimization requests you guys
[42:04] haven't seen data for SEO before it's
[42:06] basically this big API stack that allows
[42:08] you to do things like automatically
[42:10] query a service maybe some e-commerce
[42:12] website or some content website and then
[42:14] like extract things in nicely structured
[42:16] formatting um again specifically for SEO
[42:19] purposes tons of apis here as well I
[42:21] mean a lot of these services are now
[42:22] going towards like more Marketplace
[42:24] style stuff but just to give you guys an
[42:26] example you could like Google really
[42:28] quickly to scrape a big list of Google
[42:30] search results for a term and then you
[42:31] could like feed that into one of any of
[42:33] the other scrapers that we set up here
[42:34] to get data on stuff you could go Google
[42:36] Images Google Maps you could do Bing BYO
[42:39] YouTube Google's uh their own data set
[42:41] feature I don't really know what that is
[42:42] but I imagine it's pretty cool uh and
[42:44] then you can you can take this data and
[42:45] do really fun stuff with it so I'm just
[42:47] going to click try for free over here in
[42:48] the top right hand corner show you guys
[42:50] what that looks
[42:52] like and as you see here um I signed in
[42:55] to data for SEO to my own account looks
[42:57] like I have 38 million bajillion
[43:00] dollars um but obviously you'd have to
[43:03] go through the rig Rolla creating your
[43:04] own account so why don't actually just
[43:06] do that with you and then I'll just use
[43:07] that account that is 38 million
[43:08] bajillion dollars we'll click try for
[43:11] free we'll go Nikki
[43:15] Wiki uh let's use a different email I
[43:17] need a business email huh that's
[43:21] unfortunate okay I do agree to the terms
[43:23] of use
[43:25] absolutely uh bicycle
[43:28] is that a bicycle that's not a
[43:32] bicycle what does it mean when I can't
[43:35] answer these does it mean that I'm a
[43:38] robot if you look at some of my posts
[43:40] some of my comments people would
[43:42] absolutely say yes it means that that
[43:45] you're a robot um I don't know why
[43:47] people keep saying stuff like dude Nick
[43:49] nice AI Avatar bro but I'm it's not an
[43:52] AI Avatar it's not an AI Avatar at all
[43:55] it's actually just me okay anyway so I
[43:57] need to activate my account doesn't look
[43:59] like it allows you to feed in the code
[44:01] here so I'm just going to feed it in
[44:02] myself uh it's obviously you're getting
[44:04] a lot of spammers hence
[44:05] this um bicycle stuff I don't know why
[44:09] the code isn't working here let me just
[44:12] copy this link address paste it in here
[44:15] instead there you go okay great so now
[44:17] you can sign
[44:19] in and once you're in you got also um
[44:23] they're actually really big on on
[44:24] bicycles they're training um a model to
[44:26] convert all ads on planet Earth into
[44:28] bicycles they'll actually give you a
[44:29] dollar worth of API access uh credits
[44:32] which is pretty cool um I'm not going to
[44:34] do that I'm just going to go over to
[44:35] mine which is$ 38 million bajillion
[44:36] dollars with 99,999 estimated days to go
[44:40] um and yeah let's actually run through
[44:41] this the first thing that I recommend
[44:42] you do is go over to playground on the
[44:43] Le hand side there's all of their
[44:45] different API endpoints that you can
[44:47] call um what I'm going to do is I'll
[44:48] just go to serp for now just to show you
[44:50] that you could scrape Google with this
[44:51] pretty easily so maybe I'm in the UK and
[44:54] I want to scrape um let me see
[44:58] a keyword ni arrive okay then I'm going
[45:01] to send a request to this
[45:03] API there's there's a bunch of other
[45:05] terms here that are going to make more
[45:06] sense if you're a SEO person um but now
[45:09] we receive as output a structured object
[45:12] with a ton of stuff right we have um the
[45:15] first result here it's like an organic
[45:16] one with some big URL a bunch of chips
[45:20] um I'm I have like a Knowledge Graph
[45:22] profile which is cool apparently it
[45:23] finds it says I'm a freelance writer um
[45:27] you know we have a bun bunch of data
[45:28] here bunch of data you know you can use
[45:30] this to get URLs of specific things and
[45:32] then with the URLs you can then feed
[45:34] that into scrapers um that do more like
[45:36] I talked about earlier maybe appify or
[45:38] maybe rap API maybe fir crawl so a lot
[45:42] of options here to like create your own
[45:43] very complex
[45:45] flows you can do other stuff as well um
[45:48] you grab a bunch of keyword data so
[45:50] maybe you wanted to find a keyword and
[45:53] maybe again it's Nicks or location you
[45:55] want let's do United States that'll
[45:57] probably be
[45:58] better language um I'm just not going to
[46:01] select an language and then I'll do a
[46:03] request so now it's going to find us um
[46:06] a bunch of search volume related stuff
[46:08] so I don't actually know how many people
[46:10] are searching for me in 2025 apparently
[46:13] 390 is this per month H wonder if it's
[46:16] per month per day that's interesting uh
[46:20] I don't really know why they break it
[46:21] down by like the month date yeah looks
[46:24] like it's 390 per month so to the 390
[46:26] people that are Googling me who are you
[46:28] and what do you want I'm just kidding um
[46:31] you can do things like you could find
[46:32] back links so you could find links um
[46:35] for I believe you feed in a website URL
[46:38] and then it finds back links to that
[46:40] website so this is you technically now
[46:41] scraping a bunch of other websites
[46:43] looking for links to the specific
[46:45] resource that you have that's kind of
[46:49] neat it looks like that found it
[46:51] basically immediately which is really
[46:52] really
[46:54] cool and it looks like they're referring
[46:56] top level links that are Dooms BG bgs
[46:59] would be interesting I wonder where
[47:00] that's coming from um there's a Content
[47:03] generation API playground so you could
[47:05] you know feed in some text and then have
[47:07] it generate other stuff but I think
[47:08] we're kind of getting away from um the
[47:10] actual thing that matters which is the
[47:11] scraping of the uh scraping of the
[47:13] websites so yeah lots of stuff lots of
[47:16] stuff for sure now that's all good um
[47:18] but let's actually turn this into an API
[47:20] call if we head over to the API of do
[47:22] data for SEO so in my case docs. datafor
[47:24] seo.com V3 _ page SL contentor parsing
[47:30] live that's what I'm I'm curious about
[47:32] you'll see that we have a post request
[47:33] that we need to send to this URL um well
[47:36] I have a curl just like this which I can
[47:39] feed into um an API request that's what
[47:42] I'm going to do so I'm going to go back
[47:43] over here and I'm just going to import
[47:45] this curl
[47:47] import and it's going to go through and
[47:49] it's basically going to um parse out all
[47:52] these fields that I'm interested in with
[47:53] the URL which I'll go htps
[47:57] left click. AI um and then we have sort
[48:00] of like a gacha here that a lot of
[48:01] people don't understand this is the um
[48:03] authorization the authorization is a
[48:05] little bit different from most of the
[48:06] easy authorizations we've had so far we
[48:07] actually have to convert it um we have
[48:08] to go one one more step basically to
[48:10] make this work if I check out the let's
[48:15] see um
[48:17] authorization
[48:19] here what we need is we need to um get
[48:23] the login and then the P so this is your
[48:26] username and then your password then we
[48:28] have to Hash it or not hash it but we
[48:29] have to convert it into something called
[48:30] base 64 um this is just how they do
[48:33] their API key stuff I guess it's kind of
[48:35] annoying but it's just part and parcel
[48:37] of working with some apis you're just
[48:38] not always going to have it available to
[48:40] you really easily so I'm just going to
[48:42] go back to data for SEO and then I'm
[48:45] going to grab my credentials okay so
[48:47] what we need to do is we need to base 64
[48:50] encode the username and the password um
[48:52] I'm just going to leave that at what
[48:53] I've done is I've actually gone through
[48:54] and done it in this edit Fields node um
[48:57] basically what you need to do is you
[48:58] need to have your username or your login
[49:02] so maybe this is me searching Nix or
[49:04] have Reddit uh Nick left click. so that
[49:07] might be my username and then my
[49:09] password is What's called the API
[49:10] password you can find that really
[49:11] quickly and easily just by going over
[49:13] here to API access and then API password
[49:16] if you just signed up it'll be visible
[49:17] right here if it's been more than 24
[49:18] hours you actually have to send it by
[49:19] email but anyway so that's um that's
[49:21] where i' get the API password from uh
[49:24] and then once you feed it in over here
[49:26] where you're going to want to do is
[49:27] you're going to want to base 64 encode
[49:28] it like this they just require you to
[49:31] use these creds um or to operate with
[49:35] these creds as base 64 encoded versions
[49:36] Bas 64 is just a way to like translate
[49:38] into a slightly different number format
[49:40] so once you have that you would just
[49:41] feed in the variable right over
[49:43] here Ju Just as follows and then you can
[49:46] make a request to their API and receive
[49:47] data so uh it looks like I was doing
[49:50] their content parsing live you know what
[49:53] I wanted to do is I just wanted to
[49:55] call their endpoint which I think was
[49:58] their like instant
[50:02] Pages this one right over here so it's
[50:04] just V3 uh once you've sorted this out
[50:06] by the way the AP gets like
[50:07] extraordinarily easy to manage you just
[50:09] need to like figure out the
[50:09] authentication from there on out all
[50:11] you're literally doing is just swapping
[50:12] out the requests so you know if you
[50:14] wanted to do instant Pages all I'm doing
[50:16] is pumping that in there I just sent a
[50:19] request and now I receive a bunch of
[50:20] links with different headings and and so
[50:22] on and so forth that's easy the seventh
[50:24] way to scrape websites and Ed end is
[50:25] using a third party application called
[50:27] crawl Bas they're known for their
[50:29] rotating proxies which allow you to send
[50:31] very high volume um API requests so um
[50:33] it's very proxy driven this is their
[50:35] website so it's a scraping platform
[50:37] similar to Rapid API um and uh you know
[50:40] appify they support many of the major
[50:42] websites here and um the reason why
[50:44] they're so good at this is just because
[50:45] they you know as I mentioned they rotate
[50:47] the hell out of these proxies so we're
[50:49] just going to sign up to Tri it free
[50:51] I'll use my business email
[50:54] here and then continue with Emil
[50:57] email we got to add a phone number
[51:00] obviously we're going to do less than a
[51:01] thousand I'm a CTO I don't want to
[51:04] what's the animal right is that an
[51:06] animal yes it's an animal good God beep
[51:10] boop uh we're going to head over to my
[51:14] Gmail and receive this
[51:18] now so we need to confirm my account
[51:20] just going to copy this link address
[51:22] that I can do this in one
[51:23] page awesome we should be good to log in
[51:26] so that's what's
[51:27] happening we need to select the animal
[51:29] again just doesn't it doesn't believe
[51:31] really just doesn't
[51:34] believe okay great so now we have a
[51:36] crawling API smart proxy thing if you
[51:39] guys want to run like uh I don't know
[51:41] use in apps that have a proxy field
[51:43] specifically I'm just going to keep
[51:44] things simple we're doing this in n8n so
[51:46] we're going to go crawl base API we have
[51:47] a th000 free crawls remaining very first
[51:50] thing we're going to want to do is just
[51:51] click start crawling now just to get up
[51:52] and running with the
[51:54] API um and as you see here the these
[51:56] guys have probably one of the simplest
[51:57] apis possible all API URLs start with
[52:00] the folling base part click and then all
[52:02] you need to do in order to make an API
[52:04] call is run the following sort of line
[52:06] so this is a curl request obviously
[52:08] we're in n8n and one of the value
[52:11] valuable parts of NN is we can just
[52:12] import a COR request so well I'm going
[52:14] to import it as you can see here we have
[52:17] a token field then we just have the URL
[52:19] field of the place we want to crawl so
[52:21] I'm going to do left click. for now um I
[52:24] don't know if this token field was
[52:26] actually my real token I don't believe
[52:27] so maybe we'll give it a try maybe it's
[52:29] like a test token or
[52:31] something so I'm now running this and it
[52:34] looks like we just received a bunch of
[52:36] very spooky data I don't like the spooky
[52:38] data no spooky data for
[52:41] us um sometimes spooky data like
[52:47] this H this seems kind of weird to me
[52:49] actually just give me one second to make
[52:51] sure that's right we are receiving a
[52:53] data parameter back which is nice but
[52:56] yeah something about this is a little
[52:57] bit spooky um was it a get request or
[53:00] was it a post request no I guess it's a
[53:01] get request
[53:03] strange very very
[53:06] strange okay anyway they give you two
[53:09] types of tokens here um this is why I'm
[53:11] talking about it to begin with I'm also
[53:12] because I just used it before for a
[53:13] couple of applications and I found it
[53:15] very easy they give you a normal token
[53:16] and they give you a JavaScript um token
[53:19] as well so the reason why that's
[53:20] valuable is because if you're scraping
[53:22] one of these websites I talked about
[53:23] before where when you send a simple HTTP
[53:25] request nothing pops up like this is the
[53:27] this is the purpose of this you actually
[53:28] feed in a JavaScript token um when you
[53:31] use the JavaScript token it'll
[53:32] automatically launch a browser instance
[53:34] inside of um craw base for you so
[53:37] instead of you getting just like that
[53:39] empty thing back that I mentioned I'm
[53:40] you're actually going to get uh you're
[53:42] going to get like a JavaScript version
[53:44] of the website where somebody went on
[53:46] the website it loaded really briefly and
[53:48] then they grabbed the code
[53:50] afterwards so yeah we have some some API
[53:52] call stuff over here um this one's just
[53:54] using Amazon this is pretty interesting
[53:56] so I might actually give that a a go
[53:58] just to give you guys an example of said
[54:00] Amazon
[54:02] scrape uh let's just go
[54:09] www.amazon.com oh right Amazon might be
[54:11] JavaScript actually so maybe we give
[54:13] that a go no it looks like we um we got
[54:15] the data from from Amazon which is
[54:16] pretty cool if you feed that into the
[54:18] markdown converter like we had before
[54:20] it's going to feed in the HTML here pump
[54:23] it into a data
[54:24] key we've now converted this
[54:28] into uh this is very long let's go
[54:30] tabular we've now converted this into
[54:32] markdown which is cool and this is
[54:34] pretty long right obviously has all of
[54:36] the images and has all of the
[54:38] information on the site which is cool
[54:39] and then we can feed it into open AI
[54:41] like I did
[54:42] before where I message a model and I'm
[54:46] just going to copy um from my uh
[54:48] previous application here to make my
[54:49] life a little bit
[54:50] easier where the heck are you
[54:56] and then we're just going to feed in the
[54:58] code here and then because I didn't feed
[55:02] in this we should now run this we're
[55:06] going to grab data from the site and
[55:08] we're going to try and I mean you know
[55:09] we kind of all know what Amazon is and
[55:11] what it does right so I'm not expecting
[55:13] expecting anything spectacular but it's
[55:15] still going to go it's going to give me
[55:16] all of the text on this Amazon page and
[55:18] then I'm going to get a bunch of list of
[55:19] links basically absolute URLs ideally
[55:22] should play some Jeopardy
[55:25] music or going be able to play um Star
[55:28] Wars music that'd be kind of cool okay
[55:31] we now have a schema with all of the
[55:33] links on the page which is pretty cool
[55:34] we have the plain text website copy we
[55:37] have a on line summary uh you know plain
[55:39] text website copy is a lot longer than
[55:41] this obviously it's just shortening and
[55:42] truncating it for us but yeah very quick
[55:44] and easy way to use crawl base for this
[55:46] now the value in crawl base is not
[55:48] necessarily just to send them to static
[55:49] websites like I talked about it's to use
[55:51] like highly scalable scraping where
[55:55] you're scraping any applications
[55:58] consistently um as you see here the
[56:00] average API response time is between 4
[56:02] to 10 seconds so you you will receive
[56:03] results back pretty quick if you wanted
[56:05] to just send one request or 20 requests
[56:08] every second think about it like 20
[56:10] requests a second times 60 seconds a
[56:12] minute is 1,200 requests times 60
[56:14] minutes and an hour 72,000 requests
[56:16] right um sorry just jumping around the
[56:18] place here you can send 72,000 requests
[56:20] basically an hour which is crazy um and
[56:23] you can do so as quickly and as easily
[56:24] as just like adding an API call like
[56:26] and then it'll automatically distinguish
[56:28] between like a a plain text thing or a
[56:30] JavaScript thing okay the eighth way to
[56:31] scrape data in nadn specifically website
[56:34] resources is octop parse octoparse is
[56:37] very similar to some of the other
[56:38] services that we've talked about um it
[56:40] is a web scraping tool that actually
[56:41] gives you quote unquote free web
[56:43] crawlers and I'm just a fan of their ux
[56:44] I think it's very clean I think the way
[56:46] that they have their signup flow and
[56:47] stuff's really easy so if you made it to
[56:49] this part of the uh tutorial and you
[56:50] have yet to sign up to one of these
[56:52] Services give octoparse um give
[56:54] octoparse your thoughts
[56:56] let's double
[56:58] check that I haven't actually created an
[57:00] account using this no I haven't
[57:01] fantastic so I should be able to jump
[57:03] through and show you guys what this
[57:05] looks like we have a verification code
[57:07] I'm going to paste in if you're not
[57:09] familiar with jumping around and stuff
[57:11] like this um or if you're wondering how
[57:13] I'm jumping around I'm just using a
[57:14] bunch of website
[57:19] hotkeys okay great account is now ready
[57:22] so we can start a free premium trial if
[57:24] you want I think you're going to have to
[57:25] add a card um I don't know if I have
[57:26] enough credits to actually do anything
[57:29] but if I'm not then I'll start that
[57:30] trial in a second what you're going to
[57:32] have to do in order to make this work is
[57:33] you're going to want to have to download
[57:34] you're going to want to download the
[57:36] octoparse desktop app so let's give it a
[57:39] quick and easy go just going to drag
[57:42] this puppy um if you are using something
[57:45] that is not Mac OS you will not have
[57:47] this strange drag and drop feature here
[57:50] once that is done you will have octo
[57:51] parse accessible just open that up yes I
[57:54] want to open this thank you
[57:57] and the cool thing about
[57:59] octoparse um kind of relative to what
[58:03] else you know like the other scraping
[58:05] applications I talked about is this is
[58:07] just running in a desktop app um like
[58:09] kind of in in your
[58:10] computer so like it's cool because it's
[58:13] just easy to get up and running with um
[58:14] and it's also local as opposed to a lot
[58:16] of these other ones which are not so I'm
[58:19] going to Auto log in on my desktop app
[58:21] remember my password beautiful the
[58:22] simplest and easiest way to scrape a s a
[58:24] service is just to pump in the the URL
[58:26] here then click Start and basically
[58:28] what'll happen is um it'll actually
[58:30] launch like an instance of your browser
[58:31] here with this little tool that allow
[58:33] you similarly the web scraping Chrome
[58:35] extension select the elements on the
[58:36] page you want scraped so I don't know
[58:39] maybe I want these logos scraped the
[58:41] second that I tapped one you'll see it
[58:42] automatically found six similar elements
[58:45] so now I'm actually like scraping all of
[58:47] this stuff okay now we have access to
[58:50] this sort of drag and drop or um
[58:52] selector thing similar to what we had
[58:55] before if you click on one of these
[58:56] you'll see it allow you to select all
[58:58] similar
[58:59] Elements which is pretty sweet and then
[59:02] um you can also do things like click
[59:04] elements and so on and so forth extract
[59:06] the text Data here um you can also tie
[59:09] that to other things right so as you see
[59:12] I'm now mapping each of these very
[59:13] similarly to how I was doing before
[59:15] between the first field which is the
[59:17] title of the product and then the second
[59:18] field which is like the field to uh so
[59:21] that's pretty sweet we could do the same
[59:22] thing with a number of things you could
[59:23] extract like the headings and then the
[59:25] values and so on and so on and so forth
[59:27] but I'll kind of leave it there um so
[59:29] once you're done selecting all the
[59:30] elements that you want all you do is you
[59:31] click run and you have a choice between
[59:33] running it on your device versus running
[59:35] it on the cloud so um on the cloud is
[59:38] API supported that's how you're going to
[59:39] get stuff in NM but I just want you guys
[59:40] to know that you can also just run it
[59:41] here you could run it here load up the
[59:43] URL scrape all the things that you want
[59:45] on the specific page you're feeding in
[59:46] and then you can be done with it so I
[59:48] just selected run in the cloud it's now
[59:49] going to open up said Cloud instances as
[59:51] we could see we have this little field
[59:53] where it's running and extracting the
[59:54] data we're now done so I can export this
[59:58] data locally um but I could also do a
[60:00] lot of other stuff which we'll show you
[60:01] in a second so um you can dump this
[60:03] automatically to Google Sheets you could
[60:05] do zapier to connect to Google Sheets do
[60:07] like some sort of web Hook connection
[60:08] export to cloud storage uh similar stuff
[60:10] to the the web scraping Chrome extension
[60:13] um but for now let's just export this as
[60:16] Json give ourselves a little ad Json
[60:19] file here thank
[60:21] you and yeah now we have it locally now
[60:23] in order to connect with the octop par
[60:24] CPI what you're going to have to do is
[60:26] first you get up to request an access
[60:28] token the way that you do this is you
[60:29] send a post request to this URL here and
[60:33] the way that you format it is you need
[60:34] to send your username your password and
[60:38] then have the grantor type as password
[60:40] okay now password obviously just put in
[60:42] whatever your password is don't store it
[60:44] in PL text like I'm doing um with my
[60:45] hypothetical password put it somewhere
[60:47] else and then grab that data and then
[60:49] use it um but the the output of this is
[60:51] we have this big long access token
[60:52] variable which is great after that if I
[60:55] just go back to their API here um once
[60:57] we're here we can actually extract the
[60:58] data that we need so basically the thing
[61:01] that you're going to want is you're
[61:02] going to want um get data by Offset you
[61:04] can also use get non-exported data which
[61:06] is interesting so I think this just like
[61:07] dumps all of the data as not exported um
[61:10] and then sends that over to you I
[61:12] believe but anyway you could also get
[61:14] the data by offset so if I go a get
[61:17] request to open api. octop course.com SL
[61:20] all and then I just send a header with
[61:21] the URL parameter this is a get request
[61:25] uh we're going to send a header with the
[61:27] token so
[61:29] authorization
[61:30] Bearer and then feed in the access token
[61:33] here just make sure that this is just
[61:35] one space no it's
[61:37] two if I feed this in um it's saying
[61:41] that it's a bad request let me just
[61:42] triple check why I think we need three
[61:46] Fields yeah I think we need three Fields
[61:49] actually my bad um we need uh this get
[61:53] request then we need the authorization
[61:56] header like I talked about then we need
[61:58] three Fields task ID yeah right
[62:00] obviously we need to feed in the task ID
[62:01] so you need task ID offset or size so um
[62:05] we'll feed this in as query parameters
[62:07] here so send query parameters the first
[62:10] value was task ID second one was
[62:14] offset and uh offset is no Capital the
[62:18] third was size offset's going to be zero
[62:20] size going to be I don't know let's just
[62:22] do 1,000 and what we need now is we need
[62:24] the task ID of the specific run that we
[62:26] just finished oh in order to get the
[62:27] task list you head over to task list top
[62:29] right hand corner here task ID API so we
[62:32] now have access to this so if we go back
[62:35] to our NN instance we could feed that in
[62:37] here by test the step you'll see that we
[62:40] now have all the data that we just asked
[62:42] for earlier so a variety of ways to do
[62:44] this um in practice like octop par
[62:45] allows you to schedule runs you could
[62:47] schedule them um using their you know
[62:49] whatever it is uh uh like cloud service
[62:52] you could use it to scrape I don't know
[62:54] Twitter uh they have a variety of like
[62:56] other scrapers um that you can check out
[62:58] just heading over
[63:00] to this new here uh if we just go sorry
[63:03] go down to templates um there's a
[63:05] variety of other ways to scrape Google
[63:06] job scraper glass door scraper super
[63:08] Pages scraper you could schedule these
[63:10] right and then what you can do in na is
[63:11] you can just query it once a day grab
[63:13] all the data like I showed you how to do
[63:14] a moment ago dump that into some sheet
[63:17] octoparse is pretty cool it's like more
[63:18] of like an industrial Enterprise level
[63:20] application um to be honest so there
[63:23] might be some gotas if you're not super
[63:24] familiar with working with like desktop
[63:27] apps and stuff but I I like the idea
[63:28] that you can also just scrape locally
[63:30] which is pretty sweet and the last of
[63:31] our nine best ways to scrape websites in
[63:33] nadn is browserless now browserless runs
[63:36] a headless Chrome instance in the cloud
[63:38] this stuff is great for dynamic or heavy
[63:40] JavaScript websites if you've never used
[63:42] browser list before the cool part about
[63:45] browser list is allows you to actually
[63:46] bypass captas which is a big issue that
[63:48] a lot of people have um so I'm going to
[63:50] click try it for free I'm going to enter
[63:52] my email address over here verify I need
[63:56] to submit a code so let's head back over
[63:58] here thank you thank you thank you thank
[64:02] you we have a ton of free trial signups
[64:07] obviously I don't have a promo code or
[64:09] anything don't have a company name I'm
[64:11] just going to enter a password I'm using
[64:13] this to get past uh to avoid setting up
[64:15] a puppeteer and playright server sure
[64:17] I'm going to click complete we're now
[64:19] going to have a th credits inside of
[64:21] browser list which is pretty sweet um
[64:23] and we'll get a we'll get a full plan
[64:25] eventually we now have an API token so I
[64:27] can figure out how all of the stuff
[64:28] works here I'm just going to dive right
[64:29] into the API I can figure out how all of
[64:31] the API stuff works using their API docs
[64:33] which are fantastic by the way um and we
[64:35] don't want to do any of this stuff we
[64:37] just want to do HTTP apis brow list API
[64:39] index okay great so here's where we're
[64:41] at um if you want to send and receive a
[64:42] request what you need to do is uh you
[64:46] send a request to one of these endpoints
[64:48] content unblock download function PDF
[64:53] screenshot scrape or performance
[64:56] what we want for the purpose of this is
[64:58] just uh let's do content okay this is
[65:01] the request right over here so I'm just
[65:03] going to paste my API token up here copy
[65:05] this
[65:06] request feed it into nadn in the HTTP
[65:09] request module as per
[65:11] usual nice quick and easy I'm going to
[65:14] grab my API token and where it says your
[65:16] API token here I'm going to feed that in
[65:19] what I want as a website is just left
[65:21] click. a I'm going to run test step we
[65:24] are now quering the pi and in seconds we
[65:27] have access to the data same thing that
[65:30] we had before but now we're using a pass
[65:33] through and browser list is a great pass
[65:35] through um because you know uh they they
[65:38] allow you to scrape things that go far
[65:40] beyond the usual static site thing so
[65:44] like honestly and I'm just leaving this
[65:46] as a secret and sort of a little I guess
[65:48] Easter egg for people that have made it
[65:49] this far in the video like my go-to when
[65:51] scraping websites is as I mentioned do
[65:54] that HTTP request trans forg that works
[65:55] then do something like fir C.D but if
[65:57] that doesn't work I I do something like
[65:59] browserless that has all of this stuff
[66:01] built in um and I especially use browser
[66:05] list anytime that there's some sort of
[66:07] you know application where I'm just
[66:08] going to save this so I can make all my
[66:10] HTP requests really easy um especially
[66:12] when you know there's issues with captas
[66:14] and and accessing resources and stuff
[66:16] check this out not only can you do um
[66:19] the actual scrape you can do a
[66:21] screenshot of the page as well and
[66:22] because I've entered my token up here
[66:24] the requests that I'm going to setting
[66:25] up are as simple as importing the
[66:28] curl then clicking test step so
[66:31] straightforward we now have a file which
[66:34] is the screenshot now I used example
[66:36] domain there let's go left
[66:39] click. run this test now you can see
[66:42] we've actually like received a
[66:43] screenshot of the of the website
[66:47] view very sexy and my website's pretty
[66:49] long so keep in
[66:51] mind um and yeah you know obviously a
[66:53] lot you could do with that you can
[66:54] download the site you can turn the site
[66:56] into a PDF so um that's pretty neat I
[66:59] don't think I've actually used this one
[67:00] before but for the purposes of this
[67:02] demonstration why don't we give it a try
[67:04] we'll go over here import the curl paste
[67:07] it in voila the website I'm going to do
[67:10] is left click. aai going to test this
[67:14] step so now there servers doing a couple
[67:16] things like I'm scraping the site then
[67:18] converting it all into PDF format um
[67:20] probably screenshotting a bunch of stuff
[67:21] too if I view this now we now have my my
[67:24] file looks like it didn't capture all of
[67:26] the color aspects um that might just be
[67:29] difficult or whatever but I still have
[67:30] like a PDF of the site which is pretty
[67:31] neat um and yeah let you guys kind of
[67:34] screw around with this on your own but
[67:35] there are a variety of cool applications
[67:36] you can use browless for all right I
[67:39] hope you guys appreciated the nine best
[67:40] ways to scrape websites in nadn as you
[67:42] guys could see it's a combination of on
[67:45] platform scraping using the HTTP request
[67:47] module a lot of like API documentation
[67:49] stuff like that if you want to get good
[67:51] at this I'm releasing a master class on
[67:52] API stuff um uh as part of my next na
[67:56] tutorial video uh and then you know
[67:58] navigating this and then and then taking
[68:00] the data from these services and using
[68:01] them to do something that you want to do
[68:03] like artificial intelligence to give you
[68:04] a summary of the site or generate ice
[68:07] breakers for you or do something else um
[68:09] whether you're using a local application
[68:11] like octop parse or maybe the web
[68:13] scraping CH uh Chrome extension or using
[68:16] something like firra browserless appify
[68:19] rapid API and so on and so forth um you
[68:21] now have everything that you need in
[68:22] order to scrape static sites Dynamic
[68:24] sites super Js heavy websites and even
[68:26] social media websites like Tik Tok
[68:28] Twitter and Instagram thanks so much for
[68:29] making it to this point in the video if
[68:31] you have any suggestions for future
[68:32] content drop them down below more than
[68:34] happy to take your idea and run with it
[68:36] assuming it's something that I haven't
[68:37] done before and then if you guys could
[68:39] do me a really big solid like subscribe
[68:41] do all that fun YouTube stuff and I'll
[68:42] catch you on the next video thank you
[68:43] very much

Guidelines:
1. Use this template structure:
   - üéØ Main Topic/Theme
   - üí° Key Insight
   - üîç Deep Dive (3 bullet points)
   - ü§î Why This Matters
   - üé¨ Watch More
2. Keep each section concise and impactful
3. Use emojis strategically
4. Include relevant hashtags
5. Keep the total length between 1000-1300 characters
6. End with the video link

Video Link: https://youtu.be/y-eEbmNeFZo

==================================================

--- SEGMENTED PROMPT ---

You are a LinkedIn content strategist specializing in detailed content analysis. Create a segmented LinkedIn post that breaks down the key components of this YouTube video.

Video Title: The 9 Best Ways to Scrape Any Website in N8N
Channel: Nick Saraev
Duration: 1:08:45
Views: 1,230
Likes: 88
Description: Join Maker School & get your first automation customer ‚§µÔ∏è
https://www.skool.com/makerschool/about

Summary ‚§µÔ∏è
Here I show you the 9 best ways to scrape websites using N8N.

First, I'll cover the basics of handling static and dynamic sites (and the differences between them. I'll then show you how to manage direct HTTP requests in N8N before demoing tools like Firecrawl, RapidAPI, and Browserless. Plus, I'll explore options like the Web Scraper Chrome Extension and Apify for advanced tasks. Let me know if q's!

My software, tools, & deals (some give me kickbacks‚Äîthank you!)
üöÄ Instantly: https://link.nicksaraev.com/instantly-short
üìß Anymailfinder: https://link.nicksaraev.com/amf-short
ü§ñ Apify: https://console.apify.com/sign-up (30% off with code 30NICKSARAEV)
üßëüèΩ‚Äçüíª n8n: https://n8n.partnerlinks.io/h372ujv8cw80
üìà Rize: https://link.nicksaraev.com/rize-short (25% off with promo code NICK)

Follow me on other platforms üòà
üì∏ Instagram: https://www.instagram.com/nick_saraev
üïäÔ∏è Twitter/X: https://twitter.com/nicksaraev
ü§ô Blog: https://nicksaraev.com

Why watch?
If this is your first view‚Äîhi, I‚Äôm Nick! TLDR: I spent six years building automated businesses with Make.com (most notably 1SecondCopy, a content company that hit 7 figures). Today a lot of people talk about automation, but I‚Äôve noticed that very few have practical, real world success making money with it. So this channel is me chiming in and showing you what *real* systems that make *real* revenue look like.

Hopefully I can help you improve your business, and in doing so, the rest of your life üôè

Like, subscribe, and leave me a comment if you have a specific request! Thanks.

Chapters
00:00:040Introduction to Web Scraping in N8N
00:00:42 Understanding Static vs Dynamic Sites
00:02:44 First Method: HTTP Requests
00:05:44 Converting HTML to Markdown
00:09:51 Using OpenAI for Data Processing
00:10:17 Second Method: Firecrawl Service
00:11:25 Signing Up for Firecrawl
00:14:10 Third Method: Rapid API Marketplace
00:19:24 Fourth Method: Web Scraper Chrome Extension
00:25:36 Fifth Method: Appify for Scraping
00:34:30 Sixth Method: Data for SEO API
00:41:18 Seventh Method: Crawlbase for High Volume Requests
00:48:51 Eighth Method: Octoparse for Local Scraping
01:03:30 Ninth Method: Browserless for Dynamic Sites
01:08:20 Conclusion and Future Content Suggestions
Tags: automation, make.com, content creation, ai content, google sheets, chatgpt, wordpress, openai, blogging, integromat, make, automating, automate, gpt-4, gpt, openai api, indie hacking, small business, $20K/mo, cold email, make money online, make.com for people who want to make real money, make.com money, AIAA, ai automation, ai automation agency, ai automation guide, n8n scraping, n8n how to scrape, n8n scraper, n8n scrape, n8n scraping setup, n8n scrapers, n8n ai scraper, firecrawl

Transcription:
[00:00] hey Nick here today I'm going to show
[00:01] you the nine best ways to scrape any
[00:03] website in nadn you're going to be able
[00:05] to scrape static sites Dynamic sites
[00:07] JavaScript social media whatever the
[00:09] heck you want by the end of the video
[00:10] you'll know how to do it I scaled my
[00:12] automation agency to 72k a month using
[00:14] no code tools like make and nadn and
[00:16] scraping was a big part of that so this
[00:18] video is just going to give you all the
[00:18] sauce you're going to learn everything
[00:19] you need to scrape websites like that on
[00:21] your own let's get into it all right I'm
[00:23] going to jump into NN in a minute and
[00:24] actually build these alongside you and
[00:25] one other thing I'm going to do is I'm
[00:26] actually going to sign up to all the
[00:27] services in front of you walk you
[00:29] through the Authentication and the
[00:30] onboarding flows and get your API keys
[00:32] and stuff like that but just before I do
[00:34] want to explain very quickly the
[00:36] difference between a static site and
[00:38] then a dynamic site because if you don't
[00:40] know this um scraping just gets a lot
[00:42] harder and so we're just going to cover
[00:43] this in like 30 seconds and we can move
[00:45] on so basically um if this is you okay
[00:47] you're just this wonderful smiley person
[00:50] and you want to access a static website
[00:52] what you're doing is you're sending a
[00:54] request over basically to just like some
[00:56] document you know think about this as
[00:58] just like a piece of paper on a cupboard
[00:59] and there's a bunch of text on this
[01:01] piece of paper and what you do is you
[01:02] say hey can I have this piece of paper
[01:04] and then the piece of paper just comes
[01:06] back to you with all of the information
[01:07] inside of the piece of paper okay this
[01:09] is a very simplified version of what's
[01:11] actually going on but static sites are
[01:13] by far the easiest thing to scrape and
[01:15] so um this is where you know a lot of
[01:17] people think all websites are are at and
[01:20] then they kind of confuse it with this
[01:21] next step which is dynamic a dynamic
[01:23] site essentially is not like that at all
[01:26] basically what you're doing is you're
[01:27] sending a request to a piece of paper
[01:30] but the piece of paper has nothing on it
[01:32] okay what happens is this piece of paper
[01:34] then sends a request to some other dude
[01:36] which I guess in this case is just a
[01:37] server really who will then he has a
[01:40] trusty pen in his hand and he'll
[01:41] actually write all of the stuff on said
[01:44] piece of paper and then you'll get the
[01:46] piece of paper back so um there's
[01:48] actually that intermediate step okay
[01:50] where basically you are pinging some
[01:51] sort of uh you know domain name or
[01:53] whatever then that domain name shoots
[01:55] some code over forces a server to
[01:57] generate all of the contents on that
[01:58] domain and then you get it
[02:00] this is obviously kind of a two-step
[02:02] process and then this is a three-step
[02:04] process so if you just understand that
[02:08] um you know when you scrape a dynamic
[02:09] resource what you're really doing is
[02:10] you're sending a request to a page which
[02:12] sends a request back to another server
[02:14] which then fills your thing this element
[02:16] eliminates 99% of the confusion because
[02:18] most of the time like scraping issues
[02:19] are hey I just ping this page but I got
[02:21] nothing back you know the HTTP request
[02:23] or or or whatever I I sent you know it
[02:26] was fine but for some weird reason
[02:27] there's nothing on this page of of any
[02:29] substance well if that was the case for
[02:31] you it was most likely um because this
[02:32] was empty before you sent it over um
[02:35] whereas you know simple scraped static
[02:37] resources tend to just like give you
[02:38] what you want really quickly and it's
[02:39] really easy all right so hopefully we at
[02:42] least understand that there's that
[02:43] difference between static and dynamic
[02:44] sites here um I'm not going to go into
[02:46] it more than that we're actually just
[02:47] going to dive in with both feet start
[02:49] doing a little bit of scraping and then
[02:50] we'll kind of see where we land I find
[02:51] the best way to do this stuff is just by
[02:53] example and and you know being practical
[02:54] about it so the first major way to
[02:57] scrape websites in NN is using direct h
[02:59] HTTP requests this is also what I like
[03:02] to think of as the Magic in scraping
[03:05] itself what we're going to do is we're
[03:07] going to use a node called the HTTP
[03:08] request node to send a get request to
[03:10] the website we want this is going to
[03:12] work with static websites and non
[03:14] JavaScript resources so let me give you
[03:16] guys a website that I'm going to be
[03:17] scraping here this is my own site it's
[03:19] called left click I'm about to do a
[03:20] redesign um but this is a static
[03:22] resource I know this because I built the
[03:24] site you know I I did it in code and
[03:26] basically all this is is just a document
[03:27] somewhere on my or on on a server some
[03:29] more so what I'm going to want to do is
[03:31] and I'm just going to pretend that I
[03:32] haven't done any of this so uh we're
[03:34] just going to go HTTP
[03:37] request HTTP request node looks like
[03:39] this we have a method field a URL field
[03:42] authentication field query parameters
[03:44] headers body and then some options down
[03:46] here as well all I'm going to do is I'm
[03:48] just going to paste in the website that
[03:50] I want to visit okay then I'm just going
[03:52] to test the step it's that easy now the
[03:55] response from this on the right hand
[03:56] side see all this code over here this is
[03:59] what's called HTML if you're unfamiliar
[04:01] and HTML is basically just the like it's
[04:04] it's the code behind the site so if I
[04:07] were to zoom in over here you see where
[04:09] it says I don't know let's let's go to
[04:10] my website let's just find a little bit
[04:11] of little bit of texture build hands off
[04:14] growth systems okay if I just command F
[04:17] and paste this in we actually have that
[04:19] text buried somewhere in this big long
[04:21] HTML string right so all that this HTML
[04:25] is is this is the code that is sent to
[04:28] my browser which is Google Chrome in
[04:30] this case then my browser takes the code
[04:32] and it just renders it into this
[04:34] beautiful looking thing well beautiful
[04:35] is a subjective State I would say but
[04:38] this uh wonderful looking thing in front
[04:39] of us which is this website with like
[04:41] sizing and the tabs and the divs and all
[04:44] that fun stuff okay so basically what
[04:47] I'm trying to say is everything over
[04:48] here on the right hand side this is the
[04:50] entire site we can do anything we want
[04:52] with this information um and we can
[04:54] carry this information forward to to do
[04:56] any one of our any one of many flows so
[04:59] in my case right looking at a bunch of
[05:00] code isn't really very pretty so one big
[05:02] thing that you'll find in the vast
[05:03] majority of modern um scraping
[05:05] applications is you'll find that they'll
[05:06] take that HTML which we saw earlier and
[05:08] they'll convert it to something called
[05:09] markdown okay so um this is a markdown
[05:13] node we have a mode of HTML to markdown
[05:16] and all I'm going to do is I'm going to
[05:18] grab that data and I'm going stick it in
[05:20] the HTML section of the HTML to markdown
[05:22] converter what do you think is going to
[05:24] happen when I test this well we're going
[05:25] to convert this from this big long ugly
[05:27] super dense uh thing with a much these
[05:30] like greater than and less than symbols
[05:31] and we're just going to convert it into
[05:33] something a little bit shorter a little
[05:34] bit simpler This Is Us just manipulating
[05:36] file formats by the way and I find that
[05:38] manipulating file formats is a big part
[05:39] of what makes a good scraper a good
[05:41] scraper so now we have something in
[05:43] what's called markdown format what's the
[05:45] value there well markdown format does
[05:46] two things for us one it's much easier
[05:48] to parse parse just means we can extract
[05:50] different sections of the text we want
[05:52] we can structure it in some sort of
[05:54] other data format um and then in my case
[05:56] because I love using AI for everything
[05:58] it's much easier and shorter for us to
[06:00] use with AI so I'm going to give you
[06:01] guys a very simple example where we take
[06:03] this text from the static resource and
[06:05] then we just use um AI to tell us
[06:08] something about it so I'll go down to
[06:09] open Ai and then what I'm going to do is
[06:11] I'll do the message a model just have to
[06:14] connect my credential here I'm assuming
[06:16] that you've already connected a
[06:16] credential if not you're going to have
[06:18] to go to opena website when you do the
[06:20] connection um and grab your API key and
[06:22] paste it in there's some instructions
[06:24] that allow you to do so right over
[06:26] here uh what I'm going to do is I'm
[06:28] going to grab the G PT 40 Mini model
[06:31] that's just the uh I want to say most
[06:33] cost effective one as of the time of
[06:34] this recording and then what I'm going
[06:36] to do is I'm going to add three prompt
[06:37] I'm going to add a system prompt first
[06:39] I'll say you are a helpful intelligent
[06:42] web scraping
[06:43] assistant then I'm going to add a user
[06:46] prompt and I'll say your task is to take
[06:48] the
[06:49] raw
[06:51] markdown of a website and convert it
[06:54] into structured data use the following
[06:57] format and then I'm going to give it an
[06:59] example of what I want in what's called
[07:00] Json JavaScript object notation format
[07:02] so the very first thing I'm going to do
[07:04] is I'm going to have it just pull out
[07:05] all the links on the website because I
[07:07] find that that's a very common scraping
[07:08] application so I go links and then I'm
[07:10] just going to show an example of un
[07:13] array of we'll go absolute URLs this is
[07:19] very important that they're absolute
[07:20] URLs any thing that we're going to build
[07:22] after this is going to be making use of
[07:23] the absolute URLs not the relative URLs
[07:25] if you're unfamiliar with what that
[07:26] means if we Zoom way in here you see how
[07:28] there's this B uh SL left click log.png
[07:31] this is what's called a relative URL if
[07:33] you were to copy this and paste this
[07:35] into here this wouldn't actually do
[07:37] anything for us right uh what we what we
[07:39] want is we want this instead we want
[07:41] left click aka the root of the domain
[07:43] and then um left click _ logogram and
[07:46] that's how we get to the actual file
[07:47] asset so uh if we go back over
[07:50] here so if we go back over here um yeah
[07:53] you know basically we want a link array
[07:56] of absolute URLs and then I'm just going
[07:58] to want main text website copy this is
[08:02] going to be a long string containing all
[08:05] of the website
[08:06] copy containing just the text of the
[08:09] site no
[08:11] formatting and then why don't we do one
[08:13] more thing why don't we have like a
[08:16] summarized or summary let's do one line
[08:19] summary just to show you guys you can
[08:21] also use AI to do other cool stuff you
[08:23] could take this oneline summary and feed
[08:25] it into some big sequence you could have
[08:26] ai write an icebreaker for an email you
[08:29] could do a things with this but I'll say
[08:30] on line summary um brief summarization
[08:34] of what the site is and
[08:37] how what the site is about let's do that
[08:42] okay so this is our example I'm going to
[08:45] say your website URL
[08:48] is left click URL for the relative to
[08:53] Absolute
[08:54] conversions is left click. and then the
[08:56] final thing is I'm going to add one more
[08:58] user prompt I'm just going to draw drag
[08:59] all of that markdown data in here then
[09:01] I'm going to click output content as
[09:03] Json I'm going to test the step I'm
[09:06] going to take a sip of my coffee while
[09:07] this puppy processes and we now have our
[09:09] output on the right hand side if we go
[09:11] to schema view what you can see is we've
[09:14] now
[09:15] generated basically an array of links on
[09:17] the rightand side which contains every
[09:19] link on this website very cool looks
[09:21] like the vast majority of these are type
[09:23] form links for some reason don't really
[09:24] know what's about that oh right it's
[09:26] because that's basically the only thing
[09:27] on my website it's just a one P with a
[09:29] bunch of different links to time for
[09:31] that's funny um anyway you could
[09:33] obviously just get it to Output one link
[09:35] or tell it like make sure all the links
[09:36] are unique or something um and then we
[09:38] have a big chunk of plain text website
[09:40] copy right then we have a oneline
[09:41] summary of the site so this is a very
[09:44] simple example of scraping we're
[09:46] scraping a static resource obviously but
[09:48] when I build scrapers for clients or for
[09:50] my own business this is always my first
[09:52] pass I will always just make a basic
[09:54] HTTP request to the resource that I'm
[09:56] looking at because if I can make that
[09:58] http request work whether it's a get
[10:00] request or whatever the the the rest of
[10:03] my life building scraper building the
[10:05] scraper is so easy I just take the data
[10:08] I process it usually using AI or some
[10:09] very cheap Tok cheap per token thing and
[10:12] then voila you know like we've basically
[10:14] built out a scraper in this case and
[10:15] it's only taken us what three nodes
[10:17] right so that's number one the second
[10:19] way to scrape websites in NN is using a
[10:21] third party service called fir crawl and
[10:22] making an HTTP request to it I'm using
[10:25] something called their extract endpoint
[10:27] but just to make a long story short fire
[10:29] craw is a very simple but High uh
[10:32] bandwidth service that turns websites
[10:35] into large language model ready data and
[10:37] basically you know how earlier we had to
[10:38] do HTTP request and then we had to
[10:40] convert all that stuff into markdown and
[10:41] then we had to you know manipulate that
[10:43] markdown what this does is it just does
[10:45] a lot of that stuff for you it'll
[10:46] actually allow you to go scrape and then
[10:48] it will automatically convert text into
[10:49] markdown for you um so that you can do
[10:52] whatever the heck you want they turn it
[10:53] into structure data using Ai and and so
[10:54] on and so on and so forth so if I were
[10:56] to do the same thing that I just did
[10:58] earlier
[10:59] with my own
[11:00] website then I were to you know run an
[11:02] example of this what it would go do is
[11:05] it would basically spin up a server for
[11:07] me and that would actually go and
[11:08] generate markdown of the same format um
[11:10] the only difference here is it's
[11:11] actually generated new lines between
[11:13] sections of text how beautiful um and
[11:15] then now you know we have basically the
[11:17] same thing you also get it in Json which
[11:19] is pretty cool um and you know you can
[11:20] slot this into any workflow this is
[11:22] basically like the simple and easy way
[11:23] of getting started um what we're going
[11:26] to be showing you today is the extract
[11:28] endpoint which allows you to extract
[11:30] data just using a natural language
[11:32] prompt which is pretty cool and from
[11:34] here we're going to be able to take any
[11:35] URL and just turn it into structure data
[11:37] but we're not actually going to have to
[11:37] know how to parse we're not going to
[11:38] have to know any code we're not going to
[11:39] have to know any of that stuff so let me
[11:41] actually run through the signup process
[11:42] with you guys go to fire. Dev here just
[11:47] going to open this up in an incognito to
[11:48] show you guys what this looks like all
[11:49] you do is you just go sign up I'm going
[11:53] to add a password we're then going to
[11:55] have to validate this one
[11:58] oh guess these guys are Mega
[12:01] secure so now I'm going to go back to my
[12:03] email
[12:04] address I'm going to count this up and
[12:09] we have a call back here I just need to
[12:10] paste this URL and put it in here that's
[12:12] just because I you know I'm doing this
[12:14] in an incognito tab normally when you do
[12:16] this you're not going to have that step
[12:17] okay great now we're inside a fir craw
[12:20] they give you I think something like 500
[12:22] um free credits something of that nature
[12:24] anyway so what I'm going to do is I'm
[12:26] going to go through and just like give
[12:28] give this extracting point just a basic
[12:30] natural language query so um let's go
[12:34] from the homepage at left
[12:37] click. I want to
[12:40] extract a oneline summary of the website
[12:45] let's do all of the text on the
[12:50] website all of the copy on the website
[12:53] in plain text let's do a oneline summary
[12:56] of the website a oneline Icebreaker I
[12:58] can use as the first line of of an of a
[13:01] cold email to the
[13:05] owner and uh the company name and a list
[13:11] of the services they provide let's do
[13:14] that this is a lot of requests we're
[13:17] asking it to do like seven or eight
[13:19] things but all I need to do in order to
[13:21] make this work is I click generate
[13:22] parameters it's going to basically now
[13:24] generate me a big object with a bunch of
[13:26] things so copy summary Icebreaker
[13:28] company name and now I can actually go
[13:30] and I can run this okay this is the URL
[13:33] it just parsed as well let's give it a
[13:35] run what it's doing now is it's scraping
[13:38] the pages using their high throughput
[13:39] server I just love this thing like I'm
[13:42] not sponsored by fire crawl or anything
[13:43] like that but I love their uh I don't
[13:45] know I just love the design I love this
[13:47] little like burning Ember or whatever
[13:49] the heck you want to call it I love how
[13:51] simple they've tried to make everything
[13:52] it's it's great honestly okay awesome
[13:55] and now you guys see we have basically a
[13:57] big array with a bunch of sub objects we
[14:00] have a summary like I asked for a list
[14:02] of services looks like we even have
[14:04] links to the specific places oh okay
[14:06] links from the resource we have an
[14:08] icebreaker and then we have the company
[14:10] name as well so we can do a lot with
[14:11] this right but right now this is just um
[14:13] this is just on on a website how do we
[14:14] actually bring this in naden uh well
[14:16] it's pretty simple as you see there's an
[14:17] integrate Now button you can either get
[14:19] code or you can use it in zap here
[14:21] basically what we're going to want to do
[14:22] is we're going to want to run a request
[14:24] to um their endpoint and then we're
[14:26] going to want to turn that into
[14:29] basically our HTTP request let me show
[14:30] you what that looks like I'm just going
[14:33] to do all of this stuff in curl so if we
[14:35] go to curl as you can see what we need
[14:38] to do is we need to format a request
[14:40] that looks something like this but we
[14:41] need to make sure it's using the extract
[14:43] endpoint okay so I'm going to go down to
[14:45] extract and then now I have this big
[14:47] long beautiful string what I'm going to
[14:50] do is I'm going to copy
[14:51] this I'm going to go back to my NN
[14:55] instance which is right over here and
[14:56] then what I need to do is just open up
[14:58] an HTTP request module and then click
[14:59] import curl just paste all the stuff
[15:01] inside now this is an example request
[15:03] but that's okay we can actually use that
[15:04] example request to very quickly and
[15:05] easily format our our real request okay
[15:08] so we're sending a bunch of headers this
[15:09] is the endpoint that we're calling api.
[15:11] fire. dv1 extract so basically what
[15:14] we're doing now is we're like we're
[15:15] sending a request to fir craw which will
[15:16] then send a request to the website right
[15:18] so kind of a kind of a middleman and
[15:20] then all I'm going to do so if I go back
[15:22] to my example we have an API key here
[15:24] which we're going to need so I'm going
[15:25] to go here and then paste in an API key
[15:28] so that's how that work works right
[15:29] authorization is going to be the name
[15:30] value is going to be bear with a capital
[15:32] b space and then the API key and then we
[15:35] also have a body that we need to uh
[15:37] adjust or edit and this body is where
[15:39] we're going to put the links that we
[15:40] want to actually have scraped with the
[15:41] extract end point so what I'm going to
[15:44] do is I'm going to delete most of these
[15:46] I'll go back to my left click. a just
[15:48] like this the prompts um because you
[15:50] know I was just using their playground
[15:51] before we're actually going to need to
[15:52] convert this into a request for my
[15:56] service so I'm just going to paste The
[15:57] Prompt in here voila
[15:59] and now we need to put together what's
[16:00] called a schema where we have the
[16:02] objects that we asked for so in my case
[16:04] we asked for copy right so I'm going to
[16:06] go Copy Type string then summary so
[16:09] we're going to go summary type string
[16:12] then Icebreaker it's going to be
[16:14] Icebreaker type string then guess what
[16:16] we have last but not least company name
[16:18] which is going to be type string we're
[16:21] also going to want to make these fields
[16:22] required like uh you know you can set it
[16:24] up so they're not actually required when
[16:25] you do a request a fire call I'm I'm
[16:27] going to make it so they're required so
[16:28] I'll go copy
[16:29] summary Icebreaker and then company name
[16:32] actually you know what maybe I'll leave
[16:34] company name as unrequired if you think
[16:36] about it logically maybe not all the
[16:38] websites we're going to be scraping
[16:39] using this service are going to have the
[16:40] company names visible on the website I
[16:42] don't know but maybe so maybe I'll
[16:44] actually leave that as off okay great so
[16:46] now we have the API request formatted
[16:48] correctly um all we need to do at this
[16:50] point is just click test step it looks
[16:52] like we're getting a Json breaking um
[16:55] error and I think that's because I have
[16:56] this last comma and I'm just going to
[16:58] check to see if there are any commas in
[16:59] Jason you can't actually have the last
[17:00] element in an array have a comma on it
[17:03] so I think that's okay let me test it
[17:05] again all right so as you can see we
[17:06] just received an ID we've got a success
[17:08] and then we have a URL Trace array which
[17:10] is empty um if you think about this
[17:12] logically we don't actually get all the
[17:14] data that we send immediately because we
[17:16] need fir crawl to whip up the scraper
[17:18] you know do things to the data we could
[17:20] be feeding in 50 URLs here right so
[17:22] instead of just having the data
[17:23] available to us right now immediately
[17:25] what we need to do is we need to
[17:25] basically wait a little while wait until
[17:27] it's done and we need to Ping it and the
[17:29] reason why they've given us this ID
[17:31] parameter so that we could do the
[17:32] pinging so the way that you do this is
[17:34] you'd have to send a second HTTP
[17:36] request using this
[17:38] structure so the good news is we could
[17:41] just copy this
[17:42] over and then we can
[17:45] add a second um HTTP request I don't
[17:49] know where that went but I guess I'm
[17:50] just going to create it over here I'm
[17:53] going to import the curl to this request
[17:55] just like
[17:56] that then keep in mind that we just need
[17:58] to add our API key again because the
[18:00] previous node had it but this one
[18:01] doesn't so just going to go over here
[18:03] I'm going to copy this
[18:05] puppy go back over here I'm going to
[18:08] paste this in now technically what this
[18:10] is called is this is called polling um
[18:12] polling uh is where you know you're
[18:15] you're you're attempting to request a
[18:17] resource that you don't know whether or
[18:18] not is ready and there's a fair amount
[18:20] of logic that I'd recommend like putting
[18:22] into a polling flow where like when you
[18:24] try it and if it doesn't work basically
[18:25] you wait a certain amount of time and
[18:26] you retry again for the purpose of this
[18:28] video I'm not going to put all that
[18:29] stuff inside but um what I'm going to do
[18:31] is just set up this request I'm going to
[18:33] give this puppy a test let's just feed
[18:36] that in on the back end we got to put
[18:37] the extract ID right right over here
[18:40] where it said extract ID then I'm just
[18:41] going to give this a test uh looks like
[18:44] I've issued a malformed request we just
[18:46] have to make sure that everything here
[18:48] is okay specify body let me just make
[18:50] sure there's nothing else in here it was
[18:51] a get request this is a get cool we're
[18:54] not going to send a body
[18:56] then awesome and now we have all of the
[18:58] data available to us automate your
[19:01] business in the copy field summary field
[19:02] left clicks an ad performance
[19:04] optimization agency Icebreaker hi Nick I
[19:06] came across left click I'm impressed by
[19:07] you help B2B Founders scale their
[19:08] business automation keep in mind I never
[19:11] gave it my name it went it found my name
[19:12] on the website uh and then company name
[19:14] left click so quick and easy way uh
[19:17] you're going to have access to this
[19:18] template obviously without my API key in
[19:19] it um and feel free to you know use fir
[19:21] craw go nuts check out their
[19:22] documentation build out as complex a
[19:24] scraping flow as need be the third way
[19:25] to scrape websites in nadn is using
[19:28] rapid API for those of you that are
[19:29] unfamiliar rapid API is basically a
[19:31] giant Marketplace of third party
[19:33] scrapers similar to appify which I'll
[19:34] cover in a moment but instead of looking
[19:36] for um you know building out your own
[19:38] scraper for a resource let's say you're
[19:40] wanting to scrape Instagram or something
[19:41] that's not a simple static site what you
[19:43] can do is you could just get a scraper
[19:44] that somebody's already developed that
[19:46] does specifically that using proxies and
[19:48] all that tough stuff that I tend to
[19:50] abstract away um and then you just
[19:52] request uh to Rapid API which
[19:55] automatically handles the API request to
[19:56] the other thing that they want and then
[19:57] they format it and send it all back to
[19:59] and then you know you have beautiful um
[20:01] data that you could use for basically
[20:02] anything so this is what rapid API looks
[20:04] like it's basically a big Marketplace I
[20:05] just pumped in a search for website over
[20:08] here and we see 2,97 results to give you
[20:11] guys some context you can do everything
[20:12] from you know scraping social data like
[20:15] emails phone numbers and stuff like that
[20:17] from a website you could ping the ah
[20:19] refs SEO API you could find uh I don't
[20:23] know like unofficial medium data that
[20:25] they don't necessarily allow people to
[20:26] do so this is just a quick and easy way
[20:28] to I guess do a first pass after you've
[20:31] run through fir crawl maybe that doesn't
[20:32] work after you've run through HTTP
[20:33] request that doesn't work um just do a
[20:35] first pass look for something that
[20:36] scrapes the exact resource you're
[20:37] looking for and then take it from there
[20:39] so obviously for the purpose of this I'm
[20:40] just going to use the website to scraper
[20:41] API which is sort of just like a wrapper
[20:44] around what we're doing right now in
[20:45] nadn um but this website scraper API
[20:48] allows you to scrape some more Dynamic
[20:49] data um now I'm not signed up to this so
[20:51] I'm going to have to go through the
[20:52] signup process and I'm going to show you
[20:53] guys what that looks like um but yeah
[20:55] we're going to we're going to run
[20:55] through an API request to Rapid API
[20:57] which is going to make this a lot easier
[21:00] just going to put in all of my
[21:02] information
[21:04] here and then I'm going to do the
[21:07] classic email verification okay just
[21:11] copy this puppy over no thank you rise I
[21:13] use a time management app called rise
[21:16] and every time I go on my Gmail I set my
[21:18] Gmail up as like a definitely do
[21:22] not uh do during your workday let's just
[21:25] call it personal projects they don't ask
[21:26] me all these questions my goal today is
[21:28] to browse available apis awesome so
[21:31] that's their onboarding I think we're
[21:32] going to have to like pay a little bit
[21:33] of money or something like that which
[21:34] I'll sort out in a moment um but the
[21:36] scraper that I want is I just want the
[21:37] website one right so I'm going to type
[21:38] website in
[21:39] here uh I'm going to look
[21:42] for wherever it was earlier website
[21:44] scraper API and now check this out what
[21:47] we have is we have the app which is the
[21:49] name of the specific API that we're
[21:51] requesting we have an x-raid api-key and
[21:55] this is the API key we're going to use
[21:56] to make the request then we have the
[21:58] request URL which is basically what
[22:00] we're pinging and what we can do here is
[22:01] we can feed in the parameters okay what
[22:03] website we want to we want to scrape and
[22:05] then we can actually just like give it
[22:06] give it a run so I'm going to have to
[22:08] subscribe to this in order to test it uh
[22:10] I'm just going to go to the um basic
[22:12] plan and I'm going to pay money per
[22:14] month that probably seems the simplest
[22:15] way to do so okay and I just ran through
[22:17] the payment let's actually head over
[22:18] here and let's just run a test using my
[22:20] website URL we're going to test this
[22:22] endpoint now and now this actually going
[22:24] to go through Rapid API it's going to
[22:25] spin up the server and then it's going
[22:27] to send it and what we see here is we
[22:29] have multiple fields that Rapid apis or
[22:31] this particular scraper gives us let me
[22:32] just make this easier for you all to see
[22:34] we have a text content field with all of
[22:35] the content of the website which is cool
[22:37] this is basically what I did earlier um
[22:39] but instead of me having to formulate
[22:40] this request try and parse it and try
[22:42] and use AI tokens what I did is I sent
[22:43] the request to uh rapid API and did it
[22:45] all for me then we also have an HTML
[22:47] content
[22:48] field I think we have one more here
[22:51] scroll all the way down to the bottom as
[22:53] you can see there is a ton of HTML um
[22:55] and then we also have a list of all of
[22:56] the images on the website which is very
[22:58] very cool and easily formatted again
[23:00] something that I tried to do manually
[23:01] using AI but now you know we have
[23:03] everything in that nice absolute URL
[23:04] format um and then if they find any
[23:06] social media links I don't believe um
[23:09] there were more than Twitter but um if
[23:11] they find anything that's at their
[23:12] Twitter Instagram whatever then we have
[23:14] the link right over here it looks like
[23:15] they even give you the scraping time and
[23:17] if they scrape emails or phone numbers
[23:18] um they'll be there as well so I mean
[23:20] rapid AP is obviously fantastic this is
[23:21] a high throughput sort of thing and why
[23:23] don't we actually run through what this
[23:25] would look like if we were to run a curl
[23:26] request you see how it's automatically
[23:29] just formatting it as curl well that
[23:30] just means we just jump back here
[23:32] connect this to my HTTP request module
[23:35] click import curl paste it in like this
[23:38] import and it's actually going to go
[23:39] through and it's going to automatically
[23:41] map all these fields for me right query
[23:43] parameter URL left click. beautiful um
[23:45] API key x-raid API host here's the host
[23:49] here's the name of the API key here's
[23:51] everything we need well I can actually
[23:52] just recreate this request now inside of
[23:54] NN as opposed to being on rapid API and
[23:57] then I have all the data accessible to
[23:58] me here how cool is that so we can do
[24:01] this for any any major website really um
[24:03] you know there are a lot of specific
[24:06] bespoke scrapers obviously which um I
[24:07] don't know if you wanted to scrape uh
[24:09] let's go back to Discovery if you wanted
[24:11] to scrape like Instagram or something
[24:14] you could scrape um Instagram uh you
[24:16] could do like Facebook scraping you
[24:18] could scrape these large giants that are
[24:20] quite difficult to do So Meta ad Library
[24:22] Facebook ad scraper and depending on the
[24:24] plan that you're at it might be more
[24:25] cost- effective for you to sign up to
[24:27] some sort of monthly recurring thing
[24:28] rather than just pay two cents every
[24:29] single time you make one of these
[24:30] requests you just kind of got to do that
[24:32] determination yourself right like if
[24:33] you're scraping uh I don't know 50 every
[24:36] day or 100 every day or something might
[24:38] be a dollar or two a day which is
[24:39] reasonable but maybe if you want to
[24:41] scrape like 5,000 doing it the way that
[24:42] I was doing it a moment ago might might
[24:43] be infusible the next way to scrape
[24:45] websites in nadn is using the web
[24:47] scraper Chrome extension and then tying
[24:49] that to a cloud service that delivers
[24:51] the data that you just created using
[24:53] their no code tool um in nicely bundled
[24:55] formats it's called Cloud sync as of the
[24:57] time of this recording I think they
[24:58] changed the name a couple of times but
[25:00] um that's where we're at here is the
[25:02] name of the service web scraper here is
[25:05] their website essentially what happens
[25:07] is you install a little Chrome plugin
[25:08] which I'll show you guys how to do then
[25:10] you select the fields that you want
[25:11] scraped in various data formats and then
[25:14] what you do is it handles JavaScript
[25:16] sites Dynamic sites all that fun stuff
[25:19] and then you can um export that
[25:22] data as a cloud run to then send back
[25:28] sorry big sneeze to then send back to
[25:31] some API or some service um and then
[25:33] automatically do parsing and stuff like
[25:34] that so very cool I'm going to show you
[25:36] guys what that looks like um this is
[25:37] sort of a more customized way to build
[25:38] the stuff but I've seen a lot of people
[25:40] do this with naden um so we're going to
[25:42] run through what it looks like so first
[25:43] thing I'm going to want to do is I'm
[25:45] going to want to let's just go Cloud
[25:47] login or sorry um start free 7-Day trial
[25:51] as you can see you know there's a free
[25:52] browser extension here if you wanted to
[25:54] do uh I don't know like highs scale
[25:56] stuff you'd choose probably their
[25:58] project um endpoint where we Sorry
[26:01] project plan where we have 5,000 URL
[26:03] credits we can run a bunch of tasks in
[26:05] parallel we could scrape Dynamic sites
[26:07] JavaScript sites we have a bunch of
[26:08] different export options then we can
[26:09] also just connect it directly to all of
[26:11] these um what I'm going to do just
[26:12] because I want this to kind of work as a
[26:14] first go is I'm just going to sign up to
[26:15] a free Tri here beautiful just created
[26:17] my account just go left click give it a
[26:21] phone number we'll go left click. a
[26:24] we're going to go I don't know academic
[26:26] records needed per month we'll go 0 to
[26:28] th000 length of the project uh I don't
[26:30] know let's go two to 3
[26:31] months okay great so now we can import
[26:34] and run our own site map or we can use a
[26:36] premade Community sit map um what I'm
[26:38] going to do is I'm just going to import
[26:39] this we're then going to get the Chrome
[26:41] extension web
[26:43] scraper let me add that extension and
[26:45] it's going to download it do all that
[26:47] fun stuff beautiful so now we have it
[26:50] right over here I'm just going to pin it
[26:52] to my browser to make my life easier go
[26:54] to left click. a open up this puppy now
[26:57] there's a bunch of like tutorials and
[26:58] how to use this stuff um that's not that
[26:59] big of a deal but basically the thing
[27:01] you need is you need to hold command
[27:03] plus option plus I to open up your
[27:04] developer tools and you'll just find it
[27:05] on the in my case the far right so
[27:07] command option I that'll open up Dev
[27:10] tools you see all the way on the right
[27:11] hand side here I have a couple other
[27:12] things like make and and cookie editor
[27:14] but all the way on the right hand side
[27:15] here we have this web scraper thing um
[27:18] so we got what you're going to want to
[27:19] do first you're going to want to create
[27:20] a site map for the resource that you're
[27:21] going to want to scrape I'm just going
[27:22] to call it left click and I just want to
[27:24] scrape left click. okay once we have our
[27:27] sitemap if I just give a quick little
[27:28] click I can then add a new selector and
[27:30] the really cool thing about this web
[27:32] scraper is um if I just zoom out a
[27:34] little bit here uh what you can do is
[27:35] you can you can select the elements on
[27:37] the website that you want scraped so for
[27:39] instance it's a very quick and easy way
[27:40] to do this if you think about it is like
[27:42] just to show you guys an example
[27:43] structure data is uh sort of like an
[27:45] e-commerce application let's say you
[27:46] have like the title of a product and you
[27:47] have like I don't know the the
[27:49] description of a product so on my
[27:50] website really quick and easy way to do
[27:51] this is let's just call this
[27:53] products and it's a type text what I'm
[27:56] going to do is I'm going to click select
[27:58] then I'll just click on this I'll click
[28:01] on this as well and as you see it'll
[28:03] automatically find all of the headings
[28:06] that I'm looking for so that's products
[28:09] we are going to then click data I'm
[28:10] going to click done selecting data
[28:12] preview as you can see it only selected
[28:14] one of them the very first so what we're
[28:15] going to want to do is go multiple and
[28:17] now if I data preview we get all of the
[28:19] headings which is very cool so now we
[28:21] have a basically like a list of headings
[28:23] um from here I'm going to save this
[28:25] selector I'm add a new one let's go
[28:27] product
[28:28] descriptions and then going to select
[28:31] this this it'll select all of them I'll
[28:34] go multiple data preview just to make
[28:36] sure that it looks good I'm getting no
[28:37] data extracted here oh sorry I didn't
[28:39] actually select the um didn't actually
[28:41] finish it now we're getting product
[28:42] descriptions that's pretty cool um this
[28:44] is me doing this sort of like one at a
[28:46] time you can also um group The selectors
[28:49] there you go it's actually um offered to
[28:51] group it for me so we can uh group this
[28:54] into one object with products and then
[28:56] product descriptions so it's automatic
[28:58] group it now we have wrapper for
[28:59] products and products descriptions then
[29:01] we have products and product
[29:02] descriptions buried underneath we could
[29:04] go as far as we want with this but
[29:05] basically what I'm what I'm trying to
[29:06] show you guys is very simple and easy
[29:08] just drag your mouse over the specific
[29:10] thing you want if you select more than
[29:12] one it'll automatically find all of them
[29:13] on the website which is really cool okay
[29:15] great once we have this um what I can do
[29:17] is I can actually go export sitemap so
[29:20] now I have all of the code on the
[29:21] website that actually goes and finds it
[29:23] for me then I can paste this in here
[29:26] I'll just call this left click scraper
[29:28] and I'm going to import this to my cloud
[29:31] scraper uh I think I'm running into oh
[29:34] sorry I don't think we can do a space
[29:36] there my bad just call it left click and
[29:38] now what we can do is we can actually
[29:39] just like run a server instance that
[29:41] goes out and then scrapes this for us
[29:42] okay so I'm going to click scrape it
[29:45] looks like I need to verify my email so
[29:47] just make sure you do that before you
[29:48] try and get ahead of yourself like I
[29:52] was okay looks like we just verified the
[29:54] email let's head back over here refresh
[29:57] then scrape we've now scheduled a
[29:59] scraping job for this sitemap scheduling
[30:02] you know in their lingo just means that
[30:03] it's now part of their big long queue of
[30:05] thousands of other things that they're
[30:06] probably scraping through their server
[30:07] and that's fine okay I just gave this a
[30:09] refresh and as we see we have now
[30:10] finished said scraping job we have all
[30:12] of the data available to us using their
[30:14] UI but now that we've gone through this
[30:16] process of you know building out this
[30:18] this thing um how do we actually take
[30:20] that and then use it in our nadn flows
[30:22] so variety of ways um if you wanted to
[30:24] connect this let's say to specific
[30:25] service like Dropbox um Google
[30:28] you know dump anow or something Google
[30:30] Drive I'd recommend just doing it
[30:31] directly through their integration it's
[30:32] just a lot easier to get the data there
[30:34] and then you can just connect it to n
[30:36] and watch the data as it comes in or
[30:37] something you can also use the web
[30:39] scraper API uh this is pretty neat
[30:42] because you can you know that's what
[30:43] we're going to end up using it was
[30:45] pretty neat because you can uh like
[30:47] schedule jobs you can send jobs you can
[30:48] do basically everything just through the
[30:50] NN interface and then we can just
[30:52] retrieve the data afterwards which is
[30:53] pretty neat um this is basically what
[30:55] you end up getting you end up with
[30:57] scraping job ID status sitemap all this
[30:59] fun stuff and then we can set like a web
[31:02] hook URL where we we receive the request
[31:05] so um let me check we need a scraping
[31:07] for testing you need a scraping job that
[31:09] has already been finished I think our
[31:10] scraping job has already been finished
[31:12] I'm just going to go htps uh back to my
[31:15] n8n flow I'm actually going to build an
[31:18] n8n web hook give that a click I'm not
[31:22] going to have any authentication let me
[31:24] just turn all this off basically what we
[31:26] want is we we want to use this as our
[31:27] test
[31:29] event we're going to go back to the
[31:32] API paste this in
[31:35] save and I'm just going to want to give
[31:36] it a test endpoint here so
[31:39] test looks like um the push notification
[31:42] was failed the reason why is because
[31:44] it's saying this web Hook is not
[31:45] registered for post request did you mean
[31:47] to make a get request beautiful thank
[31:48] you naden we absolutely did so I'm going
[31:50] to stop listening change your HTTP HTTP
[31:53] method here to post there's basically
[31:54] two ways to call a website and this is
[31:55] one of them I'm going to listen for test
[31:57] events go back here and then
[31:59] retest awesome looks like we've now
[32:01] triggered the beginning of our workflow
[32:03] using this data let's see what sort of
[32:05] information was in
[32:06] it okay great we have the scraping job
[32:09] ID the status execution
[32:11] mode okay great so we basically have
[32:13] everything we need now to set up a flow
[32:16] where we can schedule something in this
[32:17] web scraper service that maybe monitors
[32:19] some I don't know list of e-commerce
[32:21] product or something every 12 hours and
[32:23] then we can set up a web hook in NN that
[32:24] will catch the notification get the
[32:27] update now we can do is we can ping um
[32:30] we can ping the web scraping API which
[32:32] I'll show you to set up in a second to
[32:34] request the data from that particular
[32:35] scraping run and from here we can take
[32:38] that data do whatever the heck we want
[32:39] with it but obviously let me show you an
[32:41] example of what the the actual data
[32:42] looks like so we just got the data from
[32:44] web hook let's set up an HTTP request to
[32:48] their API now where we basically get the
[32:50] ID of the thing and then we can call uh
[32:53] we can call that back so got my API
[32:55] token over here I'm going head over to
[32:56] their API documentation first okay and
[32:59] then what we want to do is download
[33:00] these scrape data in CSV format at least
[33:03] in my case I imagine most of you guys
[33:04] are going to add this to a spreadsheet
[33:05] or whatever um you can very easily do
[33:08] whatever you want there's also a Json
[33:09] format endpoint here um but let's just
[33:12] do CSV for Simplicity so I've already
[33:14] gone ahead and I've gotten the method
[33:16] which was a get request so I've added
[33:18] that up here the URL was this over here
[33:21] with the scraping job ID and then your
[33:23] API token there so what I've done is
[33:25] I've grabbed the API token and the
[33:27] scraping job ID I mean I hardcoded it in
[33:28] here just while I was doing the testing
[33:30] let's actually make this Dynamic now
[33:33] drag the scraping job ID right over here
[33:36] voila and then the API token if you guys
[33:38] remember back here on the API page you
[33:41] have your access to API token so just
[33:42] copy that
[33:43] over uh great and now if I run this I'm
[33:47] actually selecting that specific job
[33:49] then from here we have all the data that
[33:50] we just scraped as you can see there's
[33:51] like a uh the way that CSV Works
[33:54] actually let me just copy this over here
[33:55] I just wanted to give this to you guys
[33:56] as an example of a different data type
[33:58] but maybe some people here aren't really
[34:00] familiar with it basically the way that
[34:01] it works is if I just paste this into
[34:02] like a Google sheet you see how it looks
[34:04] like this what what you can do is if you
[34:05] just um split the text to columns you
[34:08] kind of see
[34:09] how kind of see how there's like these
[34:11] four pettings there's web scraper order
[34:13] web scraper startup products and product
[34:15] descriptions right I'm imagine scraping
[34:17] this for some lead genen applica sorry
[34:19] some some e-commerce application list of
[34:21] products here product descriptions maybe
[34:23] product prices maybe product whatever
[34:24] the heck you want um so yeah you can you
[34:27] can put in like a number of formats and
[34:29] I just wanted to give you guys an
[34:29] example what that looks like the next
[34:31] way to scrape websites in naden is using
[34:33] appify if you guys are no strangers to
[34:35] this channel you know that I do appify
[34:37] all the time and I talk about them all
[34:38] the time because I think that they're
[34:39] just a great service um they've now
[34:41] given me a 30% discount where anybody
[34:44] can use it for I was initially under the
[34:46] impression it was lifetime I think it's
[34:47] three months so you probably get 30% off
[34:48] your first three months just check the
[34:50] um description if you want that but Cent
[34:52] how appify is is it is a Marketplace
[34:54] very similar to Rapid API um although
[34:56] extraordinarily well Main ained and they
[34:58] also have a ton of guides set up to help
[35:00] you get you know up and running with
[35:02] scraping any sort of application so just
[35:05] as we had earlier we have Instagram
[35:06] scrapers we have Tik Tok scrapers we
[35:09] have email scrapers we have map scrapers
[35:12] Google Maps we could do I don't know
[35:15] Twitter
[35:15] scrapers uh medium scrapers right
[35:18] basically any any service out there that
[35:22] has this Dynamic aspect to it that's not
[35:24] a simple HTTP request you can make you
[35:25] could scrape it using ampify and then
[35:27] obviously you you have things too like
[35:28] just like basic website crawlers you can
[35:30] generate screenshots of sites I mean
[35:32] there's just there's so many things let
[35:33] me walk you guys through what it looks
[35:35] like now in my case I'm not actually
[35:37] going to sign up to appify because I
[35:38] have like 400 accounts but trust me when
[35:40] I say it is a very easy and simple
[35:41] process you go to app ay.com you go get
[35:45] started you put in your email and your
[35:47] password they'll give you $5 in free
[35:50] platform credit you don't need any
[35:51] credit card and you can just get up and
[35:53] running and start using this for
[35:54] yourself super easily then the second
[35:56] that you have all that you'll be Creed
[35:57] with this screen it is a console screen
[36:00] don't be concerned when you see this um
[36:03] you know this is super simple and and
[36:05] easy and and not a big deal this is one
[36:06] of my free accounts um so I just wanted
[36:08] to show you guys what you can do with a
[36:09] free account uh but from here what you
[36:12] do is you go to the store and as you can
[36:13] see I'm just dark mode all this is the
[36:15] same thing we were just looking at
[36:16] before and then um we're just going to
[36:17] run a test on the thing that we want to
[36:19] scrape okay so what I'm going to want to
[36:21] do is for the purposes of this I'm now
[36:23] going to do something different from
[36:25] what I was doing before like which was
[36:26] just left click over and over and over I
[36:27] think that kind of gets boring what I'm
[36:29] going to do is I'm going to scrape
[36:30] Instagram posts okay so what I'm going
[36:32] to do is I'm going to feed in a name
[36:34] nickf this is just my um
[36:38] Instagram uh which almost hit 10K in God
[36:41] like 15 days or something like that but
[36:42] I'm going to feed in my Instagram here
[36:44] and then I'm just going to grab like I
[36:45] don't know the last 10 posts okay save
[36:48] and start this is now going to run an
[36:49] actor actor is just their term for
[36:51] scraper which will go out it'll extract
[36:54] data from my Nick surve Instagram and as
[36:57] you can see will get a ton of fields
[36:58] caption owner full name owner Instagram
[37:00] URL comments count first comment likes
[37:02] count timestamp query tag we get
[37:05] everything from these guys which is
[37:06] really cool this might take you know 30
[37:08] 40 50 seconds we are spinning up a
[37:10] server in real time every time you do
[37:12] this as you see in bottom left hand
[37:13] corner there's a little memory tab which
[37:14] shows that we are legitimately running a
[37:16] server with one gigabyte of memory right
[37:18] now so generally my recommendation when
[37:19] you use appify is not to use it for
[37:21] oneoff requests like this feed in 5 to
[37:23] 10 15 20 Instagram Pages uh but you know
[37:26] I just got the back and voila we we have
[37:28] it it's right in front of us we have all
[37:29] of the data of that person's Instagram
[37:32] profile so you can see it's quite
[37:33] scalable in that way um so the question
[37:35] is obviously how do you get this in NN
[37:37] well appify has a really easy to use um
[37:39] API which I like
[37:40] doing all you have is if we wanted to
[37:44] get the uh let's see get data set items
[37:47] okay all I'm going to do is I'm just
[37:49] going to copy
[37:50] this go back here and then connect this
[37:53] to an HTTP request
[37:55] module as you could see we have this big
[37:58] long field here with my API appify API
[38:01] token and this specific data set that
[38:03] I'm looking for I'll show you how to get
[38:04] it dynamically but I just wanted to like
[38:06] allow you to see how to get data in
[38:07] naden really quickly now if we go to the
[38:10] schem of view we can see we legitimately
[38:12] we we already have all of the data that
[38:13] we we had from appify a second ago okay
[38:16] super easy and quick and simple to get
[38:17] up and running um we have the input URL
[38:19] field the ID field the type the short
[38:21] code caption now this is Instagram um
[38:25] every looks like we have some comments I
[38:27] don't have any style how do I create my
[38:29] man you just got to fake it till you
[38:31] make it I don't have any style either
[38:33] just some nerd in my mom's basement uh
[38:36] yeah so you you can scrape any resource
[38:38] you want here um obviously I was
[38:40] scraping an Instagram resource but like
[38:41] if you were scraping something else
[38:43] there'd be no change to this at all no
[38:45] change whatsoever now uh basically what
[38:47] we need in order to make this Dynamic
[38:49] basically make us able to run something
[38:51] in appify and then get it in NN so we
[38:54] need to set up an integration so just
[38:55] head over to this tab set up integration
[38:57] and then all you want to do is you just
[38:59] want to do web hook send an HTTP post
[39:02] web Hook when a specific actor event
[39:03] happens the actor event that we're going
[39:05] to want is basically when the run is
[39:07] succeeded the URL we're going to want to
[39:09] send this to if you think about it we
[39:11] just actually make another web hook
[39:13] request here web
[39:15] hook the URL we're going to want to send
[39:17] it to is going to be this test URL over
[39:21] here now I'm just going to delete all
[39:22] the header off stuff here because um it
[39:24] just uh complicates it especially for
[39:26] beginners um but we're going to copy
[39:28] this over head back over here paste in
[39:30] this URL and then let me see this is a
[39:32] post request I think I don't actually
[39:34] remember so we're going to have to
[39:35] double check I think it's a post
[39:37] request yeah and then what I'm going to
[39:39] do is I'm going to listen for a test
[39:40] event run the test web
[39:43] hook so we're listening we're making a
[39:45] get request okay so the fact that it
[39:47] hasn't connected yet probably tells me
[39:48] it's a post request so let's move over
[39:50] here move this down to post now let's
[39:52] listen to a test event let's run this
[39:55] puppy one more time so we just
[39:57] dispatched it and yeah the post request
[39:59] succeeded and what did we get we got
[40:00] tons of information we got a body with a
[40:02] user ID created at event data joke right
[40:05] looks like when you test something out
[40:06] they just send you a joke about how
[40:08] Chuck nurse can sketi a cow in two
[40:10] minutes have you ever heard of the word
[40:11] sketi before this moment I haven't I
[40:13] want to be known for my ability to sketi
[40:16] we'll go Instagram website
[40:19] scraper okay and now if we go back here
[40:22] right we're now listening for a test
[40:24] event so I'm going to listen for this
[40:26] test event I'm going to run the same
[40:27] scraper again maybe we'll make it five
[40:28] posts per profile just to make it a
[40:29] little
[40:30] faster and um once this is done what
[40:33] it's going to do is it's going to send a
[40:35] record of all the information we need to
[40:37] get the data over to Ann we're going to
[40:40] catch that information and then we're
[40:42] going to use it to query the the the
[40:44] database basically that it created for
[40:45] that particular Instagram run which will
[40:48] then enable us to do whatever the heck
[40:49] we want with it so it's now starting to
[40:51] crawl as we see here we had five
[40:53] requests so it should be able to do this
[40:54] in like the next 5 seconds or so okay
[40:56] and once that's done we now have an
[40:57] actor succeeded event um and then we
[41:00] have uh let me see the data that we want
[41:04] would be the default data set ID down
[41:06] over here so if we just go to that next
[41:08] HTTP request node what I can do is I can
[41:11] feed that in as a variable right
[41:14] here let going to a default data set
[41:18] ID drag that in between these two little
[41:23] lines and now we can test that step with
[41:25] actual live data now we have everything
[41:26] that we need
[41:27] so I don't know maybe now you want to
[41:29] feed this into Ai and you want to have
[41:30] ai tell you something about the last
[41:32] five posts tell you wow those last five
[41:34] posts were amazing Nick I loved the
[41:37] specifically the one on Korea and I just
[41:39] wanted to send you over some quick
[41:40] assets to help you out right you can now
[41:42] do super Dynamic and structured Outreach
[41:46] you could take that data and use it to
[41:48] like draft up your own post I mean the
[41:50] options are ultimately unlimited that's
[41:52] why I love appify so much the sixth way
[41:54] to scrape websites with NN is data for
[41:57] Co this is another thirdparty service
[41:59] but it's a very high quality one that's
[42:00] specifically geared towards search
[42:02] engine optimization requests you guys
[42:04] haven't seen data for SEO before it's
[42:06] basically this big API stack that allows
[42:08] you to do things like automatically
[42:10] query a service maybe some e-commerce
[42:12] website or some content website and then
[42:14] like extract things in nicely structured
[42:16] formatting um again specifically for SEO
[42:19] purposes tons of apis here as well I
[42:21] mean a lot of these services are now
[42:22] going towards like more Marketplace
[42:24] style stuff but just to give you guys an
[42:26] example you could like Google really
[42:28] quickly to scrape a big list of Google
[42:30] search results for a term and then you
[42:31] could like feed that into one of any of
[42:33] the other scrapers that we set up here
[42:34] to get data on stuff you could go Google
[42:36] Images Google Maps you could do Bing BYO
[42:39] YouTube Google's uh their own data set
[42:41] feature I don't really know what that is
[42:42] but I imagine it's pretty cool uh and
[42:44] then you can you can take this data and
[42:45] do really fun stuff with it so I'm just
[42:47] going to click try for free over here in
[42:48] the top right hand corner show you guys
[42:50] what that looks
[42:52] like and as you see here um I signed in
[42:55] to data for SEO to my own account looks
[42:57] like I have 38 million bajillion
[43:00] dollars um but obviously you'd have to
[43:03] go through the rig Rolla creating your
[43:04] own account so why don't actually just
[43:06] do that with you and then I'll just use
[43:07] that account that is 38 million
[43:08] bajillion dollars we'll click try for
[43:11] free we'll go Nikki
[43:15] Wiki uh let's use a different email I
[43:17] need a business email huh that's
[43:21] unfortunate okay I do agree to the terms
[43:23] of use
[43:25] absolutely uh bicycle
[43:28] is that a bicycle that's not a
[43:32] bicycle what does it mean when I can't
[43:35] answer these does it mean that I'm a
[43:38] robot if you look at some of my posts
[43:40] some of my comments people would
[43:42] absolutely say yes it means that that
[43:45] you're a robot um I don't know why
[43:47] people keep saying stuff like dude Nick
[43:49] nice AI Avatar bro but I'm it's not an
[43:52] AI Avatar it's not an AI Avatar at all
[43:55] it's actually just me okay anyway so I
[43:57] need to activate my account doesn't look
[43:59] like it allows you to feed in the code
[44:01] here so I'm just going to feed it in
[44:02] myself uh it's obviously you're getting
[44:04] a lot of spammers hence
[44:05] this um bicycle stuff I don't know why
[44:09] the code isn't working here let me just
[44:12] copy this link address paste it in here
[44:15] instead there you go okay great so now
[44:17] you can sign
[44:19] in and once you're in you got also um
[44:23] they're actually really big on on
[44:24] bicycles they're training um a model to
[44:26] convert all ads on planet Earth into
[44:28] bicycles they'll actually give you a
[44:29] dollar worth of API access uh credits
[44:32] which is pretty cool um I'm not going to
[44:34] do that I'm just going to go over to
[44:35] mine which is$ 38 million bajillion
[44:36] dollars with 99,999 estimated days to go
[44:40] um and yeah let's actually run through
[44:41] this the first thing that I recommend
[44:42] you do is go over to playground on the
[44:43] Le hand side there's all of their
[44:45] different API endpoints that you can
[44:47] call um what I'm going to do is I'll
[44:48] just go to serp for now just to show you
[44:50] that you could scrape Google with this
[44:51] pretty easily so maybe I'm in the UK and
[44:54] I want to scrape um let me see
[44:58] a keyword ni arrive okay then I'm going
[45:01] to send a request to this
[45:03] API there's there's a bunch of other
[45:05] terms here that are going to make more
[45:06] sense if you're a SEO person um but now
[45:09] we receive as output a structured object
[45:12] with a ton of stuff right we have um the
[45:15] first result here it's like an organic
[45:16] one with some big URL a bunch of chips
[45:20] um I'm I have like a Knowledge Graph
[45:22] profile which is cool apparently it
[45:23] finds it says I'm a freelance writer um
[45:27] you know we have a bun bunch of data
[45:28] here bunch of data you know you can use
[45:30] this to get URLs of specific things and
[45:32] then with the URLs you can then feed
[45:34] that into scrapers um that do more like
[45:36] I talked about earlier maybe appify or
[45:38] maybe rap API maybe fir crawl so a lot
[45:42] of options here to like create your own
[45:43] very complex
[45:45] flows you can do other stuff as well um
[45:48] you grab a bunch of keyword data so
[45:50] maybe you wanted to find a keyword and
[45:53] maybe again it's Nicks or location you
[45:55] want let's do United States that'll
[45:57] probably be
[45:58] better language um I'm just not going to
[46:01] select an language and then I'll do a
[46:03] request so now it's going to find us um
[46:06] a bunch of search volume related stuff
[46:08] so I don't actually know how many people
[46:10] are searching for me in 2025 apparently
[46:13] 390 is this per month H wonder if it's
[46:16] per month per day that's interesting uh
[46:20] I don't really know why they break it
[46:21] down by like the month date yeah looks
[46:24] like it's 390 per month so to the 390
[46:26] people that are Googling me who are you
[46:28] and what do you want I'm just kidding um
[46:31] you can do things like you could find
[46:32] back links so you could find links um
[46:35] for I believe you feed in a website URL
[46:38] and then it finds back links to that
[46:40] website so this is you technically now
[46:41] scraping a bunch of other websites
[46:43] looking for links to the specific
[46:45] resource that you have that's kind of
[46:49] neat it looks like that found it
[46:51] basically immediately which is really
[46:52] really
[46:54] cool and it looks like they're referring
[46:56] top level links that are Dooms BG bgs
[46:59] would be interesting I wonder where
[47:00] that's coming from um there's a Content
[47:03] generation API playground so you could
[47:05] you know feed in some text and then have
[47:07] it generate other stuff but I think
[47:08] we're kind of getting away from um the
[47:10] actual thing that matters which is the
[47:11] scraping of the uh scraping of the
[47:13] websites so yeah lots of stuff lots of
[47:16] stuff for sure now that's all good um
[47:18] but let's actually turn this into an API
[47:20] call if we head over to the API of do
[47:22] data for SEO so in my case docs. datafor
[47:24] seo.com V3 _ page SL contentor parsing
[47:30] live that's what I'm I'm curious about
[47:32] you'll see that we have a post request
[47:33] that we need to send to this URL um well
[47:36] I have a curl just like this which I can
[47:39] feed into um an API request that's what
[47:42] I'm going to do so I'm going to go back
[47:43] over here and I'm just going to import
[47:45] this curl
[47:47] import and it's going to go through and
[47:49] it's basically going to um parse out all
[47:52] these fields that I'm interested in with
[47:53] the URL which I'll go htps
[47:57] left click. AI um and then we have sort
[48:00] of like a gacha here that a lot of
[48:01] people don't understand this is the um
[48:03] authorization the authorization is a
[48:05] little bit different from most of the
[48:06] easy authorizations we've had so far we
[48:07] actually have to convert it um we have
[48:08] to go one one more step basically to
[48:10] make this work if I check out the let's
[48:15] see um
[48:17] authorization
[48:19] here what we need is we need to um get
[48:23] the login and then the P so this is your
[48:26] username and then your password then we
[48:28] have to Hash it or not hash it but we
[48:29] have to convert it into something called
[48:30] base 64 um this is just how they do
[48:33] their API key stuff I guess it's kind of
[48:35] annoying but it's just part and parcel
[48:37] of working with some apis you're just
[48:38] not always going to have it available to
[48:40] you really easily so I'm just going to
[48:42] go back to data for SEO and then I'm
[48:45] going to grab my credentials okay so
[48:47] what we need to do is we need to base 64
[48:50] encode the username and the password um
[48:52] I'm just going to leave that at what
[48:53] I've done is I've actually gone through
[48:54] and done it in this edit Fields node um
[48:57] basically what you need to do is you
[48:58] need to have your username or your login
[49:02] so maybe this is me searching Nix or
[49:04] have Reddit uh Nick left click. so that
[49:07] might be my username and then my
[49:09] password is What's called the API
[49:10] password you can find that really
[49:11] quickly and easily just by going over
[49:13] here to API access and then API password
[49:16] if you just signed up it'll be visible
[49:17] right here if it's been more than 24
[49:18] hours you actually have to send it by
[49:19] email but anyway so that's um that's
[49:21] where i' get the API password from uh
[49:24] and then once you feed it in over here
[49:26] where you're going to want to do is
[49:27] you're going to want to base 64 encode
[49:28] it like this they just require you to
[49:31] use these creds um or to operate with
[49:35] these creds as base 64 encoded versions
[49:36] Bas 64 is just a way to like translate
[49:38] into a slightly different number format
[49:40] so once you have that you would just
[49:41] feed in the variable right over
[49:43] here Ju Just as follows and then you can
[49:46] make a request to their API and receive
[49:47] data so uh it looks like I was doing
[49:50] their content parsing live you know what
[49:53] I wanted to do is I just wanted to
[49:55] call their endpoint which I think was
[49:58] their like instant
[50:02] Pages this one right over here so it's
[50:04] just V3 uh once you've sorted this out
[50:06] by the way the AP gets like
[50:07] extraordinarily easy to manage you just
[50:09] need to like figure out the
[50:09] authentication from there on out all
[50:11] you're literally doing is just swapping
[50:12] out the requests so you know if you
[50:14] wanted to do instant Pages all I'm doing
[50:16] is pumping that in there I just sent a
[50:19] request and now I receive a bunch of
[50:20] links with different headings and and so
[50:22] on and so forth that's easy the seventh
[50:24] way to scrape websites and Ed end is
[50:25] using a third party application called
[50:27] crawl Bas they're known for their
[50:29] rotating proxies which allow you to send
[50:31] very high volume um API requests so um
[50:33] it's very proxy driven this is their
[50:35] website so it's a scraping platform
[50:37] similar to Rapid API um and uh you know
[50:40] appify they support many of the major
[50:42] websites here and um the reason why
[50:44] they're so good at this is just because
[50:45] they you know as I mentioned they rotate
[50:47] the hell out of these proxies so we're
[50:49] just going to sign up to Tri it free
[50:51] I'll use my business email
[50:54] here and then continue with Emil
[50:57] email we got to add a phone number
[51:00] obviously we're going to do less than a
[51:01] thousand I'm a CTO I don't want to
[51:04] what's the animal right is that an
[51:06] animal yes it's an animal good God beep
[51:10] boop uh we're going to head over to my
[51:14] Gmail and receive this
[51:18] now so we need to confirm my account
[51:20] just going to copy this link address
[51:22] that I can do this in one
[51:23] page awesome we should be good to log in
[51:26] so that's what's
[51:27] happening we need to select the animal
[51:29] again just doesn't it doesn't believe
[51:31] really just doesn't
[51:34] believe okay great so now we have a
[51:36] crawling API smart proxy thing if you
[51:39] guys want to run like uh I don't know
[51:41] use in apps that have a proxy field
[51:43] specifically I'm just going to keep
[51:44] things simple we're doing this in n8n so
[51:46] we're going to go crawl base API we have
[51:47] a th000 free crawls remaining very first
[51:50] thing we're going to want to do is just
[51:51] click start crawling now just to get up
[51:52] and running with the
[51:54] API um and as you see here the these
[51:56] guys have probably one of the simplest
[51:57] apis possible all API URLs start with
[52:00] the folling base part click and then all
[52:02] you need to do in order to make an API
[52:04] call is run the following sort of line
[52:06] so this is a curl request obviously
[52:08] we're in n8n and one of the value
[52:11] valuable parts of NN is we can just
[52:12] import a COR request so well I'm going
[52:14] to import it as you can see here we have
[52:17] a token field then we just have the URL
[52:19] field of the place we want to crawl so
[52:21] I'm going to do left click. for now um I
[52:24] don't know if this token field was
[52:26] actually my real token I don't believe
[52:27] so maybe we'll give it a try maybe it's
[52:29] like a test token or
[52:31] something so I'm now running this and it
[52:34] looks like we just received a bunch of
[52:36] very spooky data I don't like the spooky
[52:38] data no spooky data for
[52:41] us um sometimes spooky data like
[52:47] this H this seems kind of weird to me
[52:49] actually just give me one second to make
[52:51] sure that's right we are receiving a
[52:53] data parameter back which is nice but
[52:56] yeah something about this is a little
[52:57] bit spooky um was it a get request or
[53:00] was it a post request no I guess it's a
[53:01] get request
[53:03] strange very very
[53:06] strange okay anyway they give you two
[53:09] types of tokens here um this is why I'm
[53:11] talking about it to begin with I'm also
[53:12] because I just used it before for a
[53:13] couple of applications and I found it
[53:15] very easy they give you a normal token
[53:16] and they give you a JavaScript um token
[53:19] as well so the reason why that's
[53:20] valuable is because if you're scraping
[53:22] one of these websites I talked about
[53:23] before where when you send a simple HTTP
[53:25] request nothing pops up like this is the
[53:27] this is the purpose of this you actually
[53:28] feed in a JavaScript token um when you
[53:31] use the JavaScript token it'll
[53:32] automatically launch a browser instance
[53:34] inside of um craw base for you so
[53:37] instead of you getting just like that
[53:39] empty thing back that I mentioned I'm
[53:40] you're actually going to get uh you're
[53:42] going to get like a JavaScript version
[53:44] of the website where somebody went on
[53:46] the website it loaded really briefly and
[53:48] then they grabbed the code
[53:50] afterwards so yeah we have some some API
[53:52] call stuff over here um this one's just
[53:54] using Amazon this is pretty interesting
[53:56] so I might actually give that a a go
[53:58] just to give you guys an example of said
[54:00] Amazon
[54:02] scrape uh let's just go
[54:09] www.amazon.com oh right Amazon might be
[54:11] JavaScript actually so maybe we give
[54:13] that a go no it looks like we um we got
[54:15] the data from from Amazon which is
[54:16] pretty cool if you feed that into the
[54:18] markdown converter like we had before
[54:20] it's going to feed in the HTML here pump
[54:23] it into a data
[54:24] key we've now converted this
[54:28] into uh this is very long let's go
[54:30] tabular we've now converted this into
[54:32] markdown which is cool and this is
[54:34] pretty long right obviously has all of
[54:36] the images and has all of the
[54:38] information on the site which is cool
[54:39] and then we can feed it into open AI
[54:41] like I did
[54:42] before where I message a model and I'm
[54:46] just going to copy um from my uh
[54:48] previous application here to make my
[54:49] life a little bit
[54:50] easier where the heck are you
[54:56] and then we're just going to feed in the
[54:58] code here and then because I didn't feed
[55:02] in this we should now run this we're
[55:06] going to grab data from the site and
[55:08] we're going to try and I mean you know
[55:09] we kind of all know what Amazon is and
[55:11] what it does right so I'm not expecting
[55:13] expecting anything spectacular but it's
[55:15] still going to go it's going to give me
[55:16] all of the text on this Amazon page and
[55:18] then I'm going to get a bunch of list of
[55:19] links basically absolute URLs ideally
[55:22] should play some Jeopardy
[55:25] music or going be able to play um Star
[55:28] Wars music that'd be kind of cool okay
[55:31] we now have a schema with all of the
[55:33] links on the page which is pretty cool
[55:34] we have the plain text website copy we
[55:37] have a on line summary uh you know plain
[55:39] text website copy is a lot longer than
[55:41] this obviously it's just shortening and
[55:42] truncating it for us but yeah very quick
[55:44] and easy way to use crawl base for this
[55:46] now the value in crawl base is not
[55:48] necessarily just to send them to static
[55:49] websites like I talked about it's to use
[55:51] like highly scalable scraping where
[55:55] you're scraping any applications
[55:58] consistently um as you see here the
[56:00] average API response time is between 4
[56:02] to 10 seconds so you you will receive
[56:03] results back pretty quick if you wanted
[56:05] to just send one request or 20 requests
[56:08] every second think about it like 20
[56:10] requests a second times 60 seconds a
[56:12] minute is 1,200 requests times 60
[56:14] minutes and an hour 72,000 requests
[56:16] right um sorry just jumping around the
[56:18] place here you can send 72,000 requests
[56:20] basically an hour which is crazy um and
[56:23] you can do so as quickly and as easily
[56:24] as just like adding an API call like
[56:26] and then it'll automatically distinguish
[56:28] between like a a plain text thing or a
[56:30] JavaScript thing okay the eighth way to
[56:31] scrape data in nadn specifically website
[56:34] resources is octop parse octoparse is
[56:37] very similar to some of the other
[56:38] services that we've talked about um it
[56:40] is a web scraping tool that actually
[56:41] gives you quote unquote free web
[56:43] crawlers and I'm just a fan of their ux
[56:44] I think it's very clean I think the way
[56:46] that they have their signup flow and
[56:47] stuff's really easy so if you made it to
[56:49] this part of the uh tutorial and you
[56:50] have yet to sign up to one of these
[56:52] Services give octoparse um give
[56:54] octoparse your thoughts
[56:56] let's double
[56:58] check that I haven't actually created an
[57:00] account using this no I haven't
[57:01] fantastic so I should be able to jump
[57:03] through and show you guys what this
[57:05] looks like we have a verification code
[57:07] I'm going to paste in if you're not
[57:09] familiar with jumping around and stuff
[57:11] like this um or if you're wondering how
[57:13] I'm jumping around I'm just using a
[57:14] bunch of website
[57:19] hotkeys okay great account is now ready
[57:22] so we can start a free premium trial if
[57:24] you want I think you're going to have to
[57:25] add a card um I don't know if I have
[57:26] enough credits to actually do anything
[57:29] but if I'm not then I'll start that
[57:30] trial in a second what you're going to
[57:32] have to do in order to make this work is
[57:33] you're going to want to have to download
[57:34] you're going to want to download the
[57:36] octoparse desktop app so let's give it a
[57:39] quick and easy go just going to drag
[57:42] this puppy um if you are using something
[57:45] that is not Mac OS you will not have
[57:47] this strange drag and drop feature here
[57:50] once that is done you will have octo
[57:51] parse accessible just open that up yes I
[57:54] want to open this thank you
[57:57] and the cool thing about
[57:59] octoparse um kind of relative to what
[58:03] else you know like the other scraping
[58:05] applications I talked about is this is
[58:07] just running in a desktop app um like
[58:09] kind of in in your
[58:10] computer so like it's cool because it's
[58:13] just easy to get up and running with um
[58:14] and it's also local as opposed to a lot
[58:16] of these other ones which are not so I'm
[58:19] going to Auto log in on my desktop app
[58:21] remember my password beautiful the
[58:22] simplest and easiest way to scrape a s a
[58:24] service is just to pump in the the URL
[58:26] here then click Start and basically
[58:28] what'll happen is um it'll actually
[58:30] launch like an instance of your browser
[58:31] here with this little tool that allow
[58:33] you similarly the web scraping Chrome
[58:35] extension select the elements on the
[58:36] page you want scraped so I don't know
[58:39] maybe I want these logos scraped the
[58:41] second that I tapped one you'll see it
[58:42] automatically found six similar elements
[58:45] so now I'm actually like scraping all of
[58:47] this stuff okay now we have access to
[58:50] this sort of drag and drop or um
[58:52] selector thing similar to what we had
[58:55] before if you click on one of these
[58:56] you'll see it allow you to select all
[58:58] similar
[58:59] Elements which is pretty sweet and then
[59:02] um you can also do things like click
[59:04] elements and so on and so forth extract
[59:06] the text Data here um you can also tie
[59:09] that to other things right so as you see
[59:12] I'm now mapping each of these very
[59:13] similarly to how I was doing before
[59:15] between the first field which is the
[59:17] title of the product and then the second
[59:18] field which is like the field to uh so
[59:21] that's pretty sweet we could do the same
[59:22] thing with a number of things you could
[59:23] extract like the headings and then the
[59:25] values and so on and so on and so forth
[59:27] but I'll kind of leave it there um so
[59:29] once you're done selecting all the
[59:30] elements that you want all you do is you
[59:31] click run and you have a choice between
[59:33] running it on your device versus running
[59:35] it on the cloud so um on the cloud is
[59:38] API supported that's how you're going to
[59:39] get stuff in NM but I just want you guys
[59:40] to know that you can also just run it
[59:41] here you could run it here load up the
[59:43] URL scrape all the things that you want
[59:45] on the specific page you're feeding in
[59:46] and then you can be done with it so I
[59:48] just selected run in the cloud it's now
[59:49] going to open up said Cloud instances as
[59:51] we could see we have this little field
[59:53] where it's running and extracting the
[59:54] data we're now done so I can export this
[59:58] data locally um but I could also do a
[60:00] lot of other stuff which we'll show you
[60:01] in a second so um you can dump this
[60:03] automatically to Google Sheets you could
[60:05] do zapier to connect to Google Sheets do
[60:07] like some sort of web Hook connection
[60:08] export to cloud storage uh similar stuff
[60:10] to the the web scraping Chrome extension
[60:13] um but for now let's just export this as
[60:16] Json give ourselves a little ad Json
[60:19] file here thank
[60:21] you and yeah now we have it locally now
[60:23] in order to connect with the octop par
[60:24] CPI what you're going to have to do is
[60:26] first you get up to request an access
[60:28] token the way that you do this is you
[60:29] send a post request to this URL here and
[60:33] the way that you format it is you need
[60:34] to send your username your password and
[60:38] then have the grantor type as password
[60:40] okay now password obviously just put in
[60:42] whatever your password is don't store it
[60:44] in PL text like I'm doing um with my
[60:45] hypothetical password put it somewhere
[60:47] else and then grab that data and then
[60:49] use it um but the the output of this is
[60:51] we have this big long access token
[60:52] variable which is great after that if I
[60:55] just go back to their API here um once
[60:57] we're here we can actually extract the
[60:58] data that we need so basically the thing
[61:01] that you're going to want is you're
[61:02] going to want um get data by Offset you
[61:04] can also use get non-exported data which
[61:06] is interesting so I think this just like
[61:07] dumps all of the data as not exported um
[61:10] and then sends that over to you I
[61:12] believe but anyway you could also get
[61:14] the data by offset so if I go a get
[61:17] request to open api. octop course.com SL
[61:20] all and then I just send a header with
[61:21] the URL parameter this is a get request
[61:25] uh we're going to send a header with the
[61:27] token so
[61:29] authorization
[61:30] Bearer and then feed in the access token
[61:33] here just make sure that this is just
[61:35] one space no it's
[61:37] two if I feed this in um it's saying
[61:41] that it's a bad request let me just
[61:42] triple check why I think we need three
[61:46] Fields yeah I think we need three Fields
[61:49] actually my bad um we need uh this get
[61:53] request then we need the authorization
[61:56] header like I talked about then we need
[61:58] three Fields task ID yeah right
[62:00] obviously we need to feed in the task ID
[62:01] so you need task ID offset or size so um
[62:05] we'll feed this in as query parameters
[62:07] here so send query parameters the first
[62:10] value was task ID second one was
[62:14] offset and uh offset is no Capital the
[62:18] third was size offset's going to be zero
[62:20] size going to be I don't know let's just
[62:22] do 1,000 and what we need now is we need
[62:24] the task ID of the specific run that we
[62:26] just finished oh in order to get the
[62:27] task list you head over to task list top
[62:29] right hand corner here task ID API so we
[62:32] now have access to this so if we go back
[62:35] to our NN instance we could feed that in
[62:37] here by test the step you'll see that we
[62:40] now have all the data that we just asked
[62:42] for earlier so a variety of ways to do
[62:44] this um in practice like octop par
[62:45] allows you to schedule runs you could
[62:47] schedule them um using their you know
[62:49] whatever it is uh uh like cloud service
[62:52] you could use it to scrape I don't know
[62:54] Twitter uh they have a variety of like
[62:56] other scrapers um that you can check out
[62:58] just heading over
[63:00] to this new here uh if we just go sorry
[63:03] go down to templates um there's a
[63:05] variety of other ways to scrape Google
[63:06] job scraper glass door scraper super
[63:08] Pages scraper you could schedule these
[63:10] right and then what you can do in na is
[63:11] you can just query it once a day grab
[63:13] all the data like I showed you how to do
[63:14] a moment ago dump that into some sheet
[63:17] octoparse is pretty cool it's like more
[63:18] of like an industrial Enterprise level
[63:20] application um to be honest so there
[63:23] might be some gotas if you're not super
[63:24] familiar with working with like desktop
[63:27] apps and stuff but I I like the idea
[63:28] that you can also just scrape locally
[63:30] which is pretty sweet and the last of
[63:31] our nine best ways to scrape websites in
[63:33] nadn is browserless now browserless runs
[63:36] a headless Chrome instance in the cloud
[63:38] this stuff is great for dynamic or heavy
[63:40] JavaScript websites if you've never used
[63:42] browser list before the cool part about
[63:45] browser list is allows you to actually
[63:46] bypass captas which is a big issue that
[63:48] a lot of people have um so I'm going to
[63:50] click try it for free I'm going to enter
[63:52] my email address over here verify I need
[63:56] to submit a code so let's head back over
[63:58] here thank you thank you thank you thank
[64:02] you we have a ton of free trial signups
[64:07] obviously I don't have a promo code or
[64:09] anything don't have a company name I'm
[64:11] just going to enter a password I'm using
[64:13] this to get past uh to avoid setting up
[64:15] a puppeteer and playright server sure
[64:17] I'm going to click complete we're now
[64:19] going to have a th credits inside of
[64:21] browser list which is pretty sweet um
[64:23] and we'll get a we'll get a full plan
[64:25] eventually we now have an API token so I
[64:27] can figure out how all of the stuff
[64:28] works here I'm just going to dive right
[64:29] into the API I can figure out how all of
[64:31] the API stuff works using their API docs
[64:33] which are fantastic by the way um and we
[64:35] don't want to do any of this stuff we
[64:37] just want to do HTTP apis brow list API
[64:39] index okay great so here's where we're
[64:41] at um if you want to send and receive a
[64:42] request what you need to do is uh you
[64:46] send a request to one of these endpoints
[64:48] content unblock download function PDF
[64:53] screenshot scrape or performance
[64:56] what we want for the purpose of this is
[64:58] just uh let's do content okay this is
[65:01] the request right over here so I'm just
[65:03] going to paste my API token up here copy
[65:05] this
[65:06] request feed it into nadn in the HTTP
[65:09] request module as per
[65:11] usual nice quick and easy I'm going to
[65:14] grab my API token and where it says your
[65:16] API token here I'm going to feed that in
[65:19] what I want as a website is just left
[65:21] click. a I'm going to run test step we
[65:24] are now quering the pi and in seconds we
[65:27] have access to the data same thing that
[65:30] we had before but now we're using a pass
[65:33] through and browser list is a great pass
[65:35] through um because you know uh they they
[65:38] allow you to scrape things that go far
[65:40] beyond the usual static site thing so
[65:44] like honestly and I'm just leaving this
[65:46] as a secret and sort of a little I guess
[65:48] Easter egg for people that have made it
[65:49] this far in the video like my go-to when
[65:51] scraping websites is as I mentioned do
[65:54] that HTTP request trans forg that works
[65:55] then do something like fir C.D but if
[65:57] that doesn't work I I do something like
[65:59] browserless that has all of this stuff
[66:01] built in um and I especially use browser
[66:05] list anytime that there's some sort of
[66:07] you know application where I'm just
[66:08] going to save this so I can make all my
[66:10] HTP requests really easy um especially
[66:12] when you know there's issues with captas
[66:14] and and accessing resources and stuff
[66:16] check this out not only can you do um
[66:19] the actual scrape you can do a
[66:21] screenshot of the page as well and
[66:22] because I've entered my token up here
[66:24] the requests that I'm going to setting
[66:25] up are as simple as importing the
[66:28] curl then clicking test step so
[66:31] straightforward we now have a file which
[66:34] is the screenshot now I used example
[66:36] domain there let's go left
[66:39] click. run this test now you can see
[66:42] we've actually like received a
[66:43] screenshot of the of the website
[66:47] view very sexy and my website's pretty
[66:49] long so keep in
[66:51] mind um and yeah you know obviously a
[66:53] lot you could do with that you can
[66:54] download the site you can turn the site
[66:56] into a PDF so um that's pretty neat I
[66:59] don't think I've actually used this one
[67:00] before but for the purposes of this
[67:02] demonstration why don't we give it a try
[67:04] we'll go over here import the curl paste
[67:07] it in voila the website I'm going to do
[67:10] is left click. aai going to test this
[67:14] step so now there servers doing a couple
[67:16] things like I'm scraping the site then
[67:18] converting it all into PDF format um
[67:20] probably screenshotting a bunch of stuff
[67:21] too if I view this now we now have my my
[67:24] file looks like it didn't capture all of
[67:26] the color aspects um that might just be
[67:29] difficult or whatever but I still have
[67:30] like a PDF of the site which is pretty
[67:31] neat um and yeah let you guys kind of
[67:34] screw around with this on your own but
[67:35] there are a variety of cool applications
[67:36] you can use browless for all right I
[67:39] hope you guys appreciated the nine best
[67:40] ways to scrape websites in nadn as you
[67:42] guys could see it's a combination of on
[67:45] platform scraping using the HTTP request
[67:47] module a lot of like API documentation
[67:49] stuff like that if you want to get good
[67:51] at this I'm releasing a master class on
[67:52] API stuff um uh as part of my next na
[67:56] tutorial video uh and then you know
[67:58] navigating this and then and then taking
[68:00] the data from these services and using
[68:01] them to do something that you want to do
[68:03] like artificial intelligence to give you
[68:04] a summary of the site or generate ice
[68:07] breakers for you or do something else um
[68:09] whether you're using a local application
[68:11] like octop parse or maybe the web
[68:13] scraping CH uh Chrome extension or using
[68:16] something like firra browserless appify
[68:19] rapid API and so on and so forth um you
[68:21] now have everything that you need in
[68:22] order to scrape static sites Dynamic
[68:24] sites super Js heavy websites and even
[68:26] social media websites like Tik Tok
[68:28] Twitter and Instagram thanks so much for
[68:29] making it to this point in the video if
[68:31] you have any suggestions for future
[68:32] content drop them down below more than
[68:34] happy to take your idea and run with it
[68:36] assuming it's something that I haven't
[68:37] done before and then if you guys could
[68:39] do me a really big solid like subscribe
[68:41] do all that fun YouTube stuff and I'll
[68:42] catch you on the next video thank you
[68:43] very much

Guidelines:
1. Structure the post in distinct segments:
   - Introduction (hook the reader)
   - Context (why this matters)
   - Key Segments (3-4 main points with timestamps if relevant)
   - Analysis (your professional perspective)
   - Conclusion (call-to-action)
2. Use clear segment separators (e.g., "---")
3. Include relevant statistics or metrics
4. Add industry-specific insights
5. Use professional hashtags
6. Keep the total length between 1000-1300 characters
7. End with the video link

Video Link: https://youtu.be/y-eEbmNeFZo

==================================================
