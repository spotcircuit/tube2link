{
  "main_points": [
    "hey Nick here today I'm going to show you the nine best ways to scrape any website in nadn you're going to be able to scrape static sites Dynamic sites JavaScript social media whatever the heck you want by the end of the video you'll know how to do it I scaled my automation agency to 72k a month using no code tools make and nadn and scraping was a big part of that so this video is just going to give you all the sauce you're going to learn everything you need to scrape websites that on your own let's get into it all I'm going to jump into NN in a minute and build these alongside you and one other thing I'm going to do is I'm going to sign up to all the services in front of you walk you through the Authentication and the onboarding flows and get your API keys and stuff that but just before I do want to explain very quickly the difference between a static site and then a dynamic site because if you don't know this scraping just gets a lot harder and so we're just going to cover this in 30 seconds and we can move on so if this is you you're just this wonderful smiley person and you want to access a static website what you're doing is you're sending a request over to just some document you know think about this as just a piece of paper on a cupboard and there's a bunch of text on this piece of paper and what you do is you say hey can I have this piece of paper and then the piece of paper just comes back to you with all of the information inside of the piece of paper this is a very simplified version of what's going on but static sites are by far the easiest thing to scrape and so this is where you know a lot of people think all websites are are at and then they kind of confuse it with this next step which is dynamic a dynamic site essentially is not that at all what you're doing is you're sending a request to a piece of paper but the piece of paper has nothing on it what happens is this piece of paper then sends a request to some other dude which I guess in this case is just a server really who will then he has a trusty pen in his hand and he'll write all of the stuff on said piece of paper and then you'll get the piece of paper back so there's that intermediate step where you are pinging some sort of you know domain name or whatever then that domain name shoots some code over forces a server to generate all of the contents on that domain and then you get it this is obviously kind of a two-step process and then this is a three-step process so if you just understand that you know when you scrape a dynamic resource what you're really doing is you're sending a request to a page which sends a request back to another server which then fills your thing this element eliminates 99% of the confusion because most of the time scraping issues are hey I just ping this page but I got nothing back you know the HTTP request or or or whatever I I sent you know it was fine but for some weird reason there's nothing on this page of of any substance if that was the case for you it was most likely because this was empty before you sent it over whereas you know simple scraped static resources tend to just give you what you want really quickly and it's really easy all so hopefully we at least understand that there's that difference between static and dynamic sites here I'm not going to go into it more than that we're just going to dive in with both feet start doing a little bit of scraping and then we'll kind of see where we land I find the best way to do this stuff is just by example and and you know being practical about it so the first major way to scrape websites in NN is using direct h HTTP requests this is also what I to think of as the Magic in scraping itself what we're going to do is we're going to use a node called the HTTP request node to send a get request to the website we want this is going to work with static websites and non JavaScript resources so let me give you guys a website that I'm going to be scraping here this is my own site it's called left click I'm about to do a redesign but this is a static resource I know this because I built the site you know I I did it in code and all this is is just a document somewhere on my or on on a server some more so what I'm going to want to do is and I'm just going to pretend that I haven't done any of this so we're just going to go HTTP request HTTP request node looks this we have a method field a URL field authentication field query parameters headers body and then some options down here as all I'm going to do is I'm just going to paste in the website that I want to visit then I'm just going to test the step it's that easy now the response from this on the hand side see all this code over here this is what's called HTML if you're unfamiliar and HTML is just the it's it's the code behind the site so if I were to zoom in over here you see where it says I don't know let's let's go to my website let's just find a little bit of little bit of texture build hands off growth systems if I just command F and paste this in we have that text buried somewhere in this big long HTML string so all that this HTML is is this is the code that is sent to my browser which is Google Chrome in this case then my browser takes the code and it just renders it into this beautiful looking thing beautiful is a subjective State I would say but this wonderful looking thing in front of us which is this website with sizing and the tabs and the divs and all that fun stuff so what I'm trying to say is everything over here on the hand side this is the entire site we can do anything we want with this information and we can carry this information forward to to do any one of our any one of many flows so in my case looking at a bunch of code isn't really very pretty so one big thing that you'll find in the vast majority of modern scraping applications is you'll find that they'll take that HTML which we saw earlier and they'll convert it to something called markdown so this is a markdown node we have a mode of HTML to markdown and all I'm going to do is I'm going to grab that data and I'm going stick it in the HTML section of the HTML to markdown converter what do you think is going to happen when I test this we're going to convert this from this big long ugly super dense thing with a much these greater than and less than symbols and we're just going to convert it into something a little bit shorter a little bit simpler This Is Us just manipulating file formats by the way and I find that manipulating file formats is a big part of what makes a good scraper a good scraper so now we have something in what's called markdown format what's the value there markdown format does two things for us one it's much easier to parse parse just means we can extract different sections of the text we want we can structure it in some sort of other data format and then in my case because I love using AI for everything it's much easier and shorter for us to use with AI so I'm going to give you guys a very simple example where we take this text from the static resource and then we just use AI to tell us something about it so I'll go down to open Ai and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection and grab your API key and paste it in there's some instructions that allow you to do so over here what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the I want to say most cost effective one as of the time of this recording and then what I'm going to do is I'm going to add three prompt I'm going to add a system prompt first I'll say you are a helpful intelligent web scraping assistant then I'm going to add a user prompt and I'll say your task is to take the raw markdown of a website and convert it into structured data use the following format and then I'm going to give it an example of what I want in what's called Json JavaScript object notation format so the very first thing I'm going to do is I'm going to have it just pull out all the links on the website because I find that that's a very common scraping application so I go links and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't do anything for us what we what we want is we want this instead we want left click aka the root of the domain and then left click _ logogram and that's how we get to the actual file asset so if we go back over here so if we go back over here yeah you know we want a link array of absolute URLs and then I'm just going to want main text website copy this is going to be a long string containing all of the website copy containing just the text of the site no formatting and then why don't we do one more thing why don't we have a summarized or summary let's do one line summary just to show you guys you can also use AI to do other cool stuff you could take this oneline summary and feed it into some big sequence you could have ai write an icebreaker for an email you could do a things with this but I'll say on line summary brief summarization of what the site is and how what the site is about let's do that so this is our example I'm going to say your website URL is left click URL for the relative to Absolute conversions is left click.",
    "and then the final thing is I'm going to add one more user prompt I'm just going to draw drag all of that markdown data in here then I'm going to click output content as Json I'm going to test the step I'm going to take a sip of my coffee while this puppy processes and we now have our output on the hand side if we go to schema view what you can see is we've now generated an array of links on the rightand side which contains every link on this website very cool looks the vast majority of these are type form links for some reason don't really know what's about that oh it's because that's the only thing on my website it's just a one P with a bunch of different links to time for that's funny anyway you could obviously just get it to Output one link or tell it make sure all the links are unique or something and then we have a big chunk of plain text website copy then we have a oneline summary of the site so this is a very simple example of scraping we're scraping a static resource obviously but when I build scrapers for clients or for my own business this is always my first pass I will always just make a basic HTTP request to the resource that I'm looking at because if I can make that http request work whether it's a get request or whatever the the the rest of my life building scraper building the scraper is so easy I just take the data I process it usually using AI or some very cheap Tok cheap per token thing and then voila you know we've built out a scraper in this case and it's only taken us what three nodes so that's number one the second way to scrape websites in NN is using a third party service called fir crawl and making an HTTP request to it I'm using something called their extract endpoint but just to make a long story short fire craw is a very simple but High bandwidth service that turns websites into large language model ready data and you know how earlier we had to do HTTP request and then we had to convert all that stuff into markdown and then we had to you know manipulate that markdown what this does is it just does a lot of that stuff for you it'll allow you to go scrape and then it will automatically convert text into markdown for you so that you can do whatever the heck you want they turn it into structure data using Ai and and so on and so on and so forth so if I were to do the same thing that I just did earlier with my own website then I were to you know run an example of this what it would go do is it would spin up a server for me and that would go and generate markdown of the same format the only difference here is it's generated new lines between sections of text how beautiful and then now you know we have the same thing you also get it in Json which is pretty cool and you know you can slot this into any workflow this is the simple and easy way of getting started what we're going to be showing you today is the extract endpoint which allows you to extract data just using a natural language prompt which is pretty cool and from here we're going to be able to take any URL and just turn it into structure data but we're not going to have to know how to parse we're not going to have to know any code we're not going to have to know any of that stuff so let me run through the signup process with you guys go to fire.",
    "Dev here just going to open this up in an incognito to show you guys what this looks all you do is you just go sign up I'm going to add a password we're then going to have to validate this one oh guess these guys are Mega secure so now I'm going to go back to my email address I'm going to count this up and we have a call back here I just need to paste this URL and put it in here that's just because I you know I'm doing this in an incognito tab normally when you do this you're not going to have that step great now we're inside a fir craw they give you I think something 500 free credits something of that nature anyway so what I'm going to do is I'm going to go through and just give give this extracting point just a basic natural language query so let's go from the homepage at left click.",
    "I want to extract a oneline summary of the website let's do all of the text on the website all of the copy on the website in plain text let's do a oneline summary of the website a oneline Icebreaker I can use as the first line of of an of a cold email to the owner and the company name and a list of the services they provide let's do that this is a lot of requests we're asking it to do seven or eight things but all I need to do in order to make this work is I click generate parameters it's going to now generate me a big object with a bunch of things so copy summary Icebreaker company name and now I can go and I can run this this is the URL it just parsed as let's give it a run what it's doing now is it's scraping the pages using their high throughput server I just love this thing I'm not sponsored by fire crawl or anything that but I love their I don't know I just love the design I love this little burning Ember or whatever the heck you want to call it I love how simple they've tried to make everything it's it's great honestly awesome and now you guys see we have a big array with a bunch of sub objects we have a summary I asked for a list of services looks we even have links to the specific places oh links from the resource we have an icebreaker and then we have the company name as so we can do a lot with this but now this is just this is just on on a website how do we bring this in naden it's pretty simple as you see there's an integrate Now button you can either get code or you can use it in zap here what we're going to want to do is we're going to want to run a request to their endpoint and then we're going to want to turn that into our HTTP request let me show you what that looks I'm just going to do all of this stuff in curl so if we go to curl as you can see what we need to do is we need to format a request that looks something this but we need to make sure it's using the extract endpoint so I'm going to go down to extract and then now I have this big long beautiful string what I'm going to do is I'm going to copy this I'm going to go back to my NN instance which is over here and then what I need to do is just open up an HTTP request module and then click import curl just paste all the stuff inside now this is an example request but that's we can use that example request to very quickly and easily format our our real request so we're sending a bunch of headers this is the endpoint that we're calling api.",
    "dv1 extract so what we're doing now is we're we're sending a request to fir craw which will then send a request to the website so kind of a kind of a middleman and then all I'm going to do so if I go back to my example we have an API key here which we're going to need so I'm going to go here and then paste in an API key so that's how that work works authorization is going to be the name value is going to be bear with a capital b space and then the API key and then we also have a body that we need to adjust or edit and this body is where we're going to put the links that we want to have scraped with the extract end point so what I'm going to do is I'm going to delete most of these I'll go back to my left click.",
    "a just this the prompts because you know I was just using their playground before we're going to need to convert this into a request for my service so I'm just going to paste The Prompt in here voila and now we need to put together what's called a schema where we have the objects that we asked for so in my case we asked for copy so I'm going to go Copy Type string then summary so we're going to go summary type string then Icebreaker it's going to be Icebreaker type string then guess what we have last but not least company name which is going to be type string we're also going to want to make these fields required you know you can set it up so they're not required when you do a request a fire call I'm I'm going to make it so they're required so I'll go copy summary Icebreaker and then company name you know what maybe I'll leave company name as unrequired if you think about it logically maybe not all the websites we're going to be scraping using this service are going to have the company names visible on the website I don't know but maybe so maybe I'll leave that as off great so now we have the API request formatted correctly all we need to do at this point is just click test step it looks we're getting a Json breaking error and I think that's because I have this last comma and I'm just going to check to see if there are any commas in Jason you can't have the last element in an array have a comma on it so I think that's let me test it again all so as you can see we just received an ID we've got a success and then we have a URL Trace array which is empty if you think about this logically we don't get all the data that we send immediately because we need fir crawl to whip up the scraper you know do things to the data we could be feeding in 50 URLs here so instead of just having the data available to us now immediately what we need to do is we need to wait a little while wait until it's done and we need to Ping it and the reason why they've given us this ID parameter so that we could do the pinging so the way that you do this is you'd have to send a second HTTP request using this structure so the good news is we could just copy this over and then we can add a second HTTP request I don't know where that went but I guess I'm just going to create it over here I'm going to import the curl to this request just that then keep in mind that we just need to add our API key again because the previous node had it but this one doesn't so just going to go over here I'm going to copy this puppy go back over here I'm going to paste this in now technically what this is called is this is called polling polling is where you know you're you're you're attempting to request a resource that you don't know whether or not is ready and there's a fair amount of logic that I'd recommend putting into a polling flow where when you try it and if it doesn't work you wait a certain amount of time and you retry again for the purpose of this video I'm not going to put all that stuff inside but what I'm going to do is just set up this request I'm going to give this puppy a test let's just feed that in on the back end we got to put the extract ID over here where it said extract ID then I'm just going to give this a test looks I've issued a malformed request we just have to make sure that everything here is specify body let me just make sure there's nothing else in here it was a get request this is a get cool we're not going to send a body then awesome and now we have all of the data available to us automate your business in the copy field summary field left clicks an ad performance optimization agency Icebreaker hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website and then company name left click so quick and easy way you're going to have access to this template obviously without my API key in it and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away and then you just request to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful data that you could use for anything so this is what rapid API looks it's a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data emails phone numbers and stuff that from a website you could ping the ah refs SEO API you could find I don't know unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just a wrapper around what we're doing now in nadn but this website scraper API allows you to scrape some more Dynamic data now I'm not signed up to this so I'm going to have to go through the signup process and I'm going to show you guys what that looks but yeah we're going to we're going to run through an API request to Rapid API which is going to make this a lot easier just going to put in all of my information here and then I'm going to do the classic email verification just copy this puppy over no thank you rise I use a time management app called rise and every time I go on my Gmail I set my Gmail up as a definitely do not do during your workday let's just call it personal projects they don't ask me all these questions my goal today is to browse available apis awesome so that's their onboarding I think we're going to have to pay a little bit of money or something that which I'll sort out in a moment but the scraper that I want is I just want the website one so I'm going to type website in here I'm going to look for wherever it was earlier website scraper API and now check this out what we have is we have the app which is the name of the specific API that we're requesting we have an x-raid api-key and this is the API key we're going to use to make the request then we have the request URL which is what we're pinging and what we can do here is we can feed in the parameters what website we want to we want to scrape and then we can just give it give it a run so I'm going to have to subscribe to this in order to test it I'm just going to go to the basic plan and I'm going to pay money per month that probably seems the simplest way to do so and I just ran through the payment let's head over here and let's just run a test using my website URL we're going to test this endpoint now and now this going to go through Rapid API it's going to spin up the server and then it's going to send it and what we see here is we have multiple fields that Rapid apis or this particular scraper gives us let me just make this easier for you all to see we have a text content field with all of the content of the website which is cool this is what I did earlier but instead of me having to formulate this request try and parse it and try and use AI tokens what I did is I sent the request to rapid API and did it all for me then we also have an HTML content field I think we have one more here scroll all the way down to the bottom as you can see there is a ton of HTML and then we also have a list of all of the images on the website which is very very cool and easily formatted again something that I tried to do manually using AI but now you know we have everything in that nice absolute URL format and then if they find any social media links I don't believe there were more than Twitter but if they find anything that's at their Twitter Instagram whatever then we have the link over here it looks they even give you the scraping time and if they scrape emails or phone numbers they'll be there as so I mean rapid AP is obviously fantastic this is a high throughput sort of thing and why don't we run through what this would look if we were to run a curl request you see how it's automatically just formatting it as curl that just means we just jump back here connect this to my HTTP request module click import curl paste it in this import and it's going to go through and it's going to automatically map all these fields for me query parameter URL left click.",
    "beautiful API key x-raid API host here's the host here's the name of the API key here's everything we need I can just recreate this request now inside of NN as opposed to being on rapid API and then I have all the data accessible to me here how cool is that so we can do this for any any major website really you know there are a lot of specific bespoke scrapers obviously which I don't know if you wanted to scrape let's go back to Discovery if you wanted to scrape Instagram or something you could scrape Instagram you could do Facebook scraping you could scrape these large giants that are quite difficult to do So Meta ad Library Facebook ad scraper and depending on the plan that you're at it might be more cost- effective for you to sign up to some sort of monthly recurring thing rather than just pay two cents every single time you make one of these requests you just kind of got to do that determination yourself if you're scraping I don't know 50 every day or 100 every day or something might be a dollar or two a day which is reasonable but maybe if you want to scrape 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites Dynamic sites all that fun stuff and then you can export that data as a cloud run to then send back sorry big sneeze to then send back to some API or some service and then automatically do parsing and stuff that so very cool I'm going to show you guys what that looks this is sort of a more customized way to build the stuff but I've seen a lot of people do this with naden so we're going to run through what it looks so first thing I'm going to want to do is I'm going to want to let's just go Cloud login or sorry start free 7-Day trial as you can see you know there's a free browser extension here if you wanted to do I don't know highs scale stuff you'd choose probably their project endpoint where we Sorry project plan where we have 5,000 URL credits we can run a bunch of tasks in parallel we could scrape Dynamic sites JavaScript sites we have a bunch of different export options then we can also just connect it directly to all of these what I'm going to do just because I want this to kind of work as a first go is I'm just going to sign up to a free Tri here beautiful just created my account just go left click give it a phone number we'll go left click.",
    "a we're going to go I don't know academic records needed per month we'll go 0 to th000 length of the project I don't know let's go two to 3 months great so now we can import and run our own site map or we can use a premade Community sit map what I'm going to do is I'm just going to import this we're then going to get the Chrome extension web scraper let me add that extension and it's going to download it do all that fun stuff beautiful so now we have it over here I'm just going to pin it to my browser to make my life easier go to left click.",
    "a open up this puppy now there's a bunch of tutorials and how to use this stuff that's not that big of a deal but the thing you need is you need to hold command plus option plus I to open up your developer tools and you'll just find it on the in my case the far so command option I that'll open up Dev tools you see all the way on the hand side here I have a couple other things make and and cookie editor but all the way on the hand side here we have this web scraper thing so we got what you're going to want to do first you're going to want to create a site map for the resource that you're going to want to scrape I'm just going to call it left click and I just want to scrape left click.",
    "once we have our sitemap if I just give a quick little click I can then add a new selector and the really cool thing about this web scraper is if I just zoom out a little bit here what you can do is you can you can select the elements on the website that you want scraped so for instance it's a very quick and easy way to do this if you think about it is just to show you guys an example structure data is sort of an e-commerce application let's say you have the title of a product and you have I don't know the the description of a product so on my website really quick and easy way to do this is let's just call this products and it's a type text what I'm going to do is I'm going to click select then I'll just click on this I'll click on this as and as you see it'll automatically find all of the headings that I'm looking for so that's products we are going to then click data I'm going to click done selecting data preview as you can see it only selected one of them the very first so what we're going to want to do is go multiple and now if I data preview we get all of the headings which is very cool so now we have a a list of headings from here I'm going to save this selector I'm add a new one let's go product descriptions and then going to select this this it'll select all of them I'll go multiple data preview just to make sure that it looks good I'm getting no data extracted here oh sorry I didn't select the didn't finish it now we're getting product descriptions that's pretty cool this is me doing this sort of one at a time you can also group The selectors there you go it's offered to group it for me so we can group this into one object with products and then product descriptions so it's automatic group it now we have wrapper for products and products descriptions then we have products and product descriptions buried underneath we could go as far as we want with this but what I'm what I'm trying to show you guys is very simple and easy just drag your mouse over the specific thing you want if you select more than one it'll automatically find all of them on the website which is really cool great once we have this what I can do is I can go export sitemap so now I have all of the code on the website that goes and finds it for me then I can paste this in here I'll just call this left click scraper and I'm going to import this to my cloud scraper I think I'm running into oh sorry I don't think we can do a space there my bad just call it left click and now what we can do is we can just run a server instance that goes out and then scrapes this for us so I'm going to click scrape it looks I need to verify my email so just make sure you do that before you try and get ahead of yourself I was looks we just verified the email let's head back over here refresh then scrape we've now scheduled a scraping job for this sitemap scheduling you know in their lingo just means that it's now part of their big long queue of thousands of other things that they're probably scraping through their server and that's fine I just gave this a refresh and as we see we have now finished said scraping job we have all of the data available to us using their UI but now that we've gone through this process of you know building out this this thing how do we take that and then use it in our nadn flows so variety of ways if you wanted to connect this let's say to specific service Dropbox Google you know dump anow or something Google Drive I'd recommend just doing it directly through their integration it's just a lot easier to get the data there and then you can just connect it to n and watch the data as it comes in or something you can also use the web scraper API this is pretty neat because you can you know that's what we're going to end up using it was pretty neat because you can schedule jobs you can send jobs you can do everything just through the NN interface and then we can just retrieve the data afterwards which is pretty neat this is what you end up getting you end up with scraping job ID status sitemap all this fun stuff and then we can set a web hook URL where we we receive the request so let me check we need a scraping for testing you need a scraping job that has already been finished I think our scraping job has already been finished I'm just going to go htps back to my n8n flow I'm going to build an n8n web hook give that a click I'm not going to have any authentication let me just turn all this off what we want is we we want to use this as our test event we're going to go back to the API paste this in save and I'm just going to want to give it a test endpoint here so test looks the push notification was failed the reason why is because it's saying this web Hook is not registered for post request did you mean to make a get request beautiful thank you naden we absolutely did so I'm going to stop listening change your HTTP HTTP method here to post there's two ways to call a website and this is one of them I'm going to listen for test events go back here and then retest awesome looks we've now triggered the beginning of our workflow using this data let's see what sort of information was in it great we have the scraping job ID the status execution mode great so we have everything we need now to set up a flow where we can schedule something in this web scraper service that maybe monitors some I don't know list of e-commerce product or something every 12 hours and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks so we just got the data from web hook let's set up an HTTP request to their API now where we get the ID of the thing and then we can call we can call that back so got my API token over here I'm going head over to their API documentation first and then what we want to do is download these scrape data in CSV format at least in my case I imagine most of you guys are going to add this to a spreadsheet or whatever you can very easily do whatever you want there's also a Json format endpoint here but let's just do CSV for Simplicity so I've already gone ahead and I've gotten the method which was a get request so I've added that up here the URL was this over here with the scraping job ID and then your API token there so what I've done is I've grabbed the API token and the scraping job ID I mean I hardcoded it in here just while I was doing the testing let's make this Dynamic now drag the scraping job ID over here voila and then the API token if you guys remember back here on the API page you have your access to API token so just copy that over great and now if I run this I'm selecting that specific job then from here we have all the data that we just scraped as you can see there's a the way that CSV Works let me just copy this over here I just wanted to give this to you guys as an example of a different data type but maybe some people here aren't really familiar with it the way that it works is if I just paste this into a Google sheet you see how it looks this what what you can do is if you just split the text to columns you kind of see how kind of see how there's these four pettings there's web scraper order web scraper startup products and product descriptions I'm imagine scraping this for some lead genen applica sorry some some e-commerce application list of products here product descriptions maybe product prices maybe product whatever the heck you want so yeah you can you can put in a number of formats and I just wanted to give you guys an example what that looks the next way to scrape websites in naden is using appify if you guys are no strangers to this channel you know that I do appify all the time and I talk about them all the time because I think that they're just a great service they've now given me a 30% discount where anybody can use it for I was initially under the impression it was lifetime I think it's three months so you probably get 30% off your first three months just check the description if you want that but Cent how appify is is it is a Marketplace very similar to Rapid API although extraordinarily Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application so just as we had earlier we have Instagram scrapers we have Tik Tok scrapers we have email scrapers we have map scrapers Google Maps we could do I don't know Twitter scrapers medium scrapers any any service out there that has this Dynamic aspect to it that's not a simple HTTP request you can make you could scrape it using ampify and then obviously you you have things too just basic website crawlers you can generate screenshots of sites I mean there's just there's so many things let me walk you guys through what it looks now in my case I'm not going to sign up to appify because I have 400 accounts but trust me when I say it is a very easy and simple process you go to app ay.com you go get started you put in your email and your password they'll give you $5 in free platform credit you don't need any credit card and you can just get up and running and start using this for yourself super easily then the second that you have all that you'll be Creed with this screen it is a console screen don't be concerned when you see this you know this is super simple and and easy and and not a big deal this is one of my free accounts so I just wanted to show you guys what you can do with a free account but from here what you do is you go to the store and as you can see I'm just dark mode all this is the same thing we were just looking at before and then we're just going to run a test on the thing that we want to scrape so what I'm going to want to do is for the purposes of this I'm now going to do something different from what I was doing before which was just left click over and over and over I think that kind of gets boring what I'm going to do is I'm going to scrape Instagram posts so what I'm going to do is I'm going to feed in a name nickf this is just my Instagram which almost hit 10K in God 15 days or something that but I'm going to feed in my Instagram here and then I'm just going to grab I don't know the last 10 posts save and start this is now going to run an actor actor is just their term for scraper which will go out it'll extract data from my Nick surve Instagram and as you can see will get a ton of fields caption owner full name owner Instagram URL comments count first comment likes count timestamp query tag we get everything from these guys which is really cool this might take you know 30 40 50 seconds we are spinning up a server in real time every time you do this as you see in bottom left hand corner there's a little memory tab which shows that we are legitimately running a server with one gigabyte of memory now so generally my recommendation when you use appify is not to use it for oneoff requests this feed in 5 to 10 15 20 Instagram Pages but you know I just got the back and voila we we have it it's in front of us we have all of the data of that person's Instagram profile so you can see it's quite scalable in that way so the question is obviously how do you get this in NN appify has a really easy to use API which I doing all you have is if we wanted to get the let's see get data set items all I'm going to do is I'm just going to copy this go back here and then connect this to an HTTP request module as you could see we have this big long field here with my API appify API token and this specific data set that I'm looking for I'll show you how to get it dynamically but I just wanted to allow you to see how to get data in naden really quickly now if we go to the schem of view we can see we legitimately we we already have all of the data that we we had from appify a second ago super easy and quick and simple to get up and running we have the input URL field the ID field the type the short code caption now this is Instagram every looks we have some comments I don't have any style how do I create my man you just got to fake it till you make it I don't have any style either just some nerd in my mom's basement yeah so you you can scrape any resource you want here obviously I was scraping an Instagram resource but if you were scraping something else there'd be no change to this at all no change whatsoever now what we need in order to make this Dynamic make us able to run something in appify and then get it in NN so we need to set up an integration so just head over to this tab set up integration and then all you want to do is you just want to do web hook send an HTTP post web Hook when a specific actor event happens the actor event that we're going to want is when the run is succeeded the URL we're going to want to send this to if you think about it we just make another web hook request here web hook the URL we're going to want to send it to is going to be this test URL over here now I'm just going to delete all the header off stuff here because it just complicates it especially for beginners but we're going to copy this over head back over here paste in this URL and then let me see this is a post request I think I don't remember so we're going to have to double check I think it's a post request yeah and then what I'm going to do is I'm going to listen for a test event run the test web hook so we're listening we're making a get request so the fact that it hasn't connected yet probably tells me it's a post request so let's move over here move this down to post now let's listen to a test event let's run this puppy one more time so we just dispatched it and yeah the post request succeeded and what did we get we got tons of information we got a body with a user ID created at event data joke looks when you test something out they just send you a joke about how Chuck nurse can sketi a cow in two minutes have you ever heard of the word sketi before this moment I haven't I want to be known for my ability to sketi we'll go Instagram website scraper and now if we go back here we're now listening for a test event so I'm going to listen for this test event I'm going to run the same scraper again maybe we'll make it five posts per profile just to make it a little faster and once this is done what it's going to do is it's going to send a record of all the information we need to get the data over to Ann we're going to catch that information and then we're going to use it to query the the the database that it created for that particular Instagram run which will then enable us to do whatever the heck we want with it so it's now starting to crawl as we see here we had five requests so it should be able to do this in the next 5 seconds or so and once that's done we now have an actor succeeded event and then we have let me see the data that we want would be the default data set ID down over here so if we just go to that next HTTP request node what I can do is I can feed that in as a variable here let going to a default data set ID drag that in between these two little lines and now we can test that step with actual live data now we have everything that we need so I don't know maybe now you want to feed this into Ai and you want to have ai tell you something about the last five posts tell you wow those last five posts were amazing Nick I loved the specifically the one on Korea and I just wanted to send you over some quick assets to help you out you can now do super Dynamic and structured Outreach you could take that data and use it to draft up your own post I mean the options are ultimately unlimited that's why I love appify so much the sixth way to scrape websites with NN is data for Co this is another thirdparty service but it's a very high quality one that's specifically geared towards search engine optimization requests you guys haven't seen data for SEO before it's this big API stack that allows you to do things automatically query a service maybe some e-commerce website or some content website and then extract things in nicely structured formatting again specifically for SEO purposes tons of apis here as I mean a lot of these services are now going towards more Marketplace style stuff but just to give you guys an example you could Google really quickly to scrape a big list of Google search results for a term and then you could feed that into one of any of the other scrapers that we set up here to get data on stuff you could go Google Images Google Maps you could do Bing BYO YouTube Google's their own data set feature I don't really know what that is but I imagine it's pretty cool and then you can you can take this data and do really fun stuff with it so I'm just going to click try for free over here in the top hand corner show you guys what that looks and as you see here I signed in to data for SEO to my own account looks I have 38 million bajillion dollars but obviously you'd have to go through the rig Rolla creating your own account so why don't just do that with you and then I'll just use that account that is 38 million bajillion dollars we'll click try for free we'll go Nikki Wiki let's use a different email I need a business email huh that's unfortunate I do agree to the terms of use absolutely bicycle is that a bicycle that's not a bicycle what does it mean when I can't answer these does it mean that I'm a robot if you look at some of my posts some of my comments people would absolutely say yes it means that that you're a robot I don't know why people keep saying stuff dude Nick nice AI Avatar bro but I'm it's not an AI Avatar it's not an AI Avatar at all it's just me anyway so I need to activate my account doesn't look it allows you to feed in the code here so I'm just going to feed it in myself it's obviously you're getting a lot of spammers hence this bicycle stuff I don't know why the code isn't working here let me just copy this link address paste it in here instead there you go great so now you can sign in and once you're in you got also they're really big on on bicycles they're training a model to convert all ads on planet Earth into bicycles they'll give you a dollar worth of API access credits which is pretty cool I'm not going to do that I'm just going to go over to mine which is$ 38 million bajillion dollars with 99,999 estimated days to go and yeah let's run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily so maybe I'm in the UK and I want to scrape let me see a keyword ni arrive then I'm going to send a request to this API there's there's a bunch of other terms here that are going to make more sense if you're a SEO person but now we receive as output a structured object with a ton of stuff we have the first result here it's an organic one with some big URL a bunch of chips I'm I have a Knowledge Graph profile which is cool apparently it finds it says I'm a freelance writer you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers that do more I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to create your own very complex flows you can do other stuff as you grab a bunch of keyword data so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language I'm just not going to select an language and then I'll do a request so now it's going to find us a bunch of search volume related stuff so I don't know how many people are searching for me in 2025 apparently 390 is this per month H wonder if it's per month per day that's interesting I don't really know why they break it down by the month date yeah looks it's 390 per month so to the 390 people that are Googling me who are you and what do you want I'm just kidding you can do things you could find back links so you could find links for I believe you feed in a website URL and then it finds back links to that website so this is you technically now scraping a bunch of other websites looking for links to the specific resource that you have that's kind of neat it looks that found it immediately which is really really cool and it looks they're referring top level links that are Dooms BG bgs would be interesting I wonder where that's coming from there's a Content generation API playground so you could you know feed in some text and then have it generate other stuff but I think we're kind of getting away from the actual thing that matters which is the scraping of the scraping of the websites so yeah lots of stuff lots of stuff for sure now that's all good but let's turn this into an API call if we head over to the API of do data for SEO so in my case docs.",
    "datafor seo.com V3 _ page SL contentor parsing live that's what I'm I'm curious about you'll see that we have a post request that we need to send to this URL I have a curl just this which I can feed into an API request that's what I'm going to do so I'm going to go back over here and I'm just going to import this curl import and it's going to go through and it's going to parse out all these fields that I'm interested in with the URL which I'll go htps left click.",
    "AI and then we have sort of a gacha here that a lot of people don't understand this is the authorization the authorization is a little bit different from most of the easy authorizations we've had so far we have to convert it we have to go one one more step to make this work if I check out the let's see authorization here what we need is we need to get the login and then the P so this is your username and then your password then we have to Hash it or not hash it but we have to convert it into something called base 64 this is just how they do their API key stuff I guess it's kind of annoying but it's just part and parcel of working with some apis you're just not always going to have it available to you really easily so I'm just going to go back to data for SEO and then I'm going to grab my credentials so what we need to do is we need to base 64 encode the username and the password I'm just going to leave that at what I've done is I've gone through and done it in this edit Fields node what you need to do is you need to have your username or your login so maybe this is me searching Nix or have Reddit Nick left click.",
    "so that might be my username and then my password is What's called the API password you can find that really quickly and easily just by going over here to API access and then API password if you just signed up it'll be visible here if it's been more than 24 hours you have to send it by email but anyway so that's that's where i' get the API password from and then once you feed it in over here where you're going to want to do is you're going to want to base 64 encode it this they just require you to use these creds or to operate with these creds as base 64 encoded versions Bas 64 is just a way to translate into a slightly different number format so once you have that you would just feed in the variable over here Ju Just as follows and then you can make a request to their API and receive data so it looks I was doing their content parsing live you know what I wanted to do is I just wanted to call their endpoint which I think was their instant Pages this one over here so it's just V3 once you've sorted this out by the way the AP gets extraordinarily easy to manage you just need to figure out the authentication from there on out all you're doing is just swapping out the requests so you know if you wanted to do instant Pages all I'm doing is pumping that in there I just sent a request and now I receive a bunch of links with different headings and and so on and so forth that's easy the seventh way to scrape websites and Ed end is using a third party application called crawl Bas they're known for their rotating proxies which allow you to send very high volume API requests so it's very proxy driven this is their website so it's a scraping platform similar to Rapid API and you know appify they support many of the major websites here and the reason why they're so good at this is just because they you know as I mentioned they rotate the hell out of these proxies so we're just going to sign up to Tri it free I'll use my business email here and then continue with Emil email we got to add a phone number obviously we're going to do less than a thousand I'm a CTO I don't want to what's the animal is that an animal yes it's an animal good God beep boop we're going to head over to my Gmail and receive this now so we need to confirm my account just going to copy this link address that I can do this in one page awesome we should be good to log in so that's what's happening we need to select the animal again just doesn't it doesn't believe really just doesn't believe great so now we have a crawling API smart proxy thing if you guys want to run I don't know use in apps that have a proxy field specifically I'm just going to keep things simple we're doing this in n8n so we're going to go crawl base API we have a th000 free crawls remaining very first thing we're going to want to do is just click start crawling now just to get up and running with the API and as you see here the these guys have probably one of the simplest apis possible all API URLs start with the folling base part click and then all you need to do in order to make an API call is run the following sort of line so this is a curl request obviously we're in n8n and one of the value valuable parts of NN is we can just import a COR request so I'm going to import it as you can see here we have a token field then we just have the URL field of the place we want to crawl so I'm going to do left click.",
    "for now I don't know if this token field was my real token I don't believe so maybe we'll give it a try maybe it's a test token or something so I'm now running this and it looks we just received a bunch of very spooky data I don't the spooky data no spooky data for us sometimes spooky data this H this seems kind of weird to me just give me one second to make sure that's we are receiving a data parameter back which is nice but yeah something about this is a little bit spooky was it a get request or was it a post request no I guess it's a get request strange very very strange anyway they give you two types of tokens here this is why I'm talking about it to begin with I'm also because I just used it before for a couple of applications and I found it very easy they give you a normal token and they give you a JavaScript token as so the reason why that's valuable is because if you're scraping one of these websites I talked about before where when you send a simple HTTP request nothing pops up this is the this is the purpose of this you feed in a JavaScript token when you use the JavaScript token it'll automatically launch a browser instance inside of craw base for you so instead of you getting just that empty thing back that I mentioned I'm you're going to get you're going to get a JavaScript version of the website where somebody went on the website it loaded really briefly and then they grabbed the code afterwards so yeah we have some some API call stuff over here this one's just using Amazon this is pretty interesting so I might give that a a go just to give you guys an example of said Amazon scrape let's just go www.amazon.com oh Amazon might be JavaScript so maybe we give that a go no it looks we we got the data from from Amazon which is pretty cool if you feed that into the markdown converter we had before it's going to feed in the HTML here pump it into a data key we've now converted this into this is very long let's go tabular we've now converted this into markdown which is cool and this is pretty long obviously has all of the images and has all of the information on the site which is cool and then we can feed it into open AI I did before where I message a model and I'm just going to copy from my previous application here to make my life a little bit easier where the heck are you and then we're just going to feed in the code here and then because I didn't feed in this we should now run this we're going to grab data from the site and we're going to try and I mean you know we kind of all know what Amazon is and what it does so I'm not expecting expecting anything spectacular but it's still going to go it's going to give me all of the text on this Amazon page and then I'm going to get a bunch of list of links absolute URLs ideally should play some Jeopardy music or going be able to play Star Wars music that'd be kind of cool we now have a schema with all of the links on the page which is pretty cool we have the plain text website copy we have a on line summary you know plain text website copy is a lot longer than this obviously it's just shortening and truncating it for us but yeah very quick and easy way to use crawl base for this now the value in crawl base is not necessarily just to send them to static websites I talked about it's to use highly scalable scraping where you're scraping any applications consistently as you see here the average API response time is between 4 to 10 seconds so you you will receive results back pretty quick if you wanted to just send one request or 20 requests every second think about it 20 requests a second times 60 seconds a minute is 1,200 requests times 60 minutes and an hour 72,000 requests sorry just jumping around the place here you can send 72,000 requests an hour which is crazy and you can do so as quickly and as easily as just adding an API call and then it'll automatically distinguish between a a plain text thing or a JavaScript thing the eighth way to scrape data in nadn specifically website resources is octop parse octoparse is very similar to some of the other services that we've talked about it is a web scraping tool that gives you quote unquote free web crawlers and I'm just a fan of their ux I think it's very clean I think the way that they have their signup flow and stuff's really easy so if you made it to this part of the tutorial and you have yet to sign up to one of these Services give octoparse give octoparse your thoughts let's double check that I haven't created an account using this no I haven't fantastic so I should be able to jump through and show you guys what this looks we have a verification code I'm going to paste in if you're not familiar with jumping around and stuff this or if you're wondering how I'm jumping around I'm just using a bunch of website hotkeys great account is now ready so we can start a free premium trial if you want I think you're going to have to add a card I don't know if I have enough credits to do anything but if I'm not then I'll start that trial in a second what you're going to have to do in order to make this work is you're going to want to have to download you're going to want to download the octoparse desktop app so let's give it a quick and easy go just going to drag this puppy if you are using something that is not Mac OS you will not have this strange drag and drop feature here once that is done you will have octo parse accessible just open that up yes I want to open this thank you and the cool thing about octoparse kind of relative to what else you know the other scraping applications I talked about is this is just running in a desktop app kind of in in your computer so it's cool because it's just easy to get up and running with and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and what'll happen is it'll launch an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm scraping all of this stuff now we have access to this sort of drag and drop or selector thing similar to what we had before if you click on one of these you'll see it allow you to select all similar Elements which is pretty sweet and then you can also do things click elements and so on and so forth extract the text Data here you can also tie that to other things so as you see I'm now mapping each of these very similarly to how I was doing before between the first field which is the title of the product and then the second field which is the field to so that's pretty sweet we could do the same thing with a number of things you could extract the headings and then the values and so on and so on and so forth but I'll kind of leave it there so once you're done selecting all the elements that you want all you do is you click run and you have a choice between running it on your device versus running it on the cloud so on the cloud is API supported that's how you're going to get stuff in NM but I just want you guys to know that you can also just run it here you could run it here load up the URL scrape all the things that you want on the specific page you're feeding in and then you can be done with it so I just selected run in the cloud it's now going to open up said Cloud instances as we could see we have this little field where it's running and extracting the data we're now done so I can export this data locally but I could also do a lot of other stuff which we'll show you in a second so you can dump this automatically to Google Sheets you could do zapier to connect to Google Sheets do some sort of web Hook connection export to cloud storage similar stuff to the the web scraping Chrome extension but for now let's just export this as Json give ourselves a little ad Json file here thank you and yeah now we have it locally now in order to connect with the octop par CPI what you're going to have to do is first you get up to request an access token the way that you do this is you send a post request to this URL here and the way that you format it is you need to send your username your password and then have the grantor type as password now password obviously just put in whatever your password is don't store it in PL text I'm doing with my hypothetical password put it somewhere else and then grab that data and then use it but the the output of this is we have this big long access token variable which is great after that if I just go back to their API here once we're here we can extract the data that we need so the thing that you're going to want is you're going to want get data by Offset you can also use get non-exported data which is interesting so I think this just dumps all of the data as not exported and then sends that over to you I believe but anyway you could also get the data by offset so if I go a get request to open api.",
    "octop course.com SL all and then I just send a header with the URL parameter this is a get request we're going to send a header with the token so authorization Bearer and then feed in the access token here just make sure that this is just one space no it's two if I feed this in it's saying that it's a bad request let me just triple check why I think we need three Fields yeah I think we need three Fields my bad we need this get request then we need the authorization header I talked about then we need three Fields task ID yeah obviously we need to feed in the task ID so you need task ID offset or size so we'll feed this in as query parameters here so send query parameters the first value was task ID second one was offset and offset is no Capital the third was size offset's going to be zero size going to be I don't know let's just do 1,000 and what we need now is we need the task ID of the specific run that we just finished oh in order to get the task list you head over to task list top hand corner here task ID API so we now have access to this so if we go back to our NN instance we could feed that in here by test the step you'll see that we now have all the data that we just asked for earlier so a variety of ways to do this in practice octop par allows you to schedule runs you could schedule them using their you know whatever it is cloud service you could use it to scrape I don't know Twitter they have a variety of other scrapers that you can check out just heading over to this new here if we just go sorry go down to templates there's a variety of other ways to scrape Google job scraper glass door scraper super Pages scraper you could schedule these and then what you can do in na is you can just query it once a day grab all the data I showed you how to do a moment ago dump that into some sheet octoparse is pretty cool it's more of an industrial Enterprise level application to be honest so there might be some gotas if you're not super familiar with working with desktop apps and stuff but I I the idea that you can also just scrape locally which is pretty sweet and the last of our nine best ways to scrape websites in nadn is browserless now browserless runs a headless Chrome instance in the cloud this stuff is great for dynamic or heavy JavaScript websites if you've never used browser list before the cool part about browser list is allows you to bypass captas which is a big issue that a lot of people have so I'm going to click try it for free I'm going to enter my email address over here verify I need to submit a code so let's head back over here thank you thank you thank you thank you we have a ton of free trial signups obviously I don't have a promo code or anything don't have a company name I'm just going to enter a password I'm using this to get past to avoid setting up a puppeteer and playright server sure I'm going to click complete we're now going to have a th credits inside of browser list which is pretty sweet and we'll get a we'll get a full plan eventually we now have an API token so I can figure out how all of the stuff works here I'm just going to dive into the API I can figure out how all of the API stuff works using their API docs which are fantastic by the way and we don't want to do any of this stuff we just want to do HTTP apis brow list API index great so here's where we're at if you want to send and receive a request what you need to do is you send a request to one of these endpoints content unblock download function PDF screenshot scrape or performance what we want for the purpose of this is just let's do content this is the request over here so I'm just going to paste my API token up here copy this request feed it into nadn in the HTTP request module as per usual nice quick and easy I'm going to grab my API token and where it says your API token here I'm going to feed that in what I want as a website is just left click.",
    "a I'm going to run test step we are now quering the pi and in seconds we have access to the data same thing that we had before but now we're using a pass through and browser list is a great pass through because you know they they allow you to scrape things that go far beyond the usual static site thing so honestly and I'm just leaving this as a secret and sort of a little I guess Easter egg for people that have made it this far in the video my go-to when scraping websites is as I mentioned do that HTTP request trans forg that works then do something fir C.D but if that doesn't work I I do something browserless that has all of this stuff built in and I especially use browser list anytime that there's some sort of you know application where I'm just going to save this so I can make all my HTP requests really easy especially when you know there's issues with captas and and accessing resources and stuff check this out not only can you do the actual scrape you can do a screenshot of the page as and because I've entered my token up here the requests that I'm going to setting up are as simple as importing the curl then clicking test step so straightforward we now have a file which is the screenshot now I used example domain there let's go left click.",
    "run this test now you can see we've received a screenshot of the of the website view very sexy and my website's pretty long so keep in mind and yeah you know obviously a lot you could do with that you can download the site you can turn the site into a PDF so that's pretty neat I don't think I've used this one before but for the purposes of this demonstration why don't we give it a try we'll go over here import the curl paste it in voila the website I'm going to do is left click.",
    "aai going to test this step so now there servers doing a couple things I'm scraping the site then converting it all into PDF format probably screenshotting a bunch of stuff too if I view this now we now have my my file looks it didn't capture all of the color aspects that might just be difficult or whatever but I still have a PDF of the site which is pretty neat and yeah let you guys kind of screw around with this on your own but there are a variety of cool applications you can use browless for all I hope you guys appreciated the nine best ways to scrape websites in nadn as you guys could see it's a combination of on platform scraping using the HTTP request module a lot of API documentation stuff that if you want to get good at this I'm releasing a master class on API stuff as part of my next na tutorial video and then you know navigating this and then and then taking the data from these services and using them to do something that you want to do artificial intelligence to give you a summary of the site or generate ice breakers for you or do something else whether you're using a local application octop parse or maybe the web scraping CH Chrome extension or using something firra browserless appify rapid API and so on and so forth you now have everything that you need in order to scrape static sites Dynamic sites super Js heavy websites and even social media websites Tik Tok Twitter and Instagram thanks so much for making it to this point in the video if you have any suggestions for future content drop them down below more than happy to take your idea and run with it assuming it's something that I haven't done before and then if you guys could do me a really big solid subscribe do all that fun YouTube stuff and I'll catch you on the next video thank you very much"
  ],
  "grouped_content": [
    {
      "main_point": "hey Nick here today I'm going to show you the nine best ways to scrape any website in nadn you're going to be able to scrape static sites Dynamic sites JavaScript social media whatever the heck you want by the end of the video you'll know how to do it I scaled my automation agency to 72k a month using no code tools make and nadn and scraping was a big part of that so this video is just going to give you all the sauce you're going to learn everything you need to scrape websites that on your own let's get into it all I'm going to jump into NN in a minute and build these alongside you and one other thing I'm going to do is I'm going to sign up to all the services in front of you walk you through the Authentication and the onboarding flows and get your API keys and stuff that but just before I do want to explain very quickly the difference between a static site and then a dynamic site because if you don't know this scraping just gets a lot harder and so we're just going to cover this in 30 seconds and we can move on so if this is you you're just this wonderful smiley person and you want to access a static website what you're doing is you're sending a request over to just some document you know think about this as just a piece of paper on a cupboard and there's a bunch of text on this piece of paper and what you do is you say hey can I have this piece of paper and then the piece of paper just comes back to you with all of the information inside of the piece of paper this is a very simplified version of what's going on but static sites are by far the easiest thing to scrape and so this is where you know a lot of people think all websites are are at and then they kind of confuse it with this next step which is dynamic a dynamic site essentially is not that at all what you're doing is you're sending a request to a piece of paper but the piece of paper has nothing on it what happens is this piece of paper then sends a request to some other dude which I guess in this case is just a server really who will then he has a trusty pen in his hand and he'll write all of the stuff on said piece of paper and then you'll get the piece of paper back so there's that intermediate step where you are pinging some sort of you know domain name or whatever then that domain name shoots some code over forces a server to generate all of the contents on that domain and then you get it this is obviously kind of a two-step process and then this is a three-step process so if you just understand that you know when you scrape a dynamic resource what you're really doing is you're sending a request to a page which sends a request back to another server which then fills your thing this element eliminates 99% of the confusion because most of the time scraping issues are hey I just ping this page but I got nothing back you know the HTTP request or or or whatever I I sent you know it was fine but for some weird reason there's nothing on this page of of any substance if that was the case for you it was most likely because this was empty before you sent it over whereas you know simple scraped static resources tend to just give you what you want really quickly and it's really easy all so hopefully we at least understand that there's that difference between static and dynamic sites here I'm not going to go into it more than that we're just going to dive in with both feet start doing a little bit of scraping and then we'll kind of see where we land I find the best way to do this stuff is just by example and and you know being practical about it so the first major way to scrape websites in NN is using direct h HTTP requests this is also what I to think of as the Magic in scraping itself what we're going to do is we're going to use a node called the HTTP request node to send a get request to the website we want this is going to work with static websites and non JavaScript resources so let me give you guys a website that I'm going to be scraping here this is my own site it's called left click I'm about to do a redesign but this is a static resource I know this because I built the site you know I I did it in code and all this is is just a document somewhere on my or on on a server some more so what I'm going to want to do is and I'm just going to pretend that I haven't done any of this so we're just going to go HTTP request HTTP request node looks this we have a method field a URL field authentication field query parameters headers body and then some options down here as all I'm going to do is I'm just going to paste in the website that I want to visit then I'm just going to test the step it's that easy now the response from this on the hand side see all this code over here this is what's called HTML if you're unfamiliar and HTML is just the it's it's the code behind the site so if I were to zoom in over here you see where it says I don't know let's let's go to my website let's just find a little bit of little bit of texture build hands off growth systems if I just command F and paste this in we have that text buried somewhere in this big long HTML string so all that this HTML is is this is the code that is sent to my browser which is Google Chrome in this case then my browser takes the code and it just renders it into this beautiful looking thing beautiful is a subjective State I would say but this wonderful looking thing in front of us which is this website with sizing and the tabs and the divs and all that fun stuff so what I'm trying to say is everything over here on the hand side this is the entire site we can do anything we want with this information and we can carry this information forward to to do any one of our any one of many flows so in my case looking at a bunch of code isn't really very pretty so one big thing that you'll find in the vast majority of modern scraping applications is you'll find that they'll take that HTML which we saw earlier and they'll convert it to something called markdown so this is a markdown node we have a mode of HTML to markdown and all I'm going to do is I'm going to grab that data and I'm going stick it in the HTML section of the HTML to markdown converter what do you think is going to happen when I test this we're going to convert this from this big long ugly super dense thing with a much these greater than and less than symbols and we're just going to convert it into something a little bit shorter a little bit simpler This Is Us just manipulating file formats by the way and I find that manipulating file formats is a big part of what makes a good scraper a good scraper so now we have something in what's called markdown format what's the value there markdown format does two things for us one it's much easier to parse parse just means we can extract different sections of the text we want we can structure it in some sort of other data format and then in my case because I love using AI for everything it's much easier and shorter for us to use with AI so I'm going to give you guys a very simple example where we take this text from the static resource and then we just use AI to tell us something about it so I'll go down to open Ai and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection and grab your API key and paste it in there's some instructions that allow you to do so over here what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the I want to say most cost effective one as of the time of this recording and then what I'm going to do is I'm going to add three prompt I'm going to add a system prompt first I'll say you are a helpful intelligent web scraping assistant then I'm going to add a user prompt and I'll say your task is to take the raw markdown of a website and convert it into structured data use the following format and then I'm going to give it an example of what I want in what's called Json JavaScript object notation format so the very first thing I'm going to do is I'm going to have it just pull out all the links on the website because I find that that's a very common scraping application so I go links and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't do anything for us what we what we want is we want this instead we want left click aka the root of the domain and then left click _ logogram and that's how we get to the actual file asset so if we go back over here so if we go back over here yeah you know we want a link array of absolute URLs and then I'm just going to want main text website copy this is going to be a long string containing all of the website copy containing just the text of the site no formatting and then why don't we do one more thing why don't we have a summarized or summary let's do one line summary just to show you guys you can also use AI to do other cool stuff you could take this oneline summary and feed it into some big sequence you could have ai write an icebreaker for an email you could do a things with this but I'll say on line summary brief summarization of what the site is and how what the site is about let's do that so this is our example I'm going to say your website URL is left click URL for the relative to Absolute conversions is left click.",
      "similar_statements": [
        "hey Nick here today I'm going to show you the nine best ways to scrape any website in nadn you're going to be able to scrape static sites Dynamic sites JavaScript social media whatever the heck you want by the end of the video you'll know how to do it I scaled my automation agency to 72k a month using no code tools make and nadn and scraping was a big part of that so this video is just going to give you all the sauce you're going to learn everything you need to scrape websites that on your own let's get into it all I'm going to jump into NN in a minute and build these alongside you and one other thing I'm going to do is I'm going to sign up to all the services in front of you walk you through the Authentication and the onboarding flows and get your API keys and stuff that but just before I do want to explain very quickly the difference between a static site and then a dynamic site because if you don't know this scraping just gets a lot harder and so we're just going to cover this in 30 seconds and we can move on so if this is you you're just this wonderful smiley person and you want to access a static website what you're doing is you're sending a request over to just some document you know think about this as just a piece of paper on a cupboard and there's a bunch of text on this piece of paper and what you do is you say hey can I have this piece of paper and then the piece of paper just comes back to you with all of the information inside of the piece of paper this is a very simplified version of what's going on but static sites are by far the easiest thing to scrape and so this is where you know a lot of people think all websites are are at and then they kind of confuse it with this next step which is dynamic a dynamic site essentially is not that at all what you're doing is you're sending a request to a piece of paper but the piece of paper has nothing on it what happens is this piece of paper then sends a request to some other dude which I guess in this case is just a server really who will then he has a trusty pen in his hand and he'll write all of the stuff on said piece of paper and then you'll get the piece of paper back so there's that intermediate step where you are pinging some sort of you know domain name or whatever then that domain name shoots some code over forces a server to generate all of the contents on that domain and then you get it this is obviously kind of a two-step process and then this is a three-step process so if you just understand that you know when you scrape a dynamic resource what you're really doing is you're sending a request to a page which sends a request back to another server which then fills your thing this element eliminates 99% of the confusion because most of the time scraping issues are hey I just ping this page but I got nothing back you know the HTTP request or or or whatever I I sent you know it was fine but for some weird reason there's nothing on this page of of any substance if that was the case for you it was most likely because this was empty before you sent it over whereas you know simple scraped static resources tend to just give you what you want really quickly and it's really easy all so hopefully we at least understand that there's that difference between static and dynamic sites here I'm not going to go into it more than that we're just going to dive in with both feet start doing a little bit of scraping and then we'll kind of see where we land I find the best way to do this stuff is just by example and and you know being practical about it so the first major way to scrape websites in NN is using direct h HTTP requests this is also what I to think of as the Magic in scraping itself what we're going to do is we're going to use a node called the HTTP request node to send a get request to the website we want this is going to work with static websites and non JavaScript resources so let me give you guys a website that I'm going to be scraping here this is my own site it's called left click I'm about to do a redesign but this is a static resource I know this because I built the site you know I I did it in code and all this is is just a document somewhere on my or on on a server some more so what I'm going to want to do is and I'm just going to pretend that I haven't done any of this so we're just going to go HTTP request HTTP request node looks this we have a method field a URL field authentication field query parameters headers body and then some options down here as all I'm going to do is I'm just going to paste in the website that I want to visit then I'm just going to test the step it's that easy now the response from this on the hand side see all this code over here this is what's called HTML if you're unfamiliar and HTML is just the it's it's the code behind the site so if I were to zoom in over here you see where it says I don't know let's let's go to my website let's just find a little bit of little bit of texture build hands off growth systems if I just command F and paste this in we have that text buried somewhere in this big long HTML string so all that this HTML is is this is the code that is sent to my browser which is Google Chrome in this case then my browser takes the code and it just renders it into this beautiful looking thing beautiful is a subjective State I would say but this wonderful looking thing in front of us which is this website with sizing and the tabs and the divs and all that fun stuff so what I'm trying to say is everything over here on the hand side this is the entire site we can do anything we want with this information and we can carry this information forward to to do any one of our any one of many flows so in my case looking at a bunch of code isn't really very pretty so one big thing that you'll find in the vast majority of modern scraping applications is you'll find that they'll take that HTML which we saw earlier and they'll convert it to something called markdown so this is a markdown node we have a mode of HTML to markdown and all I'm going to do is I'm going to grab that data and I'm going stick it in the HTML section of the HTML to markdown converter what do you think is going to happen when I test this we're going to convert this from this big long ugly super dense thing with a much these greater than and less than symbols and we're just going to convert it into something a little bit shorter a little bit simpler This Is Us just manipulating file formats by the way and I find that manipulating file formats is a big part of what makes a good scraper a good scraper so now we have something in what's called markdown format what's the value there markdown format does two things for us one it's much easier to parse parse just means we can extract different sections of the text we want we can structure it in some sort of other data format and then in my case because I love using AI for everything it's much easier and shorter for us to use with AI so I'm going to give you guys a very simple example where we take this text from the static resource and then we just use AI to tell us something about it so I'll go down to open Ai and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection and grab your API key and paste it in there's some instructions that allow you to do so over here what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the I want to say most cost effective one as of the time of this recording and then what I'm going to do is I'm going to add three prompt I'm going to add a system prompt first I'll say you are a helpful intelligent web scraping assistant then I'm going to add a user prompt and I'll say your task is to take the raw markdown of a website and convert it into structured data use the following format and then I'm going to give it an example of what I want in what's called Json JavaScript object notation format so the very first thing I'm going to do is I'm going to have it just pull out all the links on the website because I find that that's a very common scraping application so I go links and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't do anything for us what we what we want is we want this instead we want left click aka the root of the domain and then left click _ logogram and that's how we get to the actual file asset so if we go back over here so if we go back over here yeah you know we want a link array of absolute URLs and then I'm just going to want main text website copy this is going to be a long string containing all of the website copy containing just the text of the site no formatting and then why don't we do one more thing why don't we have a summarized or summary let's do one line summary just to show you guys you can also use AI to do other cool stuff you could take this oneline summary and feed it into some big sequence you could have ai write an icebreaker for an email you could do a things with this but I'll say on line summary brief summarization of what the site is and how what the site is about let's do that so this is our example I'm going to say your website URL is left click URL for the relative to Absolute conversions is left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "and then the final thing is I'm going to add one more user prompt I'm just going to draw drag all of that markdown data in here then I'm going to click output content as Json I'm going to test the step I'm going to take a sip of my coffee while this puppy processes and we now have our output on the hand side if we go to schema view what you can see is we've now generated an array of links on the rightand side which contains every link on this website very cool looks the vast majority of these are type form links for some reason don't really know what's about that oh it's because that's the only thing on my website it's just a one P with a bunch of different links to time for that's funny anyway you could obviously just get it to Output one link or tell it make sure all the links are unique or something and then we have a big chunk of plain text website copy then we have a oneline summary of the site so this is a very simple example of scraping we're scraping a static resource obviously but when I build scrapers for clients or for my own business this is always my first pass I will always just make a basic HTTP request to the resource that I'm looking at because if I can make that http request work whether it's a get request or whatever the the the rest of my life building scraper building the scraper is so easy I just take the data I process it usually using AI or some very cheap Tok cheap per token thing and then voila you know we've built out a scraper in this case and it's only taken us what three nodes so that's number one the second way to scrape websites in NN is using a third party service called fir crawl and making an HTTP request to it I'm using something called their extract endpoint but just to make a long story short fire craw is a very simple but High bandwidth service that turns websites into large language model ready data and you know how earlier we had to do HTTP request and then we had to convert all that stuff into markdown and then we had to you know manipulate that markdown what this does is it just does a lot of that stuff for you it'll allow you to go scrape and then it will automatically convert text into markdown for you so that you can do whatever the heck you want they turn it into structure data using Ai and and so on and so on and so forth so if I were to do the same thing that I just did earlier with my own website then I were to you know run an example of this what it would go do is it would spin up a server for me and that would go and generate markdown of the same format the only difference here is it's generated new lines between sections of text how beautiful and then now you know we have the same thing you also get it in Json which is pretty cool and you know you can slot this into any workflow this is the simple and easy way of getting started what we're going to be showing you today is the extract endpoint which allows you to extract data just using a natural language prompt which is pretty cool and from here we're going to be able to take any URL and just turn it into structure data but we're not going to have to know how to parse we're not going to have to know any code we're not going to have to know any of that stuff so let me run through the signup process with you guys go to fire.",
      "similar_statements": [
        "and then the final thing is I'm going to add one more user prompt I'm just going to draw drag all of that markdown data in here then I'm going to click output content as Json I'm going to test the step I'm going to take a sip of my coffee while this puppy processes and we now have our output on the hand side if we go to schema view what you can see is we've now generated an array of links on the rightand side which contains every link on this website very cool looks the vast majority of these are type form links for some reason don't really know what's about that oh it's because that's the only thing on my website it's just a one P with a bunch of different links to time for that's funny anyway you could obviously just get it to Output one link or tell it make sure all the links are unique or something and then we have a big chunk of plain text website copy then we have a oneline summary of the site so this is a very simple example of scraping we're scraping a static resource obviously but when I build scrapers for clients or for my own business this is always my first pass I will always just make a basic HTTP request to the resource that I'm looking at because if I can make that http request work whether it's a get request or whatever the the the rest of my life building scraper building the scraper is so easy I just take the data I process it usually using AI or some very cheap Tok cheap per token thing and then voila you know we've built out a scraper in this case and it's only taken us what three nodes so that's number one the second way to scrape websites in NN is using a third party service called fir crawl and making an HTTP request to it I'm using something called their extract endpoint but just to make a long story short fire craw is a very simple but High bandwidth service that turns websites into large language model ready data and you know how earlier we had to do HTTP request and then we had to convert all that stuff into markdown and then we had to you know manipulate that markdown what this does is it just does a lot of that stuff for you it'll allow you to go scrape and then it will automatically convert text into markdown for you so that you can do whatever the heck you want they turn it into structure data using Ai and and so on and so on and so forth so if I were to do the same thing that I just did earlier with my own website then I were to you know run an example of this what it would go do is it would spin up a server for me and that would go and generate markdown of the same format the only difference here is it's generated new lines between sections of text how beautiful and then now you know we have the same thing you also get it in Json which is pretty cool and you know you can slot this into any workflow this is the simple and easy way of getting started what we're going to be showing you today is the extract endpoint which allows you to extract data just using a natural language prompt which is pretty cool and from here we're going to be able to take any URL and just turn it into structure data but we're not going to have to know how to parse we're not going to have to know any code we're not going to have to know any of that stuff so let me run through the signup process with you guys go to fire."
      ],
      "frequency": 1
    },
    {
      "main_point": "Dev here just going to open this up in an incognito to show you guys what this looks all you do is you just go sign up I'm going to add a password we're then going to have to validate this one oh guess these guys are Mega secure so now I'm going to go back to my email address I'm going to count this up and we have a call back here I just need to paste this URL and put it in here that's just because I you know I'm doing this in an incognito tab normally when you do this you're not going to have that step great now we're inside a fir craw they give you I think something 500 free credits something of that nature anyway so what I'm going to do is I'm going to go through and just give give this extracting point just a basic natural language query so let's go from the homepage at left click.",
      "similar_statements": [
        "Dev here just going to open this up in an incognito to show you guys what this looks all you do is you just go sign up I'm going to add a password we're then going to have to validate this one oh guess these guys are Mega secure so now I'm going to go back to my email address I'm going to count this up and we have a call back here I just need to paste this URL and put it in here that's just because I you know I'm doing this in an incognito tab normally when you do this you're not going to have that step great now we're inside a fir craw they give you I think something 500 free credits something of that nature anyway so what I'm going to do is I'm going to go through and just give give this extracting point just a basic natural language query so let's go from the homepage at left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "I want to extract a oneline summary of the website let's do all of the text on the website all of the copy on the website in plain text let's do a oneline summary of the website a oneline Icebreaker I can use as the first line of of an of a cold email to the owner and the company name and a list of the services they provide let's do that this is a lot of requests we're asking it to do seven or eight things but all I need to do in order to make this work is I click generate parameters it's going to now generate me a big object with a bunch of things so copy summary Icebreaker company name and now I can go and I can run this this is the URL it just parsed as let's give it a run what it's doing now is it's scraping the pages using their high throughput server I just love this thing I'm not sponsored by fire crawl or anything that but I love their I don't know I just love the design I love this little burning Ember or whatever the heck you want to call it I love how simple they've tried to make everything it's it's great honestly awesome and now you guys see we have a big array with a bunch of sub objects we have a summary I asked for a list of services looks we even have links to the specific places oh links from the resource we have an icebreaker and then we have the company name as so we can do a lot with this but now this is just this is just on on a website how do we bring this in naden it's pretty simple as you see there's an integrate Now button you can either get code or you can use it in zap here what we're going to want to do is we're going to want to run a request to their endpoint and then we're going to want to turn that into our HTTP request let me show you what that looks I'm just going to do all of this stuff in curl so if we go to curl as you can see what we need to do is we need to format a request that looks something this but we need to make sure it's using the extract endpoint so I'm going to go down to extract and then now I have this big long beautiful string what I'm going to do is I'm going to copy this I'm going to go back to my NN instance which is over here and then what I need to do is just open up an HTTP request module and then click import curl just paste all the stuff inside now this is an example request but that's we can use that example request to very quickly and easily format our our real request so we're sending a bunch of headers this is the endpoint that we're calling api.",
      "similar_statements": [
        "I want to extract a oneline summary of the website let's do all of the text on the website all of the copy on the website in plain text let's do a oneline summary of the website a oneline Icebreaker I can use as the first line of of an of a cold email to the owner and the company name and a list of the services they provide let's do that this is a lot of requests we're asking it to do seven or eight things but all I need to do in order to make this work is I click generate parameters it's going to now generate me a big object with a bunch of things so copy summary Icebreaker company name and now I can go and I can run this this is the URL it just parsed as let's give it a run what it's doing now is it's scraping the pages using their high throughput server I just love this thing I'm not sponsored by fire crawl or anything that but I love their I don't know I just love the design I love this little burning Ember or whatever the heck you want to call it I love how simple they've tried to make everything it's it's great honestly awesome and now you guys see we have a big array with a bunch of sub objects we have a summary I asked for a list of services looks we even have links to the specific places oh links from the resource we have an icebreaker and then we have the company name as so we can do a lot with this but now this is just this is just on on a website how do we bring this in naden it's pretty simple as you see there's an integrate Now button you can either get code or you can use it in zap here what we're going to want to do is we're going to want to run a request to their endpoint and then we're going to want to turn that into our HTTP request let me show you what that looks I'm just going to do all of this stuff in curl so if we go to curl as you can see what we need to do is we need to format a request that looks something this but we need to make sure it's using the extract endpoint so I'm going to go down to extract and then now I have this big long beautiful string what I'm going to do is I'm going to copy this I'm going to go back to my NN instance which is over here and then what I need to do is just open up an HTTP request module and then click import curl just paste all the stuff inside now this is an example request but that's we can use that example request to very quickly and easily format our our real request so we're sending a bunch of headers this is the endpoint that we're calling api."
      ],
      "frequency": 1
    },
    {
      "main_point": "dv1 extract so what we're doing now is we're we're sending a request to fir craw which will then send a request to the website so kind of a kind of a middleman and then all I'm going to do so if I go back to my example we have an API key here which we're going to need so I'm going to go here and then paste in an API key so that's how that work works authorization is going to be the name value is going to be bear with a capital b space and then the API key and then we also have a body that we need to adjust or edit and this body is where we're going to put the links that we want to have scraped with the extract end point so what I'm going to do is I'm going to delete most of these I'll go back to my left click.",
      "similar_statements": [
        "dv1 extract so what we're doing now is we're we're sending a request to fir craw which will then send a request to the website so kind of a kind of a middleman and then all I'm going to do so if I go back to my example we have an API key here which we're going to need so I'm going to go here and then paste in an API key so that's how that work works authorization is going to be the name value is going to be bear with a capital b space and then the API key and then we also have a body that we need to adjust or edit and this body is where we're going to put the links that we want to have scraped with the extract end point so what I'm going to do is I'm going to delete most of these I'll go back to my left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "a just this the prompts because you know I was just using their playground before we're going to need to convert this into a request for my service so I'm just going to paste The Prompt in here voila and now we need to put together what's called a schema where we have the objects that we asked for so in my case we asked for copy so I'm going to go Copy Type string then summary so we're going to go summary type string then Icebreaker it's going to be Icebreaker type string then guess what we have last but not least company name which is going to be type string we're also going to want to make these fields required you know you can set it up so they're not required when you do a request a fire call I'm I'm going to make it so they're required so I'll go copy summary Icebreaker and then company name you know what maybe I'll leave company name as unrequired if you think about it logically maybe not all the websites we're going to be scraping using this service are going to have the company names visible on the website I don't know but maybe so maybe I'll leave that as off great so now we have the API request formatted correctly all we need to do at this point is just click test step it looks we're getting a Json breaking error and I think that's because I have this last comma and I'm just going to check to see if there are any commas in Jason you can't have the last element in an array have a comma on it so I think that's let me test it again all so as you can see we just received an ID we've got a success and then we have a URL Trace array which is empty if you think about this logically we don't get all the data that we send immediately because we need fir crawl to whip up the scraper you know do things to the data we could be feeding in 50 URLs here so instead of just having the data available to us now immediately what we need to do is we need to wait a little while wait until it's done and we need to Ping it and the reason why they've given us this ID parameter so that we could do the pinging so the way that you do this is you'd have to send a second HTTP request using this structure so the good news is we could just copy this over and then we can add a second HTTP request I don't know where that went but I guess I'm just going to create it over here I'm going to import the curl to this request just that then keep in mind that we just need to add our API key again because the previous node had it but this one doesn't so just going to go over here I'm going to copy this puppy go back over here I'm going to paste this in now technically what this is called is this is called polling polling is where you know you're you're you're attempting to request a resource that you don't know whether or not is ready and there's a fair amount of logic that I'd recommend putting into a polling flow where when you try it and if it doesn't work you wait a certain amount of time and you retry again for the purpose of this video I'm not going to put all that stuff inside but what I'm going to do is just set up this request I'm going to give this puppy a test let's just feed that in on the back end we got to put the extract ID over here where it said extract ID then I'm just going to give this a test looks I've issued a malformed request we just have to make sure that everything here is specify body let me just make sure there's nothing else in here it was a get request this is a get cool we're not going to send a body then awesome and now we have all of the data available to us automate your business in the copy field summary field left clicks an ad performance optimization agency Icebreaker hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website and then company name left click so quick and easy way you're going to have access to this template obviously without my API key in it and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away and then you just request to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful data that you could use for anything so this is what rapid API looks it's a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data emails phone numbers and stuff that from a website you could ping the ah refs SEO API you could find I don't know unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just a wrapper around what we're doing now in nadn but this website scraper API allows you to scrape some more Dynamic data now I'm not signed up to this so I'm going to have to go through the signup process and I'm going to show you guys what that looks but yeah we're going to we're going to run through an API request to Rapid API which is going to make this a lot easier just going to put in all of my information here and then I'm going to do the classic email verification just copy this puppy over no thank you rise I use a time management app called rise and every time I go on my Gmail I set my Gmail up as a definitely do not do during your workday let's just call it personal projects they don't ask me all these questions my goal today is to browse available apis awesome so that's their onboarding I think we're going to have to pay a little bit of money or something that which I'll sort out in a moment but the scraper that I want is I just want the website one so I'm going to type website in here I'm going to look for wherever it was earlier website scraper API and now check this out what we have is we have the app which is the name of the specific API that we're requesting we have an x-raid api-key and this is the API key we're going to use to make the request then we have the request URL which is what we're pinging and what we can do here is we can feed in the parameters what website we want to we want to scrape and then we can just give it give it a run so I'm going to have to subscribe to this in order to test it I'm just going to go to the basic plan and I'm going to pay money per month that probably seems the simplest way to do so and I just ran through the payment let's head over here and let's just run a test using my website URL we're going to test this endpoint now and now this going to go through Rapid API it's going to spin up the server and then it's going to send it and what we see here is we have multiple fields that Rapid apis or this particular scraper gives us let me just make this easier for you all to see we have a text content field with all of the content of the website which is cool this is what I did earlier but instead of me having to formulate this request try and parse it and try and use AI tokens what I did is I sent the request to rapid API and did it all for me then we also have an HTML content field I think we have one more here scroll all the way down to the bottom as you can see there is a ton of HTML and then we also have a list of all of the images on the website which is very very cool and easily formatted again something that I tried to do manually using AI but now you know we have everything in that nice absolute URL format and then if they find any social media links I don't believe there were more than Twitter but if they find anything that's at their Twitter Instagram whatever then we have the link over here it looks they even give you the scraping time and if they scrape emails or phone numbers they'll be there as so I mean rapid AP is obviously fantastic this is a high throughput sort of thing and why don't we run through what this would look if we were to run a curl request you see how it's automatically just formatting it as curl that just means we just jump back here connect this to my HTTP request module click import curl paste it in this import and it's going to go through and it's going to automatically map all these fields for me query parameter URL left click.",
      "similar_statements": [
        "a just this the prompts because you know I was just using their playground before we're going to need to convert this into a request for my service so I'm just going to paste The Prompt in here voila and now we need to put together what's called a schema where we have the objects that we asked for so in my case we asked for copy so I'm going to go Copy Type string then summary so we're going to go summary type string then Icebreaker it's going to be Icebreaker type string then guess what we have last but not least company name which is going to be type string we're also going to want to make these fields required you know you can set it up so they're not required when you do a request a fire call I'm I'm going to make it so they're required so I'll go copy summary Icebreaker and then company name you know what maybe I'll leave company name as unrequired if you think about it logically maybe not all the websites we're going to be scraping using this service are going to have the company names visible on the website I don't know but maybe so maybe I'll leave that as off great so now we have the API request formatted correctly all we need to do at this point is just click test step it looks we're getting a Json breaking error and I think that's because I have this last comma and I'm just going to check to see if there are any commas in Jason you can't have the last element in an array have a comma on it so I think that's let me test it again all so as you can see we just received an ID we've got a success and then we have a URL Trace array which is empty if you think about this logically we don't get all the data that we send immediately because we need fir crawl to whip up the scraper you know do things to the data we could be feeding in 50 URLs here so instead of just having the data available to us now immediately what we need to do is we need to wait a little while wait until it's done and we need to Ping it and the reason why they've given us this ID parameter so that we could do the pinging so the way that you do this is you'd have to send a second HTTP request using this structure so the good news is we could just copy this over and then we can add a second HTTP request I don't know where that went but I guess I'm just going to create it over here I'm going to import the curl to this request just that then keep in mind that we just need to add our API key again because the previous node had it but this one doesn't so just going to go over here I'm going to copy this puppy go back over here I'm going to paste this in now technically what this is called is this is called polling polling is where you know you're you're you're attempting to request a resource that you don't know whether or not is ready and there's a fair amount of logic that I'd recommend putting into a polling flow where when you try it and if it doesn't work you wait a certain amount of time and you retry again for the purpose of this video I'm not going to put all that stuff inside but what I'm going to do is just set up this request I'm going to give this puppy a test let's just feed that in on the back end we got to put the extract ID over here where it said extract ID then I'm just going to give this a test looks I've issued a malformed request we just have to make sure that everything here is specify body let me just make sure there's nothing else in here it was a get request this is a get cool we're not going to send a body then awesome and now we have all of the data available to us automate your business in the copy field summary field left clicks an ad performance optimization agency Icebreaker hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website and then company name left click so quick and easy way you're going to have access to this template obviously without my API key in it and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away and then you just request to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful data that you could use for anything so this is what rapid API looks it's a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data emails phone numbers and stuff that from a website you could ping the ah refs SEO API you could find I don't know unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just a wrapper around what we're doing now in nadn but this website scraper API allows you to scrape some more Dynamic data now I'm not signed up to this so I'm going to have to go through the signup process and I'm going to show you guys what that looks but yeah we're going to we're going to run through an API request to Rapid API which is going to make this a lot easier just going to put in all of my information here and then I'm going to do the classic email verification just copy this puppy over no thank you rise I use a time management app called rise and every time I go on my Gmail I set my Gmail up as a definitely do not do during your workday let's just call it personal projects they don't ask me all these questions my goal today is to browse available apis awesome so that's their onboarding I think we're going to have to pay a little bit of money or something that which I'll sort out in a moment but the scraper that I want is I just want the website one so I'm going to type website in here I'm going to look for wherever it was earlier website scraper API and now check this out what we have is we have the app which is the name of the specific API that we're requesting we have an x-raid api-key and this is the API key we're going to use to make the request then we have the request URL which is what we're pinging and what we can do here is we can feed in the parameters what website we want to we want to scrape and then we can just give it give it a run so I'm going to have to subscribe to this in order to test it I'm just going to go to the basic plan and I'm going to pay money per month that probably seems the simplest way to do so and I just ran through the payment let's head over here and let's just run a test using my website URL we're going to test this endpoint now and now this going to go through Rapid API it's going to spin up the server and then it's going to send it and what we see here is we have multiple fields that Rapid apis or this particular scraper gives us let me just make this easier for you all to see we have a text content field with all of the content of the website which is cool this is what I did earlier but instead of me having to formulate this request try and parse it and try and use AI tokens what I did is I sent the request to rapid API and did it all for me then we also have an HTML content field I think we have one more here scroll all the way down to the bottom as you can see there is a ton of HTML and then we also have a list of all of the images on the website which is very very cool and easily formatted again something that I tried to do manually using AI but now you know we have everything in that nice absolute URL format and then if they find any social media links I don't believe there were more than Twitter but if they find anything that's at their Twitter Instagram whatever then we have the link over here it looks they even give you the scraping time and if they scrape emails or phone numbers they'll be there as so I mean rapid AP is obviously fantastic this is a high throughput sort of thing and why don't we run through what this would look if we were to run a curl request you see how it's automatically just formatting it as curl that just means we just jump back here connect this to my HTTP request module click import curl paste it in this import and it's going to go through and it's going to automatically map all these fields for me query parameter URL left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "beautiful API key x-raid API host here's the host here's the name of the API key here's everything we need I can just recreate this request now inside of NN as opposed to being on rapid API and then I have all the data accessible to me here how cool is that so we can do this for any any major website really you know there are a lot of specific bespoke scrapers obviously which I don't know if you wanted to scrape let's go back to Discovery if you wanted to scrape Instagram or something you could scrape Instagram you could do Facebook scraping you could scrape these large giants that are quite difficult to do So Meta ad Library Facebook ad scraper and depending on the plan that you're at it might be more cost- effective for you to sign up to some sort of monthly recurring thing rather than just pay two cents every single time you make one of these requests you just kind of got to do that determination yourself if you're scraping I don't know 50 every day or 100 every day or something might be a dollar or two a day which is reasonable but maybe if you want to scrape 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites Dynamic sites all that fun stuff and then you can export that data as a cloud run to then send back sorry big sneeze to then send back to some API or some service and then automatically do parsing and stuff that so very cool I'm going to show you guys what that looks this is sort of a more customized way to build the stuff but I've seen a lot of people do this with naden so we're going to run through what it looks so first thing I'm going to want to do is I'm going to want to let's just go Cloud login or sorry start free 7-Day trial as you can see you know there's a free browser extension here if you wanted to do I don't know highs scale stuff you'd choose probably their project endpoint where we Sorry project plan where we have 5,000 URL credits we can run a bunch of tasks in parallel we could scrape Dynamic sites JavaScript sites we have a bunch of different export options then we can also just connect it directly to all of these what I'm going to do just because I want this to kind of work as a first go is I'm just going to sign up to a free Tri here beautiful just created my account just go left click give it a phone number we'll go left click.",
      "similar_statements": [
        "beautiful API key x-raid API host here's the host here's the name of the API key here's everything we need I can just recreate this request now inside of NN as opposed to being on rapid API and then I have all the data accessible to me here how cool is that so we can do this for any any major website really you know there are a lot of specific bespoke scrapers obviously which I don't know if you wanted to scrape let's go back to Discovery if you wanted to scrape Instagram or something you could scrape Instagram you could do Facebook scraping you could scrape these large giants that are quite difficult to do So Meta ad Library Facebook ad scraper and depending on the plan that you're at it might be more cost- effective for you to sign up to some sort of monthly recurring thing rather than just pay two cents every single time you make one of these requests you just kind of got to do that determination yourself if you're scraping I don't know 50 every day or 100 every day or something might be a dollar or two a day which is reasonable but maybe if you want to scrape 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites Dynamic sites all that fun stuff and then you can export that data as a cloud run to then send back sorry big sneeze to then send back to some API or some service and then automatically do parsing and stuff that so very cool I'm going to show you guys what that looks this is sort of a more customized way to build the stuff but I've seen a lot of people do this with naden so we're going to run through what it looks so first thing I'm going to want to do is I'm going to want to let's just go Cloud login or sorry start free 7-Day trial as you can see you know there's a free browser extension here if you wanted to do I don't know highs scale stuff you'd choose probably their project endpoint where we Sorry project plan where we have 5,000 URL credits we can run a bunch of tasks in parallel we could scrape Dynamic sites JavaScript sites we have a bunch of different export options then we can also just connect it directly to all of these what I'm going to do just because I want this to kind of work as a first go is I'm just going to sign up to a free Tri here beautiful just created my account just go left click give it a phone number we'll go left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "a we're going to go I don't know academic records needed per month we'll go 0 to th000 length of the project I don't know let's go two to 3 months great so now we can import and run our own site map or we can use a premade Community sit map what I'm going to do is I'm just going to import this we're then going to get the Chrome extension web scraper let me add that extension and it's going to download it do all that fun stuff beautiful so now we have it over here I'm just going to pin it to my browser to make my life easier go to left click.",
      "similar_statements": [
        "a we're going to go I don't know academic records needed per month we'll go 0 to th000 length of the project I don't know let's go two to 3 months great so now we can import and run our own site map or we can use a premade Community sit map what I'm going to do is I'm just going to import this we're then going to get the Chrome extension web scraper let me add that extension and it's going to download it do all that fun stuff beautiful so now we have it over here I'm just going to pin it to my browser to make my life easier go to left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "a open up this puppy now there's a bunch of tutorials and how to use this stuff that's not that big of a deal but the thing you need is you need to hold command plus option plus I to open up your developer tools and you'll just find it on the in my case the far so command option I that'll open up Dev tools you see all the way on the hand side here I have a couple other things make and and cookie editor but all the way on the hand side here we have this web scraper thing so we got what you're going to want to do first you're going to want to create a site map for the resource that you're going to want to scrape I'm just going to call it left click and I just want to scrape left click.",
      "similar_statements": [
        "a open up this puppy now there's a bunch of tutorials and how to use this stuff that's not that big of a deal but the thing you need is you need to hold command plus option plus I to open up your developer tools and you'll just find it on the in my case the far so command option I that'll open up Dev tools you see all the way on the hand side here I have a couple other things make and and cookie editor but all the way on the hand side here we have this web scraper thing so we got what you're going to want to do first you're going to want to create a site map for the resource that you're going to want to scrape I'm just going to call it left click and I just want to scrape left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "once we have our sitemap if I just give a quick little click I can then add a new selector and the really cool thing about this web scraper is if I just zoom out a little bit here what you can do is you can you can select the elements on the website that you want scraped so for instance it's a very quick and easy way to do this if you think about it is just to show you guys an example structure data is sort of an e-commerce application let's say you have the title of a product and you have I don't know the the description of a product so on my website really quick and easy way to do this is let's just call this products and it's a type text what I'm going to do is I'm going to click select then I'll just click on this I'll click on this as and as you see it'll automatically find all of the headings that I'm looking for so that's products we are going to then click data I'm going to click done selecting data preview as you can see it only selected one of them the very first so what we're going to want to do is go multiple and now if I data preview we get all of the headings which is very cool so now we have a a list of headings from here I'm going to save this selector I'm add a new one let's go product descriptions and then going to select this this it'll select all of them I'll go multiple data preview just to make sure that it looks good I'm getting no data extracted here oh sorry I didn't select the didn't finish it now we're getting product descriptions that's pretty cool this is me doing this sort of one at a time you can also group The selectors there you go it's offered to group it for me so we can group this into one object with products and then product descriptions so it's automatic group it now we have wrapper for products and products descriptions then we have products and product descriptions buried underneath we could go as far as we want with this but what I'm what I'm trying to show you guys is very simple and easy just drag your mouse over the specific thing you want if you select more than one it'll automatically find all of them on the website which is really cool great once we have this what I can do is I can go export sitemap so now I have all of the code on the website that goes and finds it for me then I can paste this in here I'll just call this left click scraper and I'm going to import this to my cloud scraper I think I'm running into oh sorry I don't think we can do a space there my bad just call it left click and now what we can do is we can just run a server instance that goes out and then scrapes this for us so I'm going to click scrape it looks I need to verify my email so just make sure you do that before you try and get ahead of yourself I was looks we just verified the email let's head back over here refresh then scrape we've now scheduled a scraping job for this sitemap scheduling you know in their lingo just means that it's now part of their big long queue of thousands of other things that they're probably scraping through their server and that's fine I just gave this a refresh and as we see we have now finished said scraping job we have all of the data available to us using their UI but now that we've gone through this process of you know building out this this thing how do we take that and then use it in our nadn flows so variety of ways if you wanted to connect this let's say to specific service Dropbox Google you know dump anow or something Google Drive I'd recommend just doing it directly through their integration it's just a lot easier to get the data there and then you can just connect it to n and watch the data as it comes in or something you can also use the web scraper API this is pretty neat because you can you know that's what we're going to end up using it was pretty neat because you can schedule jobs you can send jobs you can do everything just through the NN interface and then we can just retrieve the data afterwards which is pretty neat this is what you end up getting you end up with scraping job ID status sitemap all this fun stuff and then we can set a web hook URL where we we receive the request so let me check we need a scraping for testing you need a scraping job that has already been finished I think our scraping job has already been finished I'm just going to go htps back to my n8n flow I'm going to build an n8n web hook give that a click I'm not going to have any authentication let me just turn all this off what we want is we we want to use this as our test event we're going to go back to the API paste this in save and I'm just going to want to give it a test endpoint here so test looks the push notification was failed the reason why is because it's saying this web Hook is not registered for post request did you mean to make a get request beautiful thank you naden we absolutely did so I'm going to stop listening change your HTTP HTTP method here to post there's two ways to call a website and this is one of them I'm going to listen for test events go back here and then retest awesome looks we've now triggered the beginning of our workflow using this data let's see what sort of information was in it great we have the scraping job ID the status execution mode great so we have everything we need now to set up a flow where we can schedule something in this web scraper service that maybe monitors some I don't know list of e-commerce product or something every 12 hours and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks so we just got the data from web hook let's set up an HTTP request to their API now where we get the ID of the thing and then we can call we can call that back so got my API token over here I'm going head over to their API documentation first and then what we want to do is download these scrape data in CSV format at least in my case I imagine most of you guys are going to add this to a spreadsheet or whatever you can very easily do whatever you want there's also a Json format endpoint here but let's just do CSV for Simplicity so I've already gone ahead and I've gotten the method which was a get request so I've added that up here the URL was this over here with the scraping job ID and then your API token there so what I've done is I've grabbed the API token and the scraping job ID I mean I hardcoded it in here just while I was doing the testing let's make this Dynamic now drag the scraping job ID over here voila and then the API token if you guys remember back here on the API page you have your access to API token so just copy that over great and now if I run this I'm selecting that specific job then from here we have all the data that we just scraped as you can see there's a the way that CSV Works let me just copy this over here I just wanted to give this to you guys as an example of a different data type but maybe some people here aren't really familiar with it the way that it works is if I just paste this into a Google sheet you see how it looks this what what you can do is if you just split the text to columns you kind of see how kind of see how there's these four pettings there's web scraper order web scraper startup products and product descriptions I'm imagine scraping this for some lead genen applica sorry some some e-commerce application list of products here product descriptions maybe product prices maybe product whatever the heck you want so yeah you can you can put in a number of formats and I just wanted to give you guys an example what that looks the next way to scrape websites in naden is using appify if you guys are no strangers to this channel you know that I do appify all the time and I talk about them all the time because I think that they're just a great service they've now given me a 30% discount where anybody can use it for I was initially under the impression it was lifetime I think it's three months so you probably get 30% off your first three months just check the description if you want that but Cent how appify is is it is a Marketplace very similar to Rapid API although extraordinarily Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application so just as we had earlier we have Instagram scrapers we have Tik Tok scrapers we have email scrapers we have map scrapers Google Maps we could do I don't know Twitter scrapers medium scrapers any any service out there that has this Dynamic aspect to it that's not a simple HTTP request you can make you could scrape it using ampify and then obviously you you have things too just basic website crawlers you can generate screenshots of sites I mean there's just there's so many things let me walk you guys through what it looks now in my case I'm not going to sign up to appify because I have 400 accounts but trust me when I say it is a very easy and simple process you go to app ay.com you go get started you put in your email and your password they'll give you $5 in free platform credit you don't need any credit card and you can just get up and running and start using this for yourself super easily then the second that you have all that you'll be Creed with this screen it is a console screen don't be concerned when you see this you know this is super simple and and easy and and not a big deal this is one of my free accounts so I just wanted to show you guys what you can do with a free account but from here what you do is you go to the store and as you can see I'm just dark mode all this is the same thing we were just looking at before and then we're just going to run a test on the thing that we want to scrape so what I'm going to want to do is for the purposes of this I'm now going to do something different from what I was doing before which was just left click over and over and over I think that kind of gets boring what I'm going to do is I'm going to scrape Instagram posts so what I'm going to do is I'm going to feed in a name nickf this is just my Instagram which almost hit 10K in God 15 days or something that but I'm going to feed in my Instagram here and then I'm just going to grab I don't know the last 10 posts save and start this is now going to run an actor actor is just their term for scraper which will go out it'll extract data from my Nick surve Instagram and as you can see will get a ton of fields caption owner full name owner Instagram URL comments count first comment likes count timestamp query tag we get everything from these guys which is really cool this might take you know 30 40 50 seconds we are spinning up a server in real time every time you do this as you see in bottom left hand corner there's a little memory tab which shows that we are legitimately running a server with one gigabyte of memory now so generally my recommendation when you use appify is not to use it for oneoff requests this feed in 5 to 10 15 20 Instagram Pages but you know I just got the back and voila we we have it it's in front of us we have all of the data of that person's Instagram profile so you can see it's quite scalable in that way so the question is obviously how do you get this in NN appify has a really easy to use API which I doing all you have is if we wanted to get the let's see get data set items all I'm going to do is I'm just going to copy this go back here and then connect this to an HTTP request module as you could see we have this big long field here with my API appify API token and this specific data set that I'm looking for I'll show you how to get it dynamically but I just wanted to allow you to see how to get data in naden really quickly now if we go to the schem of view we can see we legitimately we we already have all of the data that we we had from appify a second ago super easy and quick and simple to get up and running we have the input URL field the ID field the type the short code caption now this is Instagram every looks we have some comments I don't have any style how do I create my man you just got to fake it till you make it I don't have any style either just some nerd in my mom's basement yeah so you you can scrape any resource you want here obviously I was scraping an Instagram resource but if you were scraping something else there'd be no change to this at all no change whatsoever now what we need in order to make this Dynamic make us able to run something in appify and then get it in NN so we need to set up an integration so just head over to this tab set up integration and then all you want to do is you just want to do web hook send an HTTP post web Hook when a specific actor event happens the actor event that we're going to want is when the run is succeeded the URL we're going to want to send this to if you think about it we just make another web hook request here web hook the URL we're going to want to send it to is going to be this test URL over here now I'm just going to delete all the header off stuff here because it just complicates it especially for beginners but we're going to copy this over head back over here paste in this URL and then let me see this is a post request I think I don't remember so we're going to have to double check I think it's a post request yeah and then what I'm going to do is I'm going to listen for a test event run the test web hook so we're listening we're making a get request so the fact that it hasn't connected yet probably tells me it's a post request so let's move over here move this down to post now let's listen to a test event let's run this puppy one more time so we just dispatched it and yeah the post request succeeded and what did we get we got tons of information we got a body with a user ID created at event data joke looks when you test something out they just send you a joke about how Chuck nurse can sketi a cow in two minutes have you ever heard of the word sketi before this moment I haven't I want to be known for my ability to sketi we'll go Instagram website scraper and now if we go back here we're now listening for a test event so I'm going to listen for this test event I'm going to run the same scraper again maybe we'll make it five posts per profile just to make it a little faster and once this is done what it's going to do is it's going to send a record of all the information we need to get the data over to Ann we're going to catch that information and then we're going to use it to query the the the database that it created for that particular Instagram run which will then enable us to do whatever the heck we want with it so it's now starting to crawl as we see here we had five requests so it should be able to do this in the next 5 seconds or so and once that's done we now have an actor succeeded event and then we have let me see the data that we want would be the default data set ID down over here so if we just go to that next HTTP request node what I can do is I can feed that in as a variable here let going to a default data set ID drag that in between these two little lines and now we can test that step with actual live data now we have everything that we need so I don't know maybe now you want to feed this into Ai and you want to have ai tell you something about the last five posts tell you wow those last five posts were amazing Nick I loved the specifically the one on Korea and I just wanted to send you over some quick assets to help you out you can now do super Dynamic and structured Outreach you could take that data and use it to draft up your own post I mean the options are ultimately unlimited that's why I love appify so much the sixth way to scrape websites with NN is data for Co this is another thirdparty service but it's a very high quality one that's specifically geared towards search engine optimization requests you guys haven't seen data for SEO before it's this big API stack that allows you to do things automatically query a service maybe some e-commerce website or some content website and then extract things in nicely structured formatting again specifically for SEO purposes tons of apis here as I mean a lot of these services are now going towards more Marketplace style stuff but just to give you guys an example you could Google really quickly to scrape a big list of Google search results for a term and then you could feed that into one of any of the other scrapers that we set up here to get data on stuff you could go Google Images Google Maps you could do Bing BYO YouTube Google's their own data set feature I don't really know what that is but I imagine it's pretty cool and then you can you can take this data and do really fun stuff with it so I'm just going to click try for free over here in the top hand corner show you guys what that looks and as you see here I signed in to data for SEO to my own account looks I have 38 million bajillion dollars but obviously you'd have to go through the rig Rolla creating your own account so why don't just do that with you and then I'll just use that account that is 38 million bajillion dollars we'll click try for free we'll go Nikki Wiki let's use a different email I need a business email huh that's unfortunate I do agree to the terms of use absolutely bicycle is that a bicycle that's not a bicycle what does it mean when I can't answer these does it mean that I'm a robot if you look at some of my posts some of my comments people would absolutely say yes it means that that you're a robot I don't know why people keep saying stuff dude Nick nice AI Avatar bro but I'm it's not an AI Avatar it's not an AI Avatar at all it's just me anyway so I need to activate my account doesn't look it allows you to feed in the code here so I'm just going to feed it in myself it's obviously you're getting a lot of spammers hence this bicycle stuff I don't know why the code isn't working here let me just copy this link address paste it in here instead there you go great so now you can sign in and once you're in you got also they're really big on on bicycles they're training a model to convert all ads on planet Earth into bicycles they'll give you a dollar worth of API access credits which is pretty cool I'm not going to do that I'm just going to go over to mine which is$ 38 million bajillion dollars with 99,999 estimated days to go and yeah let's run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily so maybe I'm in the UK and I want to scrape let me see a keyword ni arrive then I'm going to send a request to this API there's there's a bunch of other terms here that are going to make more sense if you're a SEO person but now we receive as output a structured object with a ton of stuff we have the first result here it's an organic one with some big URL a bunch of chips I'm I have a Knowledge Graph profile which is cool apparently it finds it says I'm a freelance writer you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers that do more I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to create your own very complex flows you can do other stuff as you grab a bunch of keyword data so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language I'm just not going to select an language and then I'll do a request so now it's going to find us a bunch of search volume related stuff so I don't know how many people are searching for me in 2025 apparently 390 is this per month H wonder if it's per month per day that's interesting I don't really know why they break it down by the month date yeah looks it's 390 per month so to the 390 people that are Googling me who are you and what do you want I'm just kidding you can do things you could find back links so you could find links for I believe you feed in a website URL and then it finds back links to that website so this is you technically now scraping a bunch of other websites looking for links to the specific resource that you have that's kind of neat it looks that found it immediately which is really really cool and it looks they're referring top level links that are Dooms BG bgs would be interesting I wonder where that's coming from there's a Content generation API playground so you could you know feed in some text and then have it generate other stuff but I think we're kind of getting away from the actual thing that matters which is the scraping of the scraping of the websites so yeah lots of stuff lots of stuff for sure now that's all good but let's turn this into an API call if we head over to the API of do data for SEO so in my case docs.",
      "similar_statements": [
        "once we have our sitemap if I just give a quick little click I can then add a new selector and the really cool thing about this web scraper is if I just zoom out a little bit here what you can do is you can you can select the elements on the website that you want scraped so for instance it's a very quick and easy way to do this if you think about it is just to show you guys an example structure data is sort of an e-commerce application let's say you have the title of a product and you have I don't know the the description of a product so on my website really quick and easy way to do this is let's just call this products and it's a type text what I'm going to do is I'm going to click select then I'll just click on this I'll click on this as and as you see it'll automatically find all of the headings that I'm looking for so that's products we are going to then click data I'm going to click done selecting data preview as you can see it only selected one of them the very first so what we're going to want to do is go multiple and now if I data preview we get all of the headings which is very cool so now we have a a list of headings from here I'm going to save this selector I'm add a new one let's go product descriptions and then going to select this this it'll select all of them I'll go multiple data preview just to make sure that it looks good I'm getting no data extracted here oh sorry I didn't select the didn't finish it now we're getting product descriptions that's pretty cool this is me doing this sort of one at a time you can also group The selectors there you go it's offered to group it for me so we can group this into one object with products and then product descriptions so it's automatic group it now we have wrapper for products and products descriptions then we have products and product descriptions buried underneath we could go as far as we want with this but what I'm what I'm trying to show you guys is very simple and easy just drag your mouse over the specific thing you want if you select more than one it'll automatically find all of them on the website which is really cool great once we have this what I can do is I can go export sitemap so now I have all of the code on the website that goes and finds it for me then I can paste this in here I'll just call this left click scraper and I'm going to import this to my cloud scraper I think I'm running into oh sorry I don't think we can do a space there my bad just call it left click and now what we can do is we can just run a server instance that goes out and then scrapes this for us so I'm going to click scrape it looks I need to verify my email so just make sure you do that before you try and get ahead of yourself I was looks we just verified the email let's head back over here refresh then scrape we've now scheduled a scraping job for this sitemap scheduling you know in their lingo just means that it's now part of their big long queue of thousands of other things that they're probably scraping through their server and that's fine I just gave this a refresh and as we see we have now finished said scraping job we have all of the data available to us using their UI but now that we've gone through this process of you know building out this this thing how do we take that and then use it in our nadn flows so variety of ways if you wanted to connect this let's say to specific service Dropbox Google you know dump anow or something Google Drive I'd recommend just doing it directly through their integration it's just a lot easier to get the data there and then you can just connect it to n and watch the data as it comes in or something you can also use the web scraper API this is pretty neat because you can you know that's what we're going to end up using it was pretty neat because you can schedule jobs you can send jobs you can do everything just through the NN interface and then we can just retrieve the data afterwards which is pretty neat this is what you end up getting you end up with scraping job ID status sitemap all this fun stuff and then we can set a web hook URL where we we receive the request so let me check we need a scraping for testing you need a scraping job that has already been finished I think our scraping job has already been finished I'm just going to go htps back to my n8n flow I'm going to build an n8n web hook give that a click I'm not going to have any authentication let me just turn all this off what we want is we we want to use this as our test event we're going to go back to the API paste this in save and I'm just going to want to give it a test endpoint here so test looks the push notification was failed the reason why is because it's saying this web Hook is not registered for post request did you mean to make a get request beautiful thank you naden we absolutely did so I'm going to stop listening change your HTTP HTTP method here to post there's two ways to call a website and this is one of them I'm going to listen for test events go back here and then retest awesome looks we've now triggered the beginning of our workflow using this data let's see what sort of information was in it great we have the scraping job ID the status execution mode great so we have everything we need now to set up a flow where we can schedule something in this web scraper service that maybe monitors some I don't know list of e-commerce product or something every 12 hours and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks so we just got the data from web hook let's set up an HTTP request to their API now where we get the ID of the thing and then we can call we can call that back so got my API token over here I'm going head over to their API documentation first and then what we want to do is download these scrape data in CSV format at least in my case I imagine most of you guys are going to add this to a spreadsheet or whatever you can very easily do whatever you want there's also a Json format endpoint here but let's just do CSV for Simplicity so I've already gone ahead and I've gotten the method which was a get request so I've added that up here the URL was this over here with the scraping job ID and then your API token there so what I've done is I've grabbed the API token and the scraping job ID I mean I hardcoded it in here just while I was doing the testing let's make this Dynamic now drag the scraping job ID over here voila and then the API token if you guys remember back here on the API page you have your access to API token so just copy that over great and now if I run this I'm selecting that specific job then from here we have all the data that we just scraped as you can see there's a the way that CSV Works let me just copy this over here I just wanted to give this to you guys as an example of a different data type but maybe some people here aren't really familiar with it the way that it works is if I just paste this into a Google sheet you see how it looks this what what you can do is if you just split the text to columns you kind of see how kind of see how there's these four pettings there's web scraper order web scraper startup products and product descriptions I'm imagine scraping this for some lead genen applica sorry some some e-commerce application list of products here product descriptions maybe product prices maybe product whatever the heck you want so yeah you can you can put in a number of formats and I just wanted to give you guys an example what that looks the next way to scrape websites in naden is using appify if you guys are no strangers to this channel you know that I do appify all the time and I talk about them all the time because I think that they're just a great service they've now given me a 30% discount where anybody can use it for I was initially under the impression it was lifetime I think it's three months so you probably get 30% off your first three months just check the description if you want that but Cent how appify is is it is a Marketplace very similar to Rapid API although extraordinarily Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application so just as we had earlier we have Instagram scrapers we have Tik Tok scrapers we have email scrapers we have map scrapers Google Maps we could do I don't know Twitter scrapers medium scrapers any any service out there that has this Dynamic aspect to it that's not a simple HTTP request you can make you could scrape it using ampify and then obviously you you have things too just basic website crawlers you can generate screenshots of sites I mean there's just there's so many things let me walk you guys through what it looks now in my case I'm not going to sign up to appify because I have 400 accounts but trust me when I say it is a very easy and simple process you go to app ay.com you go get started you put in your email and your password they'll give you $5 in free platform credit you don't need any credit card and you can just get up and running and start using this for yourself super easily then the second that you have all that you'll be Creed with this screen it is a console screen don't be concerned when you see this you know this is super simple and and easy and and not a big deal this is one of my free accounts so I just wanted to show you guys what you can do with a free account but from here what you do is you go to the store and as you can see I'm just dark mode all this is the same thing we were just looking at before and then we're just going to run a test on the thing that we want to scrape so what I'm going to want to do is for the purposes of this I'm now going to do something different from what I was doing before which was just left click over and over and over I think that kind of gets boring what I'm going to do is I'm going to scrape Instagram posts so what I'm going to do is I'm going to feed in a name nickf this is just my Instagram which almost hit 10K in God 15 days or something that but I'm going to feed in my Instagram here and then I'm just going to grab I don't know the last 10 posts save and start this is now going to run an actor actor is just their term for scraper which will go out it'll extract data from my Nick surve Instagram and as you can see will get a ton of fields caption owner full name owner Instagram URL comments count first comment likes count timestamp query tag we get everything from these guys which is really cool this might take you know 30 40 50 seconds we are spinning up a server in real time every time you do this as you see in bottom left hand corner there's a little memory tab which shows that we are legitimately running a server with one gigabyte of memory now so generally my recommendation when you use appify is not to use it for oneoff requests this feed in 5 to 10 15 20 Instagram Pages but you know I just got the back and voila we we have it it's in front of us we have all of the data of that person's Instagram profile so you can see it's quite scalable in that way so the question is obviously how do you get this in NN appify has a really easy to use API which I doing all you have is if we wanted to get the let's see get data set items all I'm going to do is I'm just going to copy this go back here and then connect this to an HTTP request module as you could see we have this big long field here with my API appify API token and this specific data set that I'm looking for I'll show you how to get it dynamically but I just wanted to allow you to see how to get data in naden really quickly now if we go to the schem of view we can see we legitimately we we already have all of the data that we we had from appify a second ago super easy and quick and simple to get up and running we have the input URL field the ID field the type the short code caption now this is Instagram every looks we have some comments I don't have any style how do I create my man you just got to fake it till you make it I don't have any style either just some nerd in my mom's basement yeah so you you can scrape any resource you want here obviously I was scraping an Instagram resource but if you were scraping something else there'd be no change to this at all no change whatsoever now what we need in order to make this Dynamic make us able to run something in appify and then get it in NN so we need to set up an integration so just head over to this tab set up integration and then all you want to do is you just want to do web hook send an HTTP post web Hook when a specific actor event happens the actor event that we're going to want is when the run is succeeded the URL we're going to want to send this to if you think about it we just make another web hook request here web hook the URL we're going to want to send it to is going to be this test URL over here now I'm just going to delete all the header off stuff here because it just complicates it especially for beginners but we're going to copy this over head back over here paste in this URL and then let me see this is a post request I think I don't remember so we're going to have to double check I think it's a post request yeah and then what I'm going to do is I'm going to listen for a test event run the test web hook so we're listening we're making a get request so the fact that it hasn't connected yet probably tells me it's a post request so let's move over here move this down to post now let's listen to a test event let's run this puppy one more time so we just dispatched it and yeah the post request succeeded and what did we get we got tons of information we got a body with a user ID created at event data joke looks when you test something out they just send you a joke about how Chuck nurse can sketi a cow in two minutes have you ever heard of the word sketi before this moment I haven't I want to be known for my ability to sketi we'll go Instagram website scraper and now if we go back here we're now listening for a test event so I'm going to listen for this test event I'm going to run the same scraper again maybe we'll make it five posts per profile just to make it a little faster and once this is done what it's going to do is it's going to send a record of all the information we need to get the data over to Ann we're going to catch that information and then we're going to use it to query the the the database that it created for that particular Instagram run which will then enable us to do whatever the heck we want with it so it's now starting to crawl as we see here we had five requests so it should be able to do this in the next 5 seconds or so and once that's done we now have an actor succeeded event and then we have let me see the data that we want would be the default data set ID down over here so if we just go to that next HTTP request node what I can do is I can feed that in as a variable here let going to a default data set ID drag that in between these two little lines and now we can test that step with actual live data now we have everything that we need so I don't know maybe now you want to feed this into Ai and you want to have ai tell you something about the last five posts tell you wow those last five posts were amazing Nick I loved the specifically the one on Korea and I just wanted to send you over some quick assets to help you out you can now do super Dynamic and structured Outreach you could take that data and use it to draft up your own post I mean the options are ultimately unlimited that's why I love appify so much the sixth way to scrape websites with NN is data for Co this is another thirdparty service but it's a very high quality one that's specifically geared towards search engine optimization requests you guys haven't seen data for SEO before it's this big API stack that allows you to do things automatically query a service maybe some e-commerce website or some content website and then extract things in nicely structured formatting again specifically for SEO purposes tons of apis here as I mean a lot of these services are now going towards more Marketplace style stuff but just to give you guys an example you could Google really quickly to scrape a big list of Google search results for a term and then you could feed that into one of any of the other scrapers that we set up here to get data on stuff you could go Google Images Google Maps you could do Bing BYO YouTube Google's their own data set feature I don't really know what that is but I imagine it's pretty cool and then you can you can take this data and do really fun stuff with it so I'm just going to click try for free over here in the top hand corner show you guys what that looks and as you see here I signed in to data for SEO to my own account looks I have 38 million bajillion dollars but obviously you'd have to go through the rig Rolla creating your own account so why don't just do that with you and then I'll just use that account that is 38 million bajillion dollars we'll click try for free we'll go Nikki Wiki let's use a different email I need a business email huh that's unfortunate I do agree to the terms of use absolutely bicycle is that a bicycle that's not a bicycle what does it mean when I can't answer these does it mean that I'm a robot if you look at some of my posts some of my comments people would absolutely say yes it means that that you're a robot I don't know why people keep saying stuff dude Nick nice AI Avatar bro but I'm it's not an AI Avatar it's not an AI Avatar at all it's just me anyway so I need to activate my account doesn't look it allows you to feed in the code here so I'm just going to feed it in myself it's obviously you're getting a lot of spammers hence this bicycle stuff I don't know why the code isn't working here let me just copy this link address paste it in here instead there you go great so now you can sign in and once you're in you got also they're really big on on bicycles they're training a model to convert all ads on planet Earth into bicycles they'll give you a dollar worth of API access credits which is pretty cool I'm not going to do that I'm just going to go over to mine which is$ 38 million bajillion dollars with 99,999 estimated days to go and yeah let's run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily so maybe I'm in the UK and I want to scrape let me see a keyword ni arrive then I'm going to send a request to this API there's there's a bunch of other terms here that are going to make more sense if you're a SEO person but now we receive as output a structured object with a ton of stuff we have the first result here it's an organic one with some big URL a bunch of chips I'm I have a Knowledge Graph profile which is cool apparently it finds it says I'm a freelance writer you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers that do more I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to create your own very complex flows you can do other stuff as you grab a bunch of keyword data so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language I'm just not going to select an language and then I'll do a request so now it's going to find us a bunch of search volume related stuff so I don't know how many people are searching for me in 2025 apparently 390 is this per month H wonder if it's per month per day that's interesting I don't really know why they break it down by the month date yeah looks it's 390 per month so to the 390 people that are Googling me who are you and what do you want I'm just kidding you can do things you could find back links so you could find links for I believe you feed in a website URL and then it finds back links to that website so this is you technically now scraping a bunch of other websites looking for links to the specific resource that you have that's kind of neat it looks that found it immediately which is really really cool and it looks they're referring top level links that are Dooms BG bgs would be interesting I wonder where that's coming from there's a Content generation API playground so you could you know feed in some text and then have it generate other stuff but I think we're kind of getting away from the actual thing that matters which is the scraping of the scraping of the websites so yeah lots of stuff lots of stuff for sure now that's all good but let's turn this into an API call if we head over to the API of do data for SEO so in my case docs."
      ],
      "frequency": 1
    },
    {
      "main_point": "datafor seo.com V3 _ page SL contentor parsing live that's what I'm I'm curious about you'll see that we have a post request that we need to send to this URL I have a curl just this which I can feed into an API request that's what I'm going to do so I'm going to go back over here and I'm just going to import this curl import and it's going to go through and it's going to parse out all these fields that I'm interested in with the URL which I'll go htps left click.",
      "similar_statements": [
        "datafor seo.com V3 _ page SL contentor parsing live that's what I'm I'm curious about you'll see that we have a post request that we need to send to this URL I have a curl just this which I can feed into an API request that's what I'm going to do so I'm going to go back over here and I'm just going to import this curl import and it's going to go through and it's going to parse out all these fields that I'm interested in with the URL which I'll go htps left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "AI and then we have sort of a gacha here that a lot of people don't understand this is the authorization the authorization is a little bit different from most of the easy authorizations we've had so far we have to convert it we have to go one one more step to make this work if I check out the let's see authorization here what we need is we need to get the login and then the P so this is your username and then your password then we have to Hash it or not hash it but we have to convert it into something called base 64 this is just how they do their API key stuff I guess it's kind of annoying but it's just part and parcel of working with some apis you're just not always going to have it available to you really easily so I'm just going to go back to data for SEO and then I'm going to grab my credentials so what we need to do is we need to base 64 encode the username and the password I'm just going to leave that at what I've done is I've gone through and done it in this edit Fields node what you need to do is you need to have your username or your login so maybe this is me searching Nix or have Reddit Nick left click.",
      "similar_statements": [
        "AI and then we have sort of a gacha here that a lot of people don't understand this is the authorization the authorization is a little bit different from most of the easy authorizations we've had so far we have to convert it we have to go one one more step to make this work if I check out the let's see authorization here what we need is we need to get the login and then the P so this is your username and then your password then we have to Hash it or not hash it but we have to convert it into something called base 64 this is just how they do their API key stuff I guess it's kind of annoying but it's just part and parcel of working with some apis you're just not always going to have it available to you really easily so I'm just going to go back to data for SEO and then I'm going to grab my credentials so what we need to do is we need to base 64 encode the username and the password I'm just going to leave that at what I've done is I've gone through and done it in this edit Fields node what you need to do is you need to have your username or your login so maybe this is me searching Nix or have Reddit Nick left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "so that might be my username and then my password is What's called the API password you can find that really quickly and easily just by going over here to API access and then API password if you just signed up it'll be visible here if it's been more than 24 hours you have to send it by email but anyway so that's that's where i' get the API password from and then once you feed it in over here where you're going to want to do is you're going to want to base 64 encode it this they just require you to use these creds or to operate with these creds as base 64 encoded versions Bas 64 is just a way to translate into a slightly different number format so once you have that you would just feed in the variable over here Ju Just as follows and then you can make a request to their API and receive data so it looks I was doing their content parsing live you know what I wanted to do is I just wanted to call their endpoint which I think was their instant Pages this one over here so it's just V3 once you've sorted this out by the way the AP gets extraordinarily easy to manage you just need to figure out the authentication from there on out all you're doing is just swapping out the requests so you know if you wanted to do instant Pages all I'm doing is pumping that in there I just sent a request and now I receive a bunch of links with different headings and and so on and so forth that's easy the seventh way to scrape websites and Ed end is using a third party application called crawl Bas they're known for their rotating proxies which allow you to send very high volume API requests so it's very proxy driven this is their website so it's a scraping platform similar to Rapid API and you know appify they support many of the major websites here and the reason why they're so good at this is just because they you know as I mentioned they rotate the hell out of these proxies so we're just going to sign up to Tri it free I'll use my business email here and then continue with Emil email we got to add a phone number obviously we're going to do less than a thousand I'm a CTO I don't want to what's the animal is that an animal yes it's an animal good God beep boop we're going to head over to my Gmail and receive this now so we need to confirm my account just going to copy this link address that I can do this in one page awesome we should be good to log in so that's what's happening we need to select the animal again just doesn't it doesn't believe really just doesn't believe great so now we have a crawling API smart proxy thing if you guys want to run I don't know use in apps that have a proxy field specifically I'm just going to keep things simple we're doing this in n8n so we're going to go crawl base API we have a th000 free crawls remaining very first thing we're going to want to do is just click start crawling now just to get up and running with the API and as you see here the these guys have probably one of the simplest apis possible all API URLs start with the folling base part click and then all you need to do in order to make an API call is run the following sort of line so this is a curl request obviously we're in n8n and one of the value valuable parts of NN is we can just import a COR request so I'm going to import it as you can see here we have a token field then we just have the URL field of the place we want to crawl so I'm going to do left click.",
      "similar_statements": [
        "so that might be my username and then my password is What's called the API password you can find that really quickly and easily just by going over here to API access and then API password if you just signed up it'll be visible here if it's been more than 24 hours you have to send it by email but anyway so that's that's where i' get the API password from and then once you feed it in over here where you're going to want to do is you're going to want to base 64 encode it this they just require you to use these creds or to operate with these creds as base 64 encoded versions Bas 64 is just a way to translate into a slightly different number format so once you have that you would just feed in the variable over here Ju Just as follows and then you can make a request to their API and receive data so it looks I was doing their content parsing live you know what I wanted to do is I just wanted to call their endpoint which I think was their instant Pages this one over here so it's just V3 once you've sorted this out by the way the AP gets extraordinarily easy to manage you just need to figure out the authentication from there on out all you're doing is just swapping out the requests so you know if you wanted to do instant Pages all I'm doing is pumping that in there I just sent a request and now I receive a bunch of links with different headings and and so on and so forth that's easy the seventh way to scrape websites and Ed end is using a third party application called crawl Bas they're known for their rotating proxies which allow you to send very high volume API requests so it's very proxy driven this is their website so it's a scraping platform similar to Rapid API and you know appify they support many of the major websites here and the reason why they're so good at this is just because they you know as I mentioned they rotate the hell out of these proxies so we're just going to sign up to Tri it free I'll use my business email here and then continue with Emil email we got to add a phone number obviously we're going to do less than a thousand I'm a CTO I don't want to what's the animal is that an animal yes it's an animal good God beep boop we're going to head over to my Gmail and receive this now so we need to confirm my account just going to copy this link address that I can do this in one page awesome we should be good to log in so that's what's happening we need to select the animal again just doesn't it doesn't believe really just doesn't believe great so now we have a crawling API smart proxy thing if you guys want to run I don't know use in apps that have a proxy field specifically I'm just going to keep things simple we're doing this in n8n so we're going to go crawl base API we have a th000 free crawls remaining very first thing we're going to want to do is just click start crawling now just to get up and running with the API and as you see here the these guys have probably one of the simplest apis possible all API URLs start with the folling base part click and then all you need to do in order to make an API call is run the following sort of line so this is a curl request obviously we're in n8n and one of the value valuable parts of NN is we can just import a COR request so I'm going to import it as you can see here we have a token field then we just have the URL field of the place we want to crawl so I'm going to do left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "for now I don't know if this token field was my real token I don't believe so maybe we'll give it a try maybe it's a test token or something so I'm now running this and it looks we just received a bunch of very spooky data I don't the spooky data no spooky data for us sometimes spooky data this H this seems kind of weird to me just give me one second to make sure that's we are receiving a data parameter back which is nice but yeah something about this is a little bit spooky was it a get request or was it a post request no I guess it's a get request strange very very strange anyway they give you two types of tokens here this is why I'm talking about it to begin with I'm also because I just used it before for a couple of applications and I found it very easy they give you a normal token and they give you a JavaScript token as so the reason why that's valuable is because if you're scraping one of these websites I talked about before where when you send a simple HTTP request nothing pops up this is the this is the purpose of this you feed in a JavaScript token when you use the JavaScript token it'll automatically launch a browser instance inside of craw base for you so instead of you getting just that empty thing back that I mentioned I'm you're going to get you're going to get a JavaScript version of the website where somebody went on the website it loaded really briefly and then they grabbed the code afterwards so yeah we have some some API call stuff over here this one's just using Amazon this is pretty interesting so I might give that a a go just to give you guys an example of said Amazon scrape let's just go www.amazon.com oh Amazon might be JavaScript so maybe we give that a go no it looks we we got the data from from Amazon which is pretty cool if you feed that into the markdown converter we had before it's going to feed in the HTML here pump it into a data key we've now converted this into this is very long let's go tabular we've now converted this into markdown which is cool and this is pretty long obviously has all of the images and has all of the information on the site which is cool and then we can feed it into open AI I did before where I message a model and I'm just going to copy from my previous application here to make my life a little bit easier where the heck are you and then we're just going to feed in the code here and then because I didn't feed in this we should now run this we're going to grab data from the site and we're going to try and I mean you know we kind of all know what Amazon is and what it does so I'm not expecting expecting anything spectacular but it's still going to go it's going to give me all of the text on this Amazon page and then I'm going to get a bunch of list of links absolute URLs ideally should play some Jeopardy music or going be able to play Star Wars music that'd be kind of cool we now have a schema with all of the links on the page which is pretty cool we have the plain text website copy we have a on line summary you know plain text website copy is a lot longer than this obviously it's just shortening and truncating it for us but yeah very quick and easy way to use crawl base for this now the value in crawl base is not necessarily just to send them to static websites I talked about it's to use highly scalable scraping where you're scraping any applications consistently as you see here the average API response time is between 4 to 10 seconds so you you will receive results back pretty quick if you wanted to just send one request or 20 requests every second think about it 20 requests a second times 60 seconds a minute is 1,200 requests times 60 minutes and an hour 72,000 requests sorry just jumping around the place here you can send 72,000 requests an hour which is crazy and you can do so as quickly and as easily as just adding an API call and then it'll automatically distinguish between a a plain text thing or a JavaScript thing the eighth way to scrape data in nadn specifically website resources is octop parse octoparse is very similar to some of the other services that we've talked about it is a web scraping tool that gives you quote unquote free web crawlers and I'm just a fan of their ux I think it's very clean I think the way that they have their signup flow and stuff's really easy so if you made it to this part of the tutorial and you have yet to sign up to one of these Services give octoparse give octoparse your thoughts let's double check that I haven't created an account using this no I haven't fantastic so I should be able to jump through and show you guys what this looks we have a verification code I'm going to paste in if you're not familiar with jumping around and stuff this or if you're wondering how I'm jumping around I'm just using a bunch of website hotkeys great account is now ready so we can start a free premium trial if you want I think you're going to have to add a card I don't know if I have enough credits to do anything but if I'm not then I'll start that trial in a second what you're going to have to do in order to make this work is you're going to want to have to download you're going to want to download the octoparse desktop app so let's give it a quick and easy go just going to drag this puppy if you are using something that is not Mac OS you will not have this strange drag and drop feature here once that is done you will have octo parse accessible just open that up yes I want to open this thank you and the cool thing about octoparse kind of relative to what else you know the other scraping applications I talked about is this is just running in a desktop app kind of in in your computer so it's cool because it's just easy to get up and running with and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and what'll happen is it'll launch an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm scraping all of this stuff now we have access to this sort of drag and drop or selector thing similar to what we had before if you click on one of these you'll see it allow you to select all similar Elements which is pretty sweet and then you can also do things click elements and so on and so forth extract the text Data here you can also tie that to other things so as you see I'm now mapping each of these very similarly to how I was doing before between the first field which is the title of the product and then the second field which is the field to so that's pretty sweet we could do the same thing with a number of things you could extract the headings and then the values and so on and so on and so forth but I'll kind of leave it there so once you're done selecting all the elements that you want all you do is you click run and you have a choice between running it on your device versus running it on the cloud so on the cloud is API supported that's how you're going to get stuff in NM but I just want you guys to know that you can also just run it here you could run it here load up the URL scrape all the things that you want on the specific page you're feeding in and then you can be done with it so I just selected run in the cloud it's now going to open up said Cloud instances as we could see we have this little field where it's running and extracting the data we're now done so I can export this data locally but I could also do a lot of other stuff which we'll show you in a second so you can dump this automatically to Google Sheets you could do zapier to connect to Google Sheets do some sort of web Hook connection export to cloud storage similar stuff to the the web scraping Chrome extension but for now let's just export this as Json give ourselves a little ad Json file here thank you and yeah now we have it locally now in order to connect with the octop par CPI what you're going to have to do is first you get up to request an access token the way that you do this is you send a post request to this URL here and the way that you format it is you need to send your username your password and then have the grantor type as password now password obviously just put in whatever your password is don't store it in PL text I'm doing with my hypothetical password put it somewhere else and then grab that data and then use it but the the output of this is we have this big long access token variable which is great after that if I just go back to their API here once we're here we can extract the data that we need so the thing that you're going to want is you're going to want get data by Offset you can also use get non-exported data which is interesting so I think this just dumps all of the data as not exported and then sends that over to you I believe but anyway you could also get the data by offset so if I go a get request to open api.",
      "similar_statements": [
        "for now I don't know if this token field was my real token I don't believe so maybe we'll give it a try maybe it's a test token or something so I'm now running this and it looks we just received a bunch of very spooky data I don't the spooky data no spooky data for us sometimes spooky data this H this seems kind of weird to me just give me one second to make sure that's we are receiving a data parameter back which is nice but yeah something about this is a little bit spooky was it a get request or was it a post request no I guess it's a get request strange very very strange anyway they give you two types of tokens here this is why I'm talking about it to begin with I'm also because I just used it before for a couple of applications and I found it very easy they give you a normal token and they give you a JavaScript token as so the reason why that's valuable is because if you're scraping one of these websites I talked about before where when you send a simple HTTP request nothing pops up this is the this is the purpose of this you feed in a JavaScript token when you use the JavaScript token it'll automatically launch a browser instance inside of craw base for you so instead of you getting just that empty thing back that I mentioned I'm you're going to get you're going to get a JavaScript version of the website where somebody went on the website it loaded really briefly and then they grabbed the code afterwards so yeah we have some some API call stuff over here this one's just using Amazon this is pretty interesting so I might give that a a go just to give you guys an example of said Amazon scrape let's just go www.amazon.com oh Amazon might be JavaScript so maybe we give that a go no it looks we we got the data from from Amazon which is pretty cool if you feed that into the markdown converter we had before it's going to feed in the HTML here pump it into a data key we've now converted this into this is very long let's go tabular we've now converted this into markdown which is cool and this is pretty long obviously has all of the images and has all of the information on the site which is cool and then we can feed it into open AI I did before where I message a model and I'm just going to copy from my previous application here to make my life a little bit easier where the heck are you and then we're just going to feed in the code here and then because I didn't feed in this we should now run this we're going to grab data from the site and we're going to try and I mean you know we kind of all know what Amazon is and what it does so I'm not expecting expecting anything spectacular but it's still going to go it's going to give me all of the text on this Amazon page and then I'm going to get a bunch of list of links absolute URLs ideally should play some Jeopardy music or going be able to play Star Wars music that'd be kind of cool we now have a schema with all of the links on the page which is pretty cool we have the plain text website copy we have a on line summary you know plain text website copy is a lot longer than this obviously it's just shortening and truncating it for us but yeah very quick and easy way to use crawl base for this now the value in crawl base is not necessarily just to send them to static websites I talked about it's to use highly scalable scraping where you're scraping any applications consistently as you see here the average API response time is between 4 to 10 seconds so you you will receive results back pretty quick if you wanted to just send one request or 20 requests every second think about it 20 requests a second times 60 seconds a minute is 1,200 requests times 60 minutes and an hour 72,000 requests sorry just jumping around the place here you can send 72,000 requests an hour which is crazy and you can do so as quickly and as easily as just adding an API call and then it'll automatically distinguish between a a plain text thing or a JavaScript thing the eighth way to scrape data in nadn specifically website resources is octop parse octoparse is very similar to some of the other services that we've talked about it is a web scraping tool that gives you quote unquote free web crawlers and I'm just a fan of their ux I think it's very clean I think the way that they have their signup flow and stuff's really easy so if you made it to this part of the tutorial and you have yet to sign up to one of these Services give octoparse give octoparse your thoughts let's double check that I haven't created an account using this no I haven't fantastic so I should be able to jump through and show you guys what this looks we have a verification code I'm going to paste in if you're not familiar with jumping around and stuff this or if you're wondering how I'm jumping around I'm just using a bunch of website hotkeys great account is now ready so we can start a free premium trial if you want I think you're going to have to add a card I don't know if I have enough credits to do anything but if I'm not then I'll start that trial in a second what you're going to have to do in order to make this work is you're going to want to have to download you're going to want to download the octoparse desktop app so let's give it a quick and easy go just going to drag this puppy if you are using something that is not Mac OS you will not have this strange drag and drop feature here once that is done you will have octo parse accessible just open that up yes I want to open this thank you and the cool thing about octoparse kind of relative to what else you know the other scraping applications I talked about is this is just running in a desktop app kind of in in your computer so it's cool because it's just easy to get up and running with and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and what'll happen is it'll launch an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm scraping all of this stuff now we have access to this sort of drag and drop or selector thing similar to what we had before if you click on one of these you'll see it allow you to select all similar Elements which is pretty sweet and then you can also do things click elements and so on and so forth extract the text Data here you can also tie that to other things so as you see I'm now mapping each of these very similarly to how I was doing before between the first field which is the title of the product and then the second field which is the field to so that's pretty sweet we could do the same thing with a number of things you could extract the headings and then the values and so on and so on and so forth but I'll kind of leave it there so once you're done selecting all the elements that you want all you do is you click run and you have a choice between running it on your device versus running it on the cloud so on the cloud is API supported that's how you're going to get stuff in NM but I just want you guys to know that you can also just run it here you could run it here load up the URL scrape all the things that you want on the specific page you're feeding in and then you can be done with it so I just selected run in the cloud it's now going to open up said Cloud instances as we could see we have this little field where it's running and extracting the data we're now done so I can export this data locally but I could also do a lot of other stuff which we'll show you in a second so you can dump this automatically to Google Sheets you could do zapier to connect to Google Sheets do some sort of web Hook connection export to cloud storage similar stuff to the the web scraping Chrome extension but for now let's just export this as Json give ourselves a little ad Json file here thank you and yeah now we have it locally now in order to connect with the octop par CPI what you're going to have to do is first you get up to request an access token the way that you do this is you send a post request to this URL here and the way that you format it is you need to send your username your password and then have the grantor type as password now password obviously just put in whatever your password is don't store it in PL text I'm doing with my hypothetical password put it somewhere else and then grab that data and then use it but the the output of this is we have this big long access token variable which is great after that if I just go back to their API here once we're here we can extract the data that we need so the thing that you're going to want is you're going to want get data by Offset you can also use get non-exported data which is interesting so I think this just dumps all of the data as not exported and then sends that over to you I believe but anyway you could also get the data by offset so if I go a get request to open api."
      ],
      "frequency": 1
    },
    {
      "main_point": "octop course.com SL all and then I just send a header with the URL parameter this is a get request we're going to send a header with the token so authorization Bearer and then feed in the access token here just make sure that this is just one space no it's two if I feed this in it's saying that it's a bad request let me just triple check why I think we need three Fields yeah I think we need three Fields my bad we need this get request then we need the authorization header I talked about then we need three Fields task ID yeah obviously we need to feed in the task ID so you need task ID offset or size so we'll feed this in as query parameters here so send query parameters the first value was task ID second one was offset and offset is no Capital the third was size offset's going to be zero size going to be I don't know let's just do 1,000 and what we need now is we need the task ID of the specific run that we just finished oh in order to get the task list you head over to task list top hand corner here task ID API so we now have access to this so if we go back to our NN instance we could feed that in here by test the step you'll see that we now have all the data that we just asked for earlier so a variety of ways to do this in practice octop par allows you to schedule runs you could schedule them using their you know whatever it is cloud service you could use it to scrape I don't know Twitter they have a variety of other scrapers that you can check out just heading over to this new here if we just go sorry go down to templates there's a variety of other ways to scrape Google job scraper glass door scraper super Pages scraper you could schedule these and then what you can do in na is you can just query it once a day grab all the data I showed you how to do a moment ago dump that into some sheet octoparse is pretty cool it's more of an industrial Enterprise level application to be honest so there might be some gotas if you're not super familiar with working with desktop apps and stuff but I I the idea that you can also just scrape locally which is pretty sweet and the last of our nine best ways to scrape websites in nadn is browserless now browserless runs a headless Chrome instance in the cloud this stuff is great for dynamic or heavy JavaScript websites if you've never used browser list before the cool part about browser list is allows you to bypass captas which is a big issue that a lot of people have so I'm going to click try it for free I'm going to enter my email address over here verify I need to submit a code so let's head back over here thank you thank you thank you thank you we have a ton of free trial signups obviously I don't have a promo code or anything don't have a company name I'm just going to enter a password I'm using this to get past to avoid setting up a puppeteer and playright server sure I'm going to click complete we're now going to have a th credits inside of browser list which is pretty sweet and we'll get a we'll get a full plan eventually we now have an API token so I can figure out how all of the stuff works here I'm just going to dive into the API I can figure out how all of the API stuff works using their API docs which are fantastic by the way and we don't want to do any of this stuff we just want to do HTTP apis brow list API index great so here's where we're at if you want to send and receive a request what you need to do is you send a request to one of these endpoints content unblock download function PDF screenshot scrape or performance what we want for the purpose of this is just let's do content this is the request over here so I'm just going to paste my API token up here copy this request feed it into nadn in the HTTP request module as per usual nice quick and easy I'm going to grab my API token and where it says your API token here I'm going to feed that in what I want as a website is just left click.",
      "similar_statements": [
        "octop course.com SL all and then I just send a header with the URL parameter this is a get request we're going to send a header with the token so authorization Bearer and then feed in the access token here just make sure that this is just one space no it's two if I feed this in it's saying that it's a bad request let me just triple check why I think we need three Fields yeah I think we need three Fields my bad we need this get request then we need the authorization header I talked about then we need three Fields task ID yeah obviously we need to feed in the task ID so you need task ID offset or size so we'll feed this in as query parameters here so send query parameters the first value was task ID second one was offset and offset is no Capital the third was size offset's going to be zero size going to be I don't know let's just do 1,000 and what we need now is we need the task ID of the specific run that we just finished oh in order to get the task list you head over to task list top hand corner here task ID API so we now have access to this so if we go back to our NN instance we could feed that in here by test the step you'll see that we now have all the data that we just asked for earlier so a variety of ways to do this in practice octop par allows you to schedule runs you could schedule them using their you know whatever it is cloud service you could use it to scrape I don't know Twitter they have a variety of other scrapers that you can check out just heading over to this new here if we just go sorry go down to templates there's a variety of other ways to scrape Google job scraper glass door scraper super Pages scraper you could schedule these and then what you can do in na is you can just query it once a day grab all the data I showed you how to do a moment ago dump that into some sheet octoparse is pretty cool it's more of an industrial Enterprise level application to be honest so there might be some gotas if you're not super familiar with working with desktop apps and stuff but I I the idea that you can also just scrape locally which is pretty sweet and the last of our nine best ways to scrape websites in nadn is browserless now browserless runs a headless Chrome instance in the cloud this stuff is great for dynamic or heavy JavaScript websites if you've never used browser list before the cool part about browser list is allows you to bypass captas which is a big issue that a lot of people have so I'm going to click try it for free I'm going to enter my email address over here verify I need to submit a code so let's head back over here thank you thank you thank you thank you we have a ton of free trial signups obviously I don't have a promo code or anything don't have a company name I'm just going to enter a password I'm using this to get past to avoid setting up a puppeteer and playright server sure I'm going to click complete we're now going to have a th credits inside of browser list which is pretty sweet and we'll get a we'll get a full plan eventually we now have an API token so I can figure out how all of the stuff works here I'm just going to dive into the API I can figure out how all of the API stuff works using their API docs which are fantastic by the way and we don't want to do any of this stuff we just want to do HTTP apis brow list API index great so here's where we're at if you want to send and receive a request what you need to do is you send a request to one of these endpoints content unblock download function PDF screenshot scrape or performance what we want for the purpose of this is just let's do content this is the request over here so I'm just going to paste my API token up here copy this request feed it into nadn in the HTTP request module as per usual nice quick and easy I'm going to grab my API token and where it says your API token here I'm going to feed that in what I want as a website is just left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "a I'm going to run test step we are now quering the pi and in seconds we have access to the data same thing that we had before but now we're using a pass through and browser list is a great pass through because you know they they allow you to scrape things that go far beyond the usual static site thing so honestly and I'm just leaving this as a secret and sort of a little I guess Easter egg for people that have made it this far in the video my go-to when scraping websites is as I mentioned do that HTTP request trans forg that works then do something fir C.D but if that doesn't work I I do something browserless that has all of this stuff built in and I especially use browser list anytime that there's some sort of you know application where I'm just going to save this so I can make all my HTP requests really easy especially when you know there's issues with captas and and accessing resources and stuff check this out not only can you do the actual scrape you can do a screenshot of the page as and because I've entered my token up here the requests that I'm going to setting up are as simple as importing the curl then clicking test step so straightforward we now have a file which is the screenshot now I used example domain there let's go left click.",
      "similar_statements": [
        "a I'm going to run test step we are now quering the pi and in seconds we have access to the data same thing that we had before but now we're using a pass through and browser list is a great pass through because you know they they allow you to scrape things that go far beyond the usual static site thing so honestly and I'm just leaving this as a secret and sort of a little I guess Easter egg for people that have made it this far in the video my go-to when scraping websites is as I mentioned do that HTTP request trans forg that works then do something fir C.D but if that doesn't work I I do something browserless that has all of this stuff built in and I especially use browser list anytime that there's some sort of you know application where I'm just going to save this so I can make all my HTP requests really easy especially when you know there's issues with captas and and accessing resources and stuff check this out not only can you do the actual scrape you can do a screenshot of the page as and because I've entered my token up here the requests that I'm going to setting up are as simple as importing the curl then clicking test step so straightforward we now have a file which is the screenshot now I used example domain there let's go left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "run this test now you can see we've received a screenshot of the of the website view very sexy and my website's pretty long so keep in mind and yeah you know obviously a lot you could do with that you can download the site you can turn the site into a PDF so that's pretty neat I don't think I've used this one before but for the purposes of this demonstration why don't we give it a try we'll go over here import the curl paste it in voila the website I'm going to do is left click.",
      "similar_statements": [
        "run this test now you can see we've received a screenshot of the of the website view very sexy and my website's pretty long so keep in mind and yeah you know obviously a lot you could do with that you can download the site you can turn the site into a PDF so that's pretty neat I don't think I've used this one before but for the purposes of this demonstration why don't we give it a try we'll go over here import the curl paste it in voila the website I'm going to do is left click."
      ],
      "frequency": 1
    },
    {
      "main_point": "aai going to test this step so now there servers doing a couple things I'm scraping the site then converting it all into PDF format probably screenshotting a bunch of stuff too if I view this now we now have my my file looks it didn't capture all of the color aspects that might just be difficult or whatever but I still have a PDF of the site which is pretty neat and yeah let you guys kind of screw around with this on your own but there are a variety of cool applications you can use browless for all I hope you guys appreciated the nine best ways to scrape websites in nadn as you guys could see it's a combination of on platform scraping using the HTTP request module a lot of API documentation stuff that if you want to get good at this I'm releasing a master class on API stuff as part of my next na tutorial video and then you know navigating this and then and then taking the data from these services and using them to do something that you want to do artificial intelligence to give you a summary of the site or generate ice breakers for you or do something else whether you're using a local application octop parse or maybe the web scraping CH Chrome extension or using something firra browserless appify rapid API and so on and so forth you now have everything that you need in order to scrape static sites Dynamic sites super Js heavy websites and even social media websites Tik Tok Twitter and Instagram thanks so much for making it to this point in the video if you have any suggestions for future content drop them down below more than happy to take your idea and run with it assuming it's something that I haven't done before and then if you guys could do me a really big solid subscribe do all that fun YouTube stuff and I'll catch you on the next video thank you very much",
      "similar_statements": [
        "aai going to test this step so now there servers doing a couple things I'm scraping the site then converting it all into PDF format probably screenshotting a bunch of stuff too if I view this now we now have my my file looks it didn't capture all of the color aspects that might just be difficult or whatever but I still have a PDF of the site which is pretty neat and yeah let you guys kind of screw around with this on your own but there are a variety of cool applications you can use browless for all I hope you guys appreciated the nine best ways to scrape websites in nadn as you guys could see it's a combination of on platform scraping using the HTTP request module a lot of API documentation stuff that if you want to get good at this I'm releasing a master class on API stuff as part of my next na tutorial video and then you know navigating this and then and then taking the data from these services and using them to do something that you want to do artificial intelligence to give you a summary of the site or generate ice breakers for you or do something else whether you're using a local application octop parse or maybe the web scraping CH Chrome extension or using something firra browserless appify rapid API and so on and so forth you now have everything that you need in order to scrape static sites Dynamic sites super Js heavy websites and even social media websites Tik Tok Twitter and Instagram thanks so much for making it to this point in the video if you have any suggestions for future content drop them down below more than happy to take your idea and run with it assuming it's something that I haven't done before and then if you guys could do me a really big solid subscribe do all that fun YouTube stuff and I'll catch you on the next video thank you very much"
      ],
      "frequency": 1
    }
  ],
  "original_length": 76831,
  "processed_length": 73183,
  "num_groups": 18
}