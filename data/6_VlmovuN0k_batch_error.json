{
  "timestamp": 1739188110.4902136,
  "error": "Error code: 400 - {'error': {'message': 'max_tokens is too large: 50000. This model supports at most 16384 completion tokens, whereas you provided 50000.', 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': None}}",
  "stack": "  File \"C:\\Users\\Big Daddy Pyatt\\CascadeProjects\\Tube2Link\\scripts\\test_batch_process.py\", line 86, in process_batch_from_enriched_prompt\n    completion = openai.chat.completions.create(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Big Daddy Pyatt\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Big Daddy Pyatt\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"C:\\Users\\Big Daddy Pyatt\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\_base_client.py\", line 1280, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Big Daddy Pyatt\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\_base_client.py\", line 957, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\Big Daddy Pyatt\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openai\\_base_client.py\", line 1061, in _request\n    raise self._make_status_error_from_response(err.response) from None\n"
}