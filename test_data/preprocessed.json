{
  "patterns": {
    "steps": [
      {
        "content": "all right I'm going to jump into NN in a minute and actually build these alongside you and one other thing I'm going to do is I'm actually going to sign up to all the services in front of you walk you through the Authentication and the onboarding flows and get your API keys and stuff like that but just before I do want to explain very quickly the difference between a static site and then a dynamic site because if you don't know this um scraping just gets a lot harder and so we're just going to cover this in like 30 seconds and we can move on so basically um if this is you",
        "context_before": "so this video is just going to give you all the sauce you're going to learn everything you need to scrape websites like that on your own let's get into it",
        "context_after": "okay you're just this wonderful smiley person and you want to access a static website what you're doing is you're sending a request over basically to just like some document you know think about this as just like a piece of paper on a cupboard and there's a bunch of text on this piece of paper and what you do is you say hey can I have this piece of paper and then the piece of paper just comes back to you with all of the information inside of the piece of paper",
        "position": 3
      },
      {
        "content": "and then they kind of confuse it with this next step which is dynamic a dynamic site essentially is not like that at all basically what you're doing is you're sending a request to a piece of paper but the piece of paper has nothing on it",
        "context_before": "and so um this is where you know a lot of people think all websites are are at",
        "context_after": "okay what happens is this piece of paper then sends a request to some other dude which I guess in this case is just a server really who will then he has a trusty pen in his hand and he'll actually write all of the stuff on said piece of paper",
        "position": 7
      },
      {
        "content": "and and you know being practical about it so the first major way to scrape websites in NN is using direct h HTTP requests this is also what I like to think of as the Magic in scraping itself what we're going to do is we're going to use a node called the HTTP request node to send a get request to the website we want this is going to work with static websites and non JavaScript resources",
        "context_before": "so hopefully we at least understand that there's that difference between static and dynamic sites here um I'm not going to go into it more than that we're actually just going to dive in with both feet start doing a little bit of scraping and then we'll kind of see where we land I find the best way to do this stuff is just by example",
        "context_after": "so let me give you guys a website that I'm going to be scraping here this is my own site it's called left click I'm about to do a redesign",
        "position": 18
      },
      {
        "content": "and then what I'm going to do is I'm going to add three prompt I'm going to add a system prompt first I'll say you are a helpful intelligent web scraping assistant",
        "context_before": "and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection um and grab your API key and paste it in there's some instructions that allow you to do so right over here uh what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the uh I want to say most cost effective one as of the time of this recording",
        "context_after": "then I'm going to add a user prompt and I'll say your task is to take the raw markdown of a website and convert it into structured data use the following format",
        "position": 43
      },
      {
        "content": "and then I'm going to give it an example of what I want in what's called Json JavaScript object notation format so the very first thing I'm going to do is I'm going to have it just pull out all the links on the website because I find that that's a very common scraping application so I go links",
        "context_before": "then I'm going to add a user prompt and I'll say your task is to take the raw markdown of a website and convert it into structured data use the following format",
        "context_after": "and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B uh SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't actually do anything for us",
        "position": 45
      },
      {
        "content": "right then we have a oneline summary of the site so this is a very simple example of scraping we're scraping a static resource obviously but when I build scrapers for clients or for my own business this is always my first pass I will always just make a basic HTTP request to the resource that I'm looking at because if I can make that http request work whether it's a get request or whatever the the the rest of my life building scraper building the scraper is so easy I just take the data I process it usually using AI or some very cheap",
        "context_before": "and then we have a big chunk of plain text website copy",
        "context_after": "Tok cheap per token thing and then voila you know like we've basically built out a scraper in this case and it's only taken us what three nodes right",
        "position": 58
      },
      {
        "content": "so that's number one the second way to scrape websites in NN is using a third party service called fir crawl and making an HTTP request to it",
        "context_before": "Tok cheap per token thing and then voila you know like we've basically built out a scraper in this case and it's only taken us what three nodes right",
        "context_after": "I'm using something called their extract endpoint but just to make a long story short fire craw is a very simple but High uh bandwidth service that turns websites into large language model ready data and basically you know how earlier we had to do HTTP request",
        "position": 60
      },
      {
        "content": "I want to extract a oneline summary of the website let's do all of the text on the website all of the copy on the website in plain text let's do a oneline summary of the website a oneline Icebreaker I can use as the first line of of an of a cold email to the owner and uh the company name and a list of the services they provide let's do that this is a lot of requests we're asking it to do like seven or eight things",
        "context_before": "so um let's go from the homepage at left click.",
        "context_after": "but all I need to do in order to make this work is I click generate parameters it's going to basically now generate me a big object with a bunch of things so copy summary Icebreaker company name",
        "position": 78
      },
      {
        "content": "right so instead of just having the data available to us right now immediately what we need to do is we need to basically wait a little while wait until it's done and we need to Ping it and the reason why they've given us this ID parameter so that we could do the pinging so the way that you do this is you'd have to send a second HTTP request using this structure so the good news is we could just copy this over",
        "context_before": "and then we have a URL Trace array which is empty um if you think about this logically we don't actually get all the data that we send immediately because we need fir crawl to whip up the scraper you know do things to the data we could be feeding in 50 URLs here",
        "context_after": "and then we can add a second um HTTP request I don't know where that went",
        "position": 125
      },
      {
        "content": "and then we can add a second um HTTP request I don't know where that went",
        "context_before": "right so instead of just having the data available to us right now immediately what we need to do is we need to basically wait a little while wait until it's done and we need to Ping it and the reason why they've given us this ID parameter so that we could do the pinging so the way that you do this is you'd have to send a second HTTP request using this structure so the good news is we could just copy this over",
        "context_after": "but I guess I'm just going to create it over here I'm going to import the curl to this request just like that then keep in mind that we just need to add our API key again because the previous node had it",
        "position": 126
      },
      {
        "content": "hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website uh and then company name left click so quick and easy way uh you're going to have access to this template obviously without my API key in it um and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is basically a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for um you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away um and then you just request uh to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful um data that you could use for basically anything so this is what rapid API looks like it's basically a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data like emails phone numbers and stuff like that from a website you could ping the ah refs SEO API you could find uh I don't know like unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work um just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just like a wrapper around what we're doing right now in nadn um but this website scraper API allows you to scrape some more Dynamic data um now I'm not signed up to this",
        "context_before": "and now we have all of the data available to us automate your business in the copy field summary field left clicks an ad performance optimization agency Icebreaker",
        "context_after": "so I'm going to have to go through the signup process",
        "position": 132
      },
      {
        "content": "but maybe if you want to scrape like 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool um in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but um that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites",
        "context_before": "right like if you're scraping uh I don't know 50 every day or 100 every day or something might be a dollar or two a day which is reasonable",
        "context_after": "Dynamic sites all that fun stuff",
        "position": 165
      },
      {
        "content": "um so we're going to run through what it looks like so first thing I'm going to want to do is I'm going to want to let's just go Cloud login or sorry um start free 7-Day trial as you can see you know there's a free browser extension here if you wanted to do uh I don't know like highs scale stuff you'd choose probably their project um endpoint where we Sorry project plan where we have 5,000 URL credits we can run a bunch of tasks in parallel we could scrape Dynamic sites JavaScript sites we have a bunch of different export options then we can also just connect it directly to all of these um what I'm going to do just because I want this to kind of work as a first go is I'm just going to sign up to a free Tri here beautiful just created my account just go left click give it a phone number we'll go left click.",
        "context_before": "but I've seen a lot of people do this with naden",
        "context_after": "a we're going to go I don't know academic records needed per month we'll go 0 to th000 length of the project uh I don't know let's go two to 3 months",
        "position": 169
      },
      {
        "content": "um so we got what you're going to want to do first you're going to want to create a site map for the resource that you're going to want to scrape I'm just going to call it left click",
        "context_before": "I that'll open up Dev tools you see all the way on the right hand side here I have a couple other things like make and and cookie editor but all the way on the right hand side here we have this web scraper thing",
        "context_after": "and I just want to scrape left click.",
        "position": 177
      },
      {
        "content": "once we have our sitemap if I just give a quick little click I can then add a new selector and the really cool thing about this web scraper is um if I just zoom out a little bit here uh what you can do is you can you can select the elements on the website that you want scraped so for instance it's a very quick and easy way to do this if you think about it is like just to show you guys an example structure data is uh sort of like an e-commerce application let's say you have like the title of a product and you have like I don't know the the description of a product so on my website really quick and easy way to do this is let's just call this products and it's a type text what I'm going to do is I'm going to click select then I'll just click on this I'll click on this as well and as you see it'll automatically find all of the headings that I'm looking for so that's products we are going to then click data I'm going to click done selecting data preview as you can see it only selected one of them the very first",
        "context_before": "okay",
        "context_after": "so what we're going to want to do is go multiple",
        "position": 180
      },
      {
        "content": "and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping um we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks like so we just got the data from web hook let's set up an HTTP request to their API now where we basically get the ID of the thing",
        "context_before": "so we basically have everything we need now to set up a flow where we can schedule something in this web scraper service that maybe monitors some I don't know list of e-commerce product or something every 12 hours",
        "context_after": "and then we can call uh we can call that back",
        "position": 207
      },
      {
        "content": "so got my API token over here I'm going head over to their API documentation first",
        "context_before": "and then we can call uh we can call that back",
        "context_after": "okay",
        "position": 209
      },
      {
        "content": "yeah you can you can put in like a number of formats and I just wanted to give you guys an example what that looks like the next way to scrape websites in naden is using appify if you guys are no strangers to this channel you know that I do appify all the time",
        "context_before": "so",
        "context_after": "and I talk about them all the time because I think that they're just a great service um they've now given me a 30% discount where anybody can use it for I was initially under the impression it was lifetime",
        "position": 221
      },
      {
        "content": "so you probably get 30% off your first three months just check the um description if you want that",
        "context_before": "I think it's three months",
        "context_after": "but Cent how appify is is it is a Marketplace very similar to Rapid API um although extraordinarily well Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application",
        "position": 224
      },
      {
        "content": "so just as we had earlier we have Instagram scrapers we have Tik Tok scrapers we have email scrapers we have map scrapers Google Maps we could do I don't know Twitter scrapers uh medium scrapers right basically any any service out there that has this Dynamic aspect to it that's not a simple HTTP request you can make you could scrape it using ampify and then obviously you you have things too like just like basic website crawlers you can generate screenshots of sites I mean there's just there's so many things let me walk you guys through what it looks like now in my case I'm not actually going to sign up to appify because I have like 400 accounts but trust me when I say it is a very easy and simple process you go to app ay.com you go get started you put in your email and your password they'll give you $5 in free platform credit you don't need any credit card and you can just get up and running and start using this for yourself super easily then the second that you have all that you'll be Creed with this screen it is a console screen don't be concerned when you see this um you know this is super simple and and easy and and not a big deal this is one of my free accounts",
        "context_before": "but Cent how appify is is it is a Marketplace very similar to Rapid API um although extraordinarily well Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application",
        "context_after": "um so I just wanted to show you guys what you can do with a free account uh but from here what you do is you go to the store and as you can see I'm just dark mode all this is the same thing we were just looking at before",
        "position": 226
      },
      {
        "content": "and then I'm just going to grab like I don't know the last 10 posts okay save and start this is now going to run an actor actor is just their term for scraper which will go out it'll extract data from my Nick surve Instagram and as you can see will get a ton of fields caption owner full name owner Instagram URL comments count first comment likes count timestamp query tag we get everything from these guys which is really cool this might take you know 30 40 50 seconds we are spinning up a server in real time every time you do this as you see in bottom left hand corner there's a little memory tab which shows that we are legitimately running a server with one gigabyte of memory right now so generally my recommendation when you use appify is not to use it for oneoff requests like this feed in 5 to 10 15 20 Instagram Pages",
        "context_before": "but I'm going to feed in my Instagram here",
        "context_after": "uh but you know I just got the back",
        "position": 235
      },
      {
        "content": "but I just wanted to like allow you to see how to get data in naden really quickly now if we go to the schem of view we can see we legitimately we we already have all of the data that we we had from appify a second ago okay super easy and quick and simple to get up and running um we have the input URL field the ID field the type the short code caption now this is Instagram um every looks like we have some comments I don't have any style how do I create my man you just got to fake it till you make it",
        "context_before": "and this specific data set that I'm looking for I'll show you how to get it dynamically",
        "context_after": "I don't have any style either just some nerd in my mom's basement",
        "position": 243
      },
      {
        "content": "so it's now starting to crawl as we see here we had five requests so it should be able to do this in like the next 5 seconds",
        "context_before": "and then we're going to use it to query the the the database basically that it created for that particular Instagram run which will then enable us to do whatever the heck we want with it",
        "context_after": "or so okay",
        "position": 259
      },
      {
        "content": "so if we just go to that next HTTP request node what I can do is I can feed that in as a variable right here let going to a default data set ID drag that in between these two little lines and now we can test that step with actual live data now we have everything that we need so I don't know maybe now you want to feed this into Ai",
        "context_before": "and then we have uh let me see the data that we want would be the default data set ID down over here",
        "context_after": "and you want to have ai tell you something about the last five posts tell you",
        "position": 263
      },
      {
        "content": "I mean the options are ultimately unlimited that's why I love appify so much the sixth way to scrape websites with NN is data for Co this is another thirdparty service but it's a very high quality one that's specifically geared towards search engine optimization requests you guys haven't seen data for SEO before it's basically this big API stack that allows you to do things like automatically query a service maybe some e-commerce website or some content website and then like extract things in nicely structured formatting um again specifically for SEO purposes tons of apis here as well I mean a lot of these services are now going towards like more Marketplace style stuff",
        "context_before": "right you can now do super Dynamic and structured Outreach you could take that data and use it to like draft up your own post",
        "context_after": "but just to give you guys an example you could like Google really quickly to scrape a big list of Google search results for a term",
        "position": 269
      },
      {
        "content": "and yeah let's actually run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call um what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily",
        "context_before": "so now you can sign in and once you're in you got also um they're actually really big on on bicycles they're training um a model to convert all ads on planet Earth into bicycles they'll actually give you a dollar worth of API access uh credits which is pretty cool um I'm not going to do that I'm just going to go over to mine which is$ 38 million bajillion dollars with 99,999 estimated days to go um",
        "context_after": "so maybe I'm in the UK and I want to scrape um let me see a keyword ni arrive",
        "position": 287
      },
      {
        "content": "right we have um the first result here it's like an organic one with some big URL a bunch of chips um I'm I have like a Knowledge Graph profile which is cool",
        "context_before": "okay then I'm going to send a request to this API there's there's a bunch of other terms here that are going to make more sense if you're a SEO person um but now we receive as output a structured object with a ton of stuff",
        "context_after": "apparently it finds it says I'm a freelance writer",
        "position": 290
      },
      {
        "content": "I just sent a request and now I receive a bunch of links with different headings and and so on and so forth that's easy the seventh way to scrape websites and Ed end is using a third party application called crawl Bas they're known for their rotating proxies which allow you to send very high volume um API requests",
        "context_before": "so you know if you wanted to do instant Pages all I'm doing is pumping that in there",
        "context_after": "so um it's very proxy driven this is their website so it's a scraping platform similar to Rapid API um and uh you know appify they support many of the major websites here and um the reason why they're so good at this is just because they you know as I mentioned they rotate the hell out of these proxies",
        "position": 330
      },
      {
        "content": "so we're going to go crawl base API we have a th000 free crawls remaining very first thing we're going to want to do is just click start crawling now just to get up and running with the API um and as you see here the these guys have probably one of the simplest apis possible all API URLs start with the folling base part click",
        "context_before": "so now we have a crawling API smart proxy thing if you guys want to run like uh I don't know use in apps that have a proxy field specifically I'm just going to keep things simple we're doing this in n8n",
        "context_after": "and then all you need to do in order to make an API call is run the following sort of line",
        "position": 340
      },
      {
        "content": "so I'm now running this and it looks like we just received a bunch of very spooky data I don't like the spooky data no spooky data for us um sometimes spooky data like this H this seems kind of weird to me actually just give me one second to make sure that's right we are receiving a data parameter back which is nice",
        "context_before": "maybe it's like a test token or something",
        "context_after": "but yeah something about this is a little bit spooky um was it a get request or was it a post request",
        "position": 347
      },
      {
        "content": "but yeah very quick and easy way to use crawl base for this now the value in crawl base is not necessarily just to send them to static websites like I talked about it's to use like highly scalable scraping where you're scraping any applications consistently um as you see here the average API response time is between 4 to 10 seconds",
        "context_before": "obviously it's just shortening and truncating it for us",
        "context_after": "so you you will receive results back pretty quick if you wanted to just send one request or 20 requests every second think about it like 20 requests a second times 60 seconds a minute is 1,200 requests times 60 minutes and an hour 72,000 requests",
        "position": 374
      },
      {
        "content": "so you you will receive results back pretty quick if you wanted to just send one request or 20 requests every second think about it like 20 requests a second times 60 seconds a minute is 1,200 requests times 60 minutes and an hour 72,000 requests",
        "context_before": "but yeah very quick and easy way to use crawl base for this now the value in crawl base is not necessarily just to send them to static websites like I talked about it's to use like highly scalable scraping where you're scraping any applications consistently um as you see here the average API response time is between 4 to 10 seconds",
        "context_after": "right um sorry just jumping around the place here you can send 72,000 requests basically an hour which is crazy um and you can do so as quickly and as easily as just like adding an API call like",
        "position": 375
      },
      {
        "content": "but if I'm not then I'll start that trial in a second what you're going to have to do in order to make this work is you're going to want to have to download you're going to want to download the octoparse desktop app",
        "context_before": "um I don't know if I have enough credits to actually do anything",
        "context_after": "so let's give it a quick and easy go just going to drag this puppy",
        "position": 386
      },
      {
        "content": "and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and basically what'll happen is um it'll actually launch like an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm actually like scraping all of this stuff",
        "context_before": "so like it's cool because it's just easy to get up and running with um",
        "context_after": "okay now we have access to this sort of drag and drop or um selector thing similar to what we had before if you click on one of these you'll see it allow you to select all similar Elements which is pretty sweet",
        "position": 391
      },
      {
        "content": "so as you see I'm now mapping each of these very similarly to how I was doing before between the first field which is the title of the product and then the second field which is like the field to uh so that's pretty sweet we could do the same thing with a number of things you could extract like the headings and then the values and so on and so on and so forth but I'll kind of leave it there um so once you're done selecting all the elements that you want all you do is you click run and you have a choice between running it on your device versus running it on the cloud so um on the cloud is API supported that's how you're going to get stuff in NM",
        "context_before": "um you can also tie that to other things right",
        "context_after": "but I just want you guys to know that you can also just run it here you could run it here load up the URL scrape all the things that you want on the specific page you're feeding in",
        "position": 395
      },
      {
        "content": "um but I could also do a lot of other stuff which we'll show you in a second",
        "context_before": "so I just selected run in the cloud it's now going to open up said Cloud instances as we could see we have this little field where it's running and extracting the data we're now done so I can export this data locally",
        "context_after": "so um you can dump this automatically to Google Sheets you could do zapier to connect to Google Sheets do like some sort of web Hook connection export to cloud storage uh similar stuff to the the web scraping Chrome extension um but for now let's just export this as Json give ourselves a little ad Json file here thank you",
        "position": 399
      },
      {
        "content": "yeah now we have it locally now in order to connect with the octop par CPI what you're going to have to do is first you get up to request an access token the way that you do this is you send a post request to this URL here and the way that you format it is you need to send your username your password and then have the grantor type as password okay now password obviously just put in whatever your password is don't store it in PL text like I'm doing um with my hypothetical password put it somewhere else and then grab that data and then use it um but the the output of this is we have this big long access token variable which is great after that if I just go back to their API here um once we're here we can actually extract the data that we need so basically the thing that you're going to want is you're going to want um get data by Offset you can also use get non-exported data which is interesting so I think this just like dumps all of the data as not exported um and then sends that over to you",
        "context_before": "and",
        "context_after": "I believe",
        "position": 402
      },
      {
        "content": "so um we'll feed this in as query parameters here so send query parameters the first value was task ID second one was offset and uh offset is no Capital the third was size offset's going to be zero size going to be I don't know let's just do 1,000 and what we need now is we need the task ID of the specific run that we just finished oh in order to get the task list you head over to task list top right hand corner here task ID API",
        "context_before": "right obviously we need to feed in the task ID so you need task ID offset or size",
        "context_after": "so we now have access to this so if we go back to our NN instance we could feed that in here by test the step you'll see that we now have all the data that we just asked for earlier so a variety of ways to do this um in practice like octop par allows you to schedule runs you could schedule them um using their you know whatever it is uh uh like cloud service you could use it to scrape I don't know Twitter",
        "position": 411
      },
      {
        "content": "a I'm going to run test step we are now quering the pi and in seconds we have access to the data same thing that we had before but now we're using a pass through and browser list is a great pass through um because you know uh they they allow you to scrape things that go far beyond the usual static site thing so like honestly",
        "context_before": "and where it says your API token here I'm going to feed that in what I want as a website is just left click.",
        "context_after": "and I'm just leaving this as a secret and sort of a little I guess Easter egg for people that have made it this far in the video like my go-to when scraping websites is as I mentioned do that HTTP request trans forg that works then do something like fir C.D",
        "position": 426
      },
      {
        "content": "I hope you guys appreciated the nine best ways to scrape websites in nadn as you guys could see it's a combination of on platform scraping using the HTTP request module a lot of like API documentation stuff like that if you want to get good at this I'm releasing a master class on API stuff um uh as part of my next na tutorial video uh",
        "context_before": "and yeah let you guys kind of screw around with this on your own but there are a variety of cool applications you can use browless for all right",
        "context_after": "and then you know navigating this and then and then taking the data from these services and using them to do something that you want to do like artificial intelligence to give you a summary of the site or generate ice breakers for you or do something else um whether you're using a local application like octop parse or maybe the web scraping CH uh Chrome extension or using something like firra browserless appify rapid API and so on and so forth um you now have everything that you need in order to scrape static sites Dynamic sites super Js heavy websites and even social media websites like Tik Tok Twitter and Instagram thanks so much for making it to this point in the video if you have any suggestions for future content drop them down below more than happy to take your idea and run with it assuming it's something that I haven't done before and then if you guys could do me a really big solid like subscribe do all that fun YouTube stuff",
        "position": 438
      },
      {
        "content": "and I'll catch you on the next video thank you very much",
        "context_before": "and then you know navigating this and then and then taking the data from these services and using them to do something that you want to do like artificial intelligence to give you a summary of the site or generate ice breakers for you or do something else um whether you're using a local application like octop parse or maybe the web scraping CH uh Chrome extension or using something like firra browserless appify rapid API and so on and so forth um you now have everything that you need in order to scrape static sites Dynamic sites super Js heavy websites and even social media websites like Tik Tok Twitter and Instagram thanks so much for making it to this point in the video if you have any suggestions for future content drop them down below more than happy to take your idea and run with it assuming it's something that I haven't done before and then if you guys could do me a really big solid like subscribe do all that fun YouTube stuff",
        "context_after": "",
        "position": 440
      }
    ],
    "examples": [
      {
        "content": "so uh we're just going to go HTTP request HTTP request node looks like this we have a method field a URL field authentication field query parameters headers body and then some options down here as well all",
        "context_before": "and I'm just going to pretend that I haven't done any of this",
        "context_after": "I'm going to do is I'm just going to paste in the website that I want to visit",
        "position": 24
      },
      {
        "content": "so if we go to curl as you can see what we need to do is we need to format a request that looks something like this",
        "context_before": "and then we're going to want to turn that into basically our HTTP request let me show you what that looks like I'm just going to do all of this stuff in curl",
        "context_after": "but we need to make sure it's using the extract endpoint",
        "position": 91
      },
      {
        "content": "a just like this the prompts um because you know I was just using their playground before we're actually going to need to convert this into a request for my service",
        "context_before": "so what I'm going to do is I'm going to delete most of these I'll go back to my left click.",
        "context_after": "so I'm just going to paste The Prompt in here voila",
        "position": 107
      },
      {
        "content": "and why don't we actually run through what this would look like if we were to run a curl request you see how it's automatically just formatting it as curl well that just means we just jump back here connect this to my HTTP request module click import curl paste it in like this import",
        "context_before": "so I mean rapid AP is obviously fantastic this is a high throughput sort of thing",
        "context_after": "and it's actually going to go through and it's going to automatically map all these fields for me right query parameter URL left click.",
        "position": 158
      },
      {
        "content": "but maybe some people here aren't really familiar with it basically the way that it works is if I just paste this into like a Google sheet you see how it looks like this what what you can do is if you just um split the text to columns you kind of see how kind of see how there's like these four pettings there's web scraper order web scraper startup products and product descriptions",
        "context_before": "so just copy that over uh great and now if I run this I'm actually selecting that specific job then from here we have all the data that we just scraped as you can see there's like a uh the way that CSV Works actually let me just copy this over here I just wanted to give this to you guys as an example of a different data type",
        "context_after": "right",
        "position": 217
      },
      {
        "content": "and then I'm just going to grab like I don't know the last 10 posts okay save and start this is now going to run an actor actor is just their term for scraper which will go out it'll extract data from my Nick surve Instagram and as you can see will get a ton of fields caption owner full name owner Instagram URL comments count first comment likes count timestamp query tag we get everything from these guys which is really cool this might take you know 30 40 50 seconds we are spinning up a server in real time every time you do this as you see in bottom left hand corner there's a little memory tab which shows that we are legitimately running a server with one gigabyte of memory right now so generally my recommendation when you use appify is not to use it for oneoff requests like this feed in 5 to 10 15 20 Instagram Pages",
        "context_before": "but I'm going to feed in my Instagram here",
        "context_after": "uh but you know I just got the back",
        "position": 235
      },
      {
        "content": "um well I have a curl just like this which I can feed into um an API request that's what I'm going to do",
        "context_before": "datafor seo.com V3 _ page SL contentor parsing live that's what I'm I'm curious about you'll see that we have a post request that we need to send to this URL",
        "context_after": "so I'm going to go back over here",
        "position": 303
      },
      {
        "content": "and then once you feed it in over here where you're going to want to do is you're going to want to base 64 encode it like this they just require you to use these creds um or to operate with these creds as base 64 encoded versions Bas 64 is just a way to like translate into a slightly different number format so once you have that you would just feed in the variable right over here Ju",
        "context_before": "so that's um that's where i' get the API password from uh",
        "context_after": "Just as follows and then you can make a request to their API and receive data",
        "position": 323
      },
      {
        "content": "so I'm now running this and it looks like we just received a bunch of very spooky data I don't like the spooky data no spooky data for us um sometimes spooky data like this H this seems kind of weird to me actually just give me one second to make sure that's right we are receiving a data parameter back which is nice",
        "context_before": "maybe it's like a test token or something",
        "context_after": "but yeah something about this is a little bit spooky um was it a get request or was it a post request",
        "position": 347
      },
      {
        "content": "so the reason why that's valuable is because if you're scraping one of these websites I talked about before where when you send a simple HTTP request nothing pops up like this is the this is the purpose of this you actually feed in a JavaScript",
        "context_before": "and they give you a JavaScript um token as well",
        "context_after": "token",
        "position": 353
      },
      {
        "content": "so I should be able to jump through and show you guys what this looks like we have a verification code I'm going to paste in if you're not familiar with jumping around and stuff like this um or if you're wondering how I'm jumping around I'm just using a bunch of website hotkeys",
        "context_before": "no I haven't fantastic",
        "context_after": "okay great account is now ready",
        "position": 382
      }
    ],
    "key_points": [
      {
        "content": "all right I'm going to jump into NN in a minute and actually build these alongside you and one other thing I'm going to do is I'm actually going to sign up to all the services in front of you walk you through the Authentication and the onboarding flows and get your API keys and stuff like that but just before I do want to explain very quickly the difference between a static site and then a dynamic site because if you don't know this um scraping just gets a lot harder and so we're just going to cover this in like 30 seconds and we can move on so basically um if this is you",
        "context_before": "so this video is just going to give you all the sauce you're going to learn everything you need to scrape websites like that on your own let's get into it",
        "context_after": "okay you're just this wonderful smiley person and you want to access a static website what you're doing is you're sending a request over basically to just like some document you know think about this as just like a piece of paper on a cupboard and there's a bunch of text on this piece of paper and what you do is you say hey can I have this piece of paper and then the piece of paper just comes back to you with all of the information inside of the piece of paper",
        "position": 3
      },
      {
        "content": "and then they kind of confuse it with this next step which is dynamic a dynamic site essentially is not like that at all basically what you're doing is you're sending a request to a piece of paper but the piece of paper has nothing on it",
        "context_before": "and so um this is where you know a lot of people think all websites are are at",
        "context_after": "okay what happens is this piece of paper then sends a request to some other dude which I guess in this case is just a server really who will then he has a trusty pen in his hand and he'll actually write all of the stuff on said piece of paper",
        "position": 7
      },
      {
        "content": "and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection um and grab your API key and paste it in there's some instructions that allow you to do so right over here uh what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the uh I want to say most cost effective one as of the time of this recording",
        "context_before": "so I'll go down to open Ai",
        "context_after": "and then what I'm going to do is I'm going to add three prompt I'm going to add a system prompt first I'll say you are a helpful intelligent web scraping assistant",
        "position": 42
      },
      {
        "content": "and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B uh SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't actually do anything for us",
        "context_before": "and then I'm going to give it an example of what I want in what's called Json JavaScript object notation format so the very first thing I'm going to do is I'm going to have it just pull out all the links on the website because I find that that's a very common scraping application so I go links",
        "context_after": "right uh what we what we want is we want this instead we want left click aka the root of the domain and then um left click _ logogram and that's how we get to the actual file asset",
        "position": 46
      },
      {
        "content": "so kind of a kind of a middleman and then all I'm going to do so if I go back to my example we have an API key here which we're going to need so I'm going to go here and then paste in an API key so that's how that work works right authorization is going to be the name value is going to be bear with a capital b space",
        "context_before": "right",
        "context_after": "and then the API key",
        "position": 103
      },
      {
        "content": "and then the API key",
        "context_before": "so kind of a kind of a middleman and then all I'm going to do so if I go back to my example we have an API key here which we're going to need so I'm going to go here and then paste in an API key so that's how that work works right authorization is going to be the name value is going to be bear with a capital b space",
        "context_after": "and then we also have a body that we need to uh adjust or edit and this body is where we're going to put the links that we want to actually have scraped with the extract end point",
        "position": 104
      },
      {
        "content": "but I guess I'm just going to create it over here I'm going to import the curl to this request just like that then keep in mind that we just need to add our API key again because the previous node had it",
        "context_before": "and then we can add a second um HTTP request I don't know where that went",
        "context_after": "but this one doesn't so just going to go over here I'm going to copy this puppy go back over here I'm going to paste this in now technically what this is called is this is called polling um polling uh is where you know you're you're you're attempting to request a resource that you don't know whether or not is ready and there's a fair amount of logic that I'd recommend like putting into a polling flow where like when you try it and if it doesn't work basically you wait a certain amount of time and you retry again for the purpose of this video I'm not going to put all that stuff inside but um what I'm going to do is just set up this request I'm going to give this puppy a test let's just feed that in on the back end we got to put the extract ID right right over here where it said extract ID",
        "position": 127
      },
      {
        "content": "hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website uh and then company name left click so quick and easy way uh you're going to have access to this template obviously without my API key in it um and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is basically a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for um you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away um and then you just request uh to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful um data that you could use for basically anything so this is what rapid API looks like it's basically a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data like emails phone numbers and stuff like that from a website you could ping the ah refs SEO API you could find uh I don't know like unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work um just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just like a wrapper around what we're doing right now in nadn um but this website scraper API allows you to scrape some more Dynamic data um now I'm not signed up to this",
        "context_before": "and now we have all of the data available to us automate your business in the copy field summary field left clicks an ad performance optimization agency Icebreaker",
        "context_after": "so I'm going to have to go through the signup process",
        "position": 132
      },
      {
        "content": "uh I'm going to look for wherever it was earlier website scraper API and now check this out what we have is we have the app which is the name of the specific API that we're requesting we have an x-raid api-key and this is the API key we're going to use to make the request then we have the request URL which is basically what we're pinging and what we can do here is we can feed in the parameters",
        "context_before": "so I'm going to type website in here",
        "context_after": "okay",
        "position": 143
      },
      {
        "content": "beautiful um API key x-raid API host here's the host here's the name of the API key here's everything we need",
        "context_before": "and it's actually going to go through and it's going to automatically map all these fields for me right query parameter URL left click.",
        "context_after": "well I can actually just recreate this request now inside of NN as opposed to being on rapid API and then I have all the data accessible to me here how cool is that so we can do this for any any major website",
        "position": 160
      },
      {
        "content": "but maybe if you want to scrape like 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool um in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but um that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites",
        "context_before": "right like if you're scraping uh I don't know 50 every day or 100 every day or something might be a dollar or two a day which is reasonable",
        "context_after": "Dynamic sites all that fun stuff",
        "position": 165
      },
      {
        "content": "and then the API token if you guys remember back here on the API page you have your access to API token",
        "context_before": "I mean I hardcoded it in here just while I was doing the testing let's actually make this Dynamic now drag the scraping job ID right over here voila",
        "context_after": "so just copy that over uh great and now if I run this I'm actually selecting that specific job then from here we have all the data that we just scraped as you can see there's like a uh the way that CSV Works actually let me just copy this over here I just wanted to give this to you guys as an example of a different data type",
        "position": 215
      },
      {
        "content": "um but we're going to copy this over head back over here paste in this URL and then let me see this is a post request I think I don't actually remember so we're going to have to double check I think it's a post request",
        "context_before": "um obviously I was scraping an Instagram resource but like if you were scraping something else there'd be no change to this at all no change whatsoever now uh basically what we need in order to make this Dynamic basically make us able to run something in appify and then get it in NN so we need to set up an integration so just head over to this tab set up integration and then all you want to do is you just want to do web hook send an HTTP post web Hook when a specific actor event happens the actor event that we're going to want is basically when the run is succeeded the URL we're going to want to send this to if you think about it we just actually make another web hook request here web hook the URL we're going to want to send it to is going to be this test URL over here now I'm just going to delete all the header off stuff here because um it just uh complicates it especially for beginners",
        "context_after": "yeah",
        "position": 249
      },
      {
        "content": "so maybe I'm in the UK and I want to scrape um let me see a keyword ni arrive",
        "context_before": "and yeah let's actually run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call um what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily",
        "context_after": "okay then I'm going to send a request to this API there's there's a bunch of other terms here that are going to make more sense if you're a SEO person um but now we receive as output a structured object with a ton of stuff",
        "position": 288
      },
      {
        "content": "um you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers um that do more like I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to like create your own very complex flows you can do other stuff as well um you grab a bunch of keyword data",
        "context_before": "apparently it finds it says I'm a freelance writer",
        "context_after": "so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language um I'm just not going to select an language",
        "position": 292
      },
      {
        "content": "so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language um I'm just not going to select an language",
        "context_before": "um you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers um that do more like I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to like create your own very complex flows you can do other stuff as well um you grab a bunch of keyword data",
        "context_after": "and then I'll do a request so now it's going to find us um a bunch of search volume related stuff",
        "position": 293
      },
      {
        "content": "but we have to convert it into something called base 64 um this is just how they do their API key stuff I guess it's kind of annoying",
        "context_before": "and then your password then we have to Hash it or not hash it",
        "context_after": "but it's just part and parcel of working with some apis you're just not always going to have it available to you really easily",
        "position": 312
      },
      {
        "content": "no it looks like we um we got the data from from Amazon which is pretty cool if you feed that into the markdown converter like we had before it's going to feed in the HTML here pump it into a data key we've now converted this into uh this is very long let's go tabular",
        "context_before": "so maybe we give that a go",
        "context_after": "we've now converted this into markdown which is cool and this is pretty long right obviously has all of the images and has all of the information on the site which is cool",
        "position": 362
      },
      {
        "content": "so I should be able to jump through and show you guys what this looks like we have a verification code I'm going to paste in if you're not familiar with jumping around and stuff like this um or if you're wondering how I'm jumping around I'm just using a bunch of website hotkeys",
        "context_before": "no I haven't fantastic",
        "context_after": "okay great account is now ready",
        "position": 382
      },
      {
        "content": "and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and basically what'll happen is um it'll actually launch like an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm actually like scraping all of this stuff",
        "context_before": "so like it's cool because it's just easy to get up and running with um",
        "context_after": "okay now we have access to this sort of drag and drop or um selector thing similar to what we had before if you click on one of these you'll see it allow you to select all similar Elements which is pretty sweet",
        "position": 391
      }
    ],
    "lists": [],
    "code_blocks": []
  },
  "semantic": {
    "actions": [
      {
        "content": "hey Nick here today I'm going to show you the nine best ways to scrape any website in nadn you're going to be able to scrape static sites Dynamic sites JavaScript social media whatever the heck you want by the end of the video you'll know how to do it",
        "importance": 0.20046402513980865
      },
      {
        "content": "so this video is just going to give you all the sauce you're going to learn everything you need to scrape websites like that on your own let's get into it",
        "importance": 0.2548321783542633
      },
      {
        "content": "all right I'm going to jump into NN in a minute and actually build these alongside you and one other thing I'm going to do is I'm actually going to sign up to all the services in front of you walk you through the Authentication and the onboarding flows and get your API keys and stuff like that but just before I do want to explain very quickly the difference between a static site and then a dynamic site because if you don't know this um scraping just gets a lot harder and so we're just going to cover this in like 30 seconds and we can move on so basically um if this is you",
        "importance": 0.23577475547790527
      },
      {
        "content": "okay you're just this wonderful smiley person and you want to access a static website what you're doing is you're sending a request over basically to just like some document you know think about this as just like a piece of paper on a cupboard and there's a bunch of text on this piece of paper and what you do is you say hey can I have this piece of paper and then the piece of paper just comes back to you with all of the information inside of the piece of paper",
        "importance": 0.2613759934902191
      },
      {
        "content": "and then they kind of confuse it with this next step which is dynamic a dynamic site essentially is not like that at all basically what you're doing is you're sending a request to a piece of paper but the piece of paper has nothing on it",
        "importance": 0.19274604320526123
      },
      {
        "content": "so um there's actually that intermediate step okay where basically you are pinging some sort of uh you know domain name or whatever then that domain name shoots some code over forces a server to generate all of the contents on that domain and then you get it this is obviously kind of a two-step process and then this is a three-step process",
        "importance": 0.17818470299243927
      },
      {
        "content": "so if you just understand that um you know when you scrape a dynamic resource what you're really doing is you're sending a request to a page which sends a request back to another server which then fills your thing this element eliminates 99% of the confusion because most of the time like scraping issues are hey",
        "importance": 0.20846855640411377
      },
      {
        "content": "so hopefully we at least understand that there's that difference between static and dynamic sites here um I'm not going to go into it more than that we're actually just going to dive in with both feet start doing a little bit of scraping and then we'll kind of see where we land I find the best way to do this stuff is just by example",
        "importance": 0.21559247374534607
      },
      {
        "content": "and and you know being practical about it so the first major way to scrape websites in NN is using direct h HTTP requests this is also what I like to think of as the Magic in scraping itself what we're going to do is we're going to use a node called the HTTP request node to send a get request to the website we want this is going to work with static websites and non JavaScript resources",
        "importance": 0.2260614037513733
      },
      {
        "content": "so let me give you guys a website that I'm going to be scraping here this is my own site it's called left click I'm about to do a redesign",
        "importance": 0.2579813599586487
      },
      {
        "content": "I I did it in code and basically all this is is just a document somewhere on my or on on a server some more so what I'm going to want to do is",
        "importance": 0.22165930271148682
      },
      {
        "content": "and I'm just going to pretend that I haven't done any of this",
        "importance": 0.13215765357017517
      },
      {
        "content": "so uh we're just going to go HTTP request HTTP request node looks like this we have a method field a URL field authentication field query parameters headers body and then some options down here as well all",
        "importance": 0.15930473804473877
      },
      {
        "content": "I'm going to do is I'm just going to paste in the website that I want to visit",
        "importance": 0.2564518451690674
      },
      {
        "content": "so if I were to zoom in over here you see where it says I don't know let's let's go to my website let's just find a little bit of little bit of texture build hands off growth systems",
        "importance": 0.12208835035562515
      },
      {
        "content": "so basically what I'm trying to say is everything over here on the right hand side this is the entire site we can do anything we want with this information um and we can carry this information forward to to do any one of our any one of many flows so in my case right looking at a bunch of code isn't really very pretty so one big thing that you'll find in the vast majority of modern um scraping applications is you'll find that they'll take that HTML which we saw earlier and they'll convert it to something called markdown",
        "importance": 0.26878485083580017
      },
      {
        "content": "okay so um this is a markdown node we have a mode of HTML to markdown and all I'm going to do is I'm going to grab that data and I'm going stick it in the HTML section of the HTML to markdown converter",
        "importance": 0.1931125819683075
      },
      {
        "content": "what do you think is going to happen when I test this",
        "importance": 0.10918277502059937
      },
      {
        "content": "and I find that manipulating file formats is a big part of what makes a good scraper a good scraper so now we have something in what's called markdown format what's the value there well markdown format does two things for us one",
        "importance": 0.14872342348098755
      },
      {
        "content": "so I'll go down to open Ai",
        "importance": 0.1423763781785965
      },
      {
        "content": "and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection um and grab your API key and paste it in there's some instructions that allow you to do so right over here uh what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the uh I want to say most cost effective one as of the time of this recording",
        "importance": 0.125139519572258
      },
      {
        "content": "and then what I'm going to do is I'm going to add three prompt I'm going to add a system prompt first I'll say you are a helpful intelligent web scraping assistant",
        "importance": 0.28791213035583496
      },
      {
        "content": "then I'm going to add a user prompt and I'll say your task is to take the raw markdown of a website and convert it into structured data use the following format",
        "importance": 0.18410317599773407
      },
      {
        "content": "and then I'm going to give it an example of what I want in what's called Json JavaScript object notation format so the very first thing I'm going to do is I'm going to have it just pull out all the links on the website because I find that that's a very common scraping application so I go links",
        "importance": 0.244273841381073
      },
      {
        "content": "and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B uh SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't actually do anything for us",
        "importance": 0.1927015483379364
      },
      {
        "content": "right uh what we what we want is we want this instead we want left click aka the root of the domain and then um left click _ logogram and that's how we get to the actual file asset",
        "importance": 0.12521161139011383
      },
      {
        "content": "and then why don't we do one more thing why don't we have like a summarized or summary let's do one line summary just to show you guys you can also use AI to do other cool stuff you could take this oneline summary and feed it into some big sequence you could have ai write an icebreaker for an email you could do a things with this but I'll say on line summary um brief summarization of what the site is and how what the site is about let's do that",
        "importance": 0.1930384486913681
      },
      {
        "content": "and then the final thing is I'm going to add one more user prompt I'm just going to draw drag all of that markdown data in here then I'm going to click output content as Json I'm going to test the step I'm going to take a sip of my coffee while this puppy processes and we now have our output on the right hand side if we go to schema view what you can see is we've now generated basically an array of links on the rightand side which contains every link on this website very cool looks like the vast majority of these are type form links for some reason don't really know what's about that",
        "importance": 0.21184638142585754
      },
      {
        "content": "um anyway you could obviously just get it to Output one link or tell it like make sure all the links are unique or something um",
        "importance": 0.19620844721794128
      },
      {
        "content": "right then we have a oneline summary of the site so this is a very simple example of scraping we're scraping a static resource obviously but when I build scrapers for clients or for my own business this is always my first pass I will always just make a basic HTTP request to the resource that I'm looking at because if I can make that http request work whether it's a get request or whatever the the the rest of my life building scraper building the scraper is so easy I just take the data I process it usually using AI or some very cheap",
        "importance": 0.2538143992424011
      },
      {
        "content": "I'm using something called their extract endpoint but just to make a long story short fire craw is a very simple but High uh bandwidth service that turns websites into large language model ready data and basically you know how earlier we had to do HTTP request",
        "importance": 0.23322933912277222
      },
      {
        "content": "and then we had to convert all that stuff into markdown",
        "importance": 0.14162960648536682
      },
      {
        "content": "and then we had to you know manipulate that markdown what this does is it just does a lot of that stuff for you",
        "importance": 0.17610567808151245
      },
      {
        "content": "and then it will automatically convert text into markdown for you um so that you can do whatever the heck you want they turn it into structure data using Ai and and so on and so on and so forth",
        "importance": 0.1913050264120102
      },
      {
        "content": "so if I were to do the same thing that I just did earlier with my own website then I were to you know run an example of this what it would go do is it would basically spin up a server for me and that would actually go and generate markdown of the same format um the only difference here is it's actually generated new lines between sections of text how beautiful um",
        "importance": 0.20253990590572357
      },
      {
        "content": "and then now you know we have basically the same thing you also get it in Json which is pretty cool um and you know you can slot this into any workflow this is basically like the simple and easy way of getting started um what we're going to be showing you today is the extract endpoint which allows you to extract data just using a natural language prompt which is pretty cool and from here we're going to be able to take any URL and just turn it into structure data but we're not actually going to have to know how to parse we're not going to have to know any code we're not going to have to know any of that stuff so let me actually run through the signup process with you guys go to fire.",
        "importance": 0.28142547607421875
      },
      {
        "content": "Dev here just going to open this up in an incognito to show you guys what this looks like all you do is you just go sign up I'm going to add a password we're then going to have to validate this one",
        "importance": 0.21945422887802124
      },
      {
        "content": "because I you know I'm doing this in an incognito tab normally when you do this you're not going to have that step",
        "importance": 0.19889554381370544
      },
      {
        "content": "so what I'm going to do is I'm going to go through and just like give give this extracting point just a basic natural language query",
        "importance": 0.1771712750196457
      },
      {
        "content": "I want to extract a oneline summary of the website let's do all of the text on the website all of the copy on the website in plain text let's do a oneline summary of the website a oneline Icebreaker I can use as the first line of of an of a cold email to the owner and uh the company name and a list of the services they provide let's do that this is a lot of requests we're asking it to do like seven or eight things",
        "importance": 0.21276533603668213
      },
      {
        "content": "but all I need to do in order to make this work is I click generate parameters it's going to basically now generate me a big object with a bunch of things so copy summary Icebreaker company name",
        "importance": 0.1737709641456604
      },
      {
        "content": "and I can run this",
        "importance": 0.138749361038208
      },
      {
        "content": "okay this is the URL it just parsed as well let's give it a run what it's doing now is it's scraping the pages using their high throughput server I just love this thing like I'm not sponsored by fire crawl or anything like that",
        "importance": 0.25245389342308044
      },
      {
        "content": "uh I don't know I just love the design I love this little like burning Ember or whatever the heck you want to call it I love how simple they've tried to make everything it's it's great",
        "importance": 0.09482628107070923
      },
      {
        "content": "oh okay links from the resource we have an icebreaker and then we have the company name as well so we can do a lot with this right",
        "importance": 0.18380562961101532
      },
      {
        "content": "but right now this is just um this is just on on a website how do we actually bring this in naden",
        "importance": 0.19564282894134521
      },
      {
        "content": "uh well it's pretty simple as you see there's an integrate Now button you can either get code or you can use it in zap here basically what we're going to want to do is we're going to want to run a request to um their endpoint",
        "importance": 0.22881974279880524
      },
      {
        "content": "and then we're going to want to turn that into basically our HTTP request let me show you what that looks like I'm just going to do all of this stuff in curl",
        "importance": 0.2674461007118225
      },
      {
        "content": "so if we go to curl as you can see what we need to do is we need to format a request that looks something like this",
        "importance": 0.22926105558872223
      },
      {
        "content": "but we need to make sure it's using the extract endpoint",
        "importance": 0.1968395859003067
      },
      {
        "content": "so I'm going to go down to extract",
        "importance": 0.17427675426006317
      },
      {
        "content": "and then now I have this big long beautiful string what I'm going to do is I'm going to copy this",
        "importance": 0.18904563784599304
      },
      {
        "content": "and then what I need to do is just open up an HTTP request module and then click import curl just paste all the stuff inside now this is an example request",
        "importance": 0.200449138879776
      },
      {
        "content": "dv1 extract so basically what we're doing now is we're like we're sending a request to fir craw which will then send a request to the website",
        "importance": 0.22539348900318146
      },
      {
        "content": "so kind of a kind of a middleman and then all I'm going to do so if I go back to my example we have an API key here which we're going to need so I'm going to go here and then paste in an API key so that's how that work works right authorization is going to be the name value is going to be bear with a capital b space",
        "importance": 0.17086653411388397
      },
      {
        "content": "so what I'm going to do is I'm going to delete most of these I'll go back to my left click.",
        "importance": 0.19894032180309296
      },
      {
        "content": "so we're going to go summary type string then Icebreaker it's going to be Icebreaker type string then guess what we have last but not least company name which is going to be type string we're also going to want to make these fields required like uh you know you can set it up so they're not actually required when you do a request",
        "importance": 0.1668839156627655
      },
      {
        "content": "logically maybe not all the websites we're going to be scraping using this service are going to have the company names visible on the website I don't know",
        "importance": 0.22857724130153656
      },
      {
        "content": "so now we have the API request formatted correctly um all we need to do at this point is just click test step it looks like we're getting a Json breaking um error",
        "importance": 0.18530511856079102
      },
      {
        "content": "and I'm just going to check to see if there are any commas in Jason you can't actually have the last element in an array have a comma on it",
        "importance": 0.06757350265979767
      },
      {
        "content": "and then we have a URL Trace array which is empty um if you think about this logically we don't actually get all the data that we send immediately because we need fir crawl to whip up the scraper you know do things to the data we could be feeding in 50 URLs here",
        "importance": 0.2338445633649826
      },
      {
        "content": "right so instead of just having the data available to us right now immediately what we need to do is we need to basically wait a little while wait until it's done and we need to Ping it and the reason why they've given us this ID parameter so that we could do the pinging so the way that you do this is you'd have to send a second HTTP request using this structure so the good news is we could just copy this over",
        "importance": 0.17911531031131744
      },
      {
        "content": "and then we can add a second um HTTP request I don't know where that went",
        "importance": 0.21980181336402893
      },
      {
        "content": "but this one doesn't so just going to go over here I'm going to copy this puppy go back over here I'm going to paste this in now technically what this is called is this is called polling um polling uh is where you know you're you're you're attempting to request a resource that you don't know whether or not is ready and there's a fair amount of logic that I'd recommend like putting into a polling flow where like when you try it and if it doesn't work basically you wait a certain amount of time and you retry again for the purpose of this video I'm not going to put all that stuff inside but um what I'm going to do is just set up this request I'm going to give this puppy a test let's just feed that in on the back end we got to put the extract ID right right over here where it said extract ID",
        "importance": 0.24896839261054993
      },
      {
        "content": "then I'm just going to give this a test uh looks like I've issued a malformed request we just have to make sure that everything here is okay specify body let me just make sure there's nothing else in here it was a get request this is a get cool",
        "importance": 0.25013870000839233
      },
      {
        "content": "and now we have all of the data available to us automate your business in the copy field summary field left clicks an ad performance optimization agency Icebreaker",
        "importance": 0.21573899686336517
      },
      {
        "content": "hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website uh and then company name left click so quick and easy way uh you're going to have access to this template obviously without my API key in it um and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is basically a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for um you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away um and then you just request uh to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful um data that you could use for basically anything so this is what rapid API looks like it's basically a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data like emails phone numbers and stuff like that from a website you could ping the ah refs SEO API you could find uh I don't know like unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work um just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just like a wrapper around what we're doing right now in nadn um but this website scraper API allows you to scrape some more Dynamic data um now I'm not signed up to this",
        "importance": 0.2482997626066208
      },
      {
        "content": "but yeah we're going to we're going to run through an API request to Rapid API which is going to make this a lot easier just going to put in all of my information here",
        "importance": 0.29782992601394653
      },
      {
        "content": "and then I'm going to do the classic email verification",
        "importance": 0.19491468369960785
      },
      {
        "content": "thank you rise I use a time management app called rise and every time I go on my Gmail I set my Gmail up as like a definitely do not uh do during your workday let's just call it personal projects they don't ask me all these questions my goal today is to browse available apis awesome",
        "importance": 0.18903133273124695
      },
      {
        "content": "uh I'm going to look for wherever it was earlier website scraper API and now check this out what we have is we have the app which is the name of the specific API that we're requesting we have an x-raid api-key and this is the API key we're going to use to make the request then we have the request URL which is basically what we're pinging and what we can do here is we can feed in the parameters",
        "importance": 0.2328415811061859
      },
      {
        "content": "and then we can actually just like give it give it a run",
        "importance": 0.11019933968782425
      },
      {
        "content": "and I'm going to pay money per month that probably seems the simplest way to do",
        "importance": 0.14455965161323547
      },
      {
        "content": "and I just ran through the payment let's actually head over here and let's just run a test using my website URL we're going to test this endpoint now and now this actually going to go through Rapid API it's going to spin up the server",
        "importance": 0.2064916491508484
      },
      {
        "content": "and then it's going to send it and what we see here is we have multiple fields that Rapid apis or this particular scraper gives us let me just make this easier for you all to see we have a text content field with all of the content of the website which is cool this is basically what I did earlier um but instead of me having to formulate this request try and parse it and try and use AI tokens what I did is I sent the request to uh rapid API and did it all for me then we also have an HTML content field I think we have one more here scroll all the way down to the bottom as you can see there is a ton of HTML",
        "importance": 0.28863582015037537
      },
      {
        "content": "um and then we also have a list of all of the images on the website which is very very cool and easily formatted again something that I tried to do manually using AI",
        "importance": 0.21543043851852417
      },
      {
        "content": "and then if they find any social media links I don't believe um there were more than Twitter",
        "importance": 0.10420998930931091
      },
      {
        "content": "and why don't we actually run through what this would look like if we were to run a curl request you see how it's automatically just formatting it as curl well that just means we just jump back here connect this to my HTTP request module click import curl paste it in like this import",
        "importance": 0.20710508525371552
      },
      {
        "content": "well I can actually just recreate this request now inside of NN as opposed to being on rapid API and then I have all the data accessible to me here how cool is that so we can do this for any any major website",
        "importance": 0.28279203176498413
      },
      {
        "content": "really um you know there are a lot of specific bespoke scrapers obviously which um I don't know if you wanted to scrape uh let's go back to Discovery if you wanted to scrape like Instagram or something you could scrape um Instagram uh you could do like Facebook scraping you could scrape these large giants that are quite difficult to do",
        "importance": 0.24614152312278748
      },
      {
        "content": "So Meta ad Library Facebook ad scraper and depending on the plan that you're at it might be more cost- effective for you to sign up to some sort of monthly recurring thing rather than just pay two cents every single time you make one of these requests you just kind of got to do that determination yourself",
        "importance": 0.20122095942497253
      },
      {
        "content": "right like if you're scraping uh I don't know 50 every day or 100 every day or something might be a dollar or two a day which is reasonable",
        "importance": 0.13686202466487885
      },
      {
        "content": "but maybe if you want to scrape like 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool um in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but um that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites",
        "importance": 0.2462385594844818
      },
      {
        "content": "and then you can um export that data as a cloud run to then send back sorry big sneeze to then send back to some API or some service um and then automatically do parsing and stuff like that so very cool I'm going to show you guys what that looks like um this is sort of a more customized way to build the stuff",
        "importance": 0.23855715990066528
      },
      {
        "content": "but I've seen a lot of people do this with naden",
        "importance": 0.0810120478272438
      },
      {
        "content": "um so we're going to run through what it looks like so first thing I'm going to want to do is I'm going to want to let's just go Cloud login or sorry um start free 7-Day trial as you can see you know there's a free browser extension here if you wanted to do uh I don't know like highs scale stuff you'd choose probably their project um endpoint where we Sorry project plan where we have 5,000 URL credits we can run a bunch of tasks in parallel we could scrape Dynamic sites JavaScript sites we have a bunch of different export options then we can also just connect it directly to all of these um what I'm going to do just because I want this to kind of work as a first go is I'm just going to sign up to a free Tri here beautiful just created my account just go left click give it a phone number we'll go left click.",
        "importance": 0.2566417157649994
      },
      {
        "content": "a we're going to go I don't know academic records needed per month we'll go 0 to th000 length of the project uh I don't know let's go two to 3 months",
        "importance": 0.11027133464813232
      },
      {
        "content": "so now we can import and run our own site map or we can use a premade Community sit map um what I'm going to do is I'm just going to import this we're then going to get the Chrome extension web scraper let me add that extension and it's going to download it do all that fun stuff beautiful",
        "importance": 0.24704788625240326
      },
      {
        "content": "a open up this puppy now there's a bunch of like tutorials and how to use this stuff um that's not that big of a deal but basically the thing you need is you need to hold command plus option plus I to open up your developer tools and you'll just find it on the in my case the far right",
        "importance": 0.15940484404563904
      },
      {
        "content": "um so we got what you're going to want to do first you're going to want to create a site map for the resource that you're going to want to scrape I'm just going to call it left click",
        "importance": 0.2451121062040329
      },
      {
        "content": "once we have our sitemap if I just give a quick little click I can then add a new selector and the really cool thing about this web scraper is um if I just zoom out a little bit here uh what you can do is you can you can select the elements on the website that you want scraped so for instance it's a very quick and easy way to do this if you think about it is like just to show you guys an example structure data is uh sort of like an e-commerce application let's say you have like the title of a product and you have like I don't know the the description of a product so on my website really quick and easy way to do this is let's just call this products and it's a type text what I'm going to do is I'm going to click select then I'll just click on this I'll click on this as well and as you see it'll automatically find all of the headings that I'm looking for so that's products we are going to then click data I'm going to click done selecting data preview as you can see it only selected one of them the very first",
        "importance": 0.18476736545562744
      },
      {
        "content": "so what we're going to want to do is go multiple",
        "importance": 0.14140912890434265
      },
      {
        "content": "I'll go multiple data preview just to make sure that it looks good I'm getting no data extracted here",
        "importance": 0.17482350766658783
      },
      {
        "content": "oh sorry I didn't actually select the um didn't actually finish it now we're getting product descriptions that's pretty cool um this is me doing this sort of like one at a time you can also um group",
        "importance": 0.1901884377002716
      },
      {
        "content": "okay great once we have this um what I can do is I can actually go export sitemap so now I have all of the code on the website that actually goes and finds it for me then I can paste this in here",
        "importance": 0.19155468046665192
      },
      {
        "content": "I'll just call this left click scraper and I'm going to import this to my cloud scraper uh I think I'm running into",
        "importance": 0.27955442667007446
      },
      {
        "content": "I don't think we can do a space there my bad just call it left click and now what we can do is we can actually just like run a server instance that goes out and then scrapes this for us",
        "importance": 0.25468796491622925
      },
      {
        "content": "so I'm going to click scrape it looks like I need to verify my email so just make sure you do that before you try and get ahead of yourself like I was okay looks like we just verified the email let's head back over here refresh then scrape",
        "importance": 0.23115280270576477
      },
      {
        "content": "okay I just gave this a refresh and as we see we have now finished said scraping job we have all of the data available to us using their UI but now that we've gone through this process of you know building out this this thing um how do we actually take that and then use it in our nadn flows so variety of ways um if you wanted to connect this let's say to specific service like Dropbox um Google you know dump anow or something Google Drive I'd recommend just doing it directly through their integration it's just a lot easier to get the data there",
        "importance": 0.2691945731639862
      },
      {
        "content": "and then you can just connect it to n and watch the data as it comes in or something you can also use the web scraper API uh this is pretty neat because you can you know that's what we're going to end up using it was pretty neat because you can uh like schedule jobs you can send jobs you can do basically everything just through the NN interface",
        "importance": 0.2527582049369812
      },
      {
        "content": "and then we can set like a web hook URL where we we receive the request so um let me check we need a scraping for testing you need a scraping job that has already been finished I think our scraping job has already been finished I'm just going to go htps uh back to my n8n flow I'm actually going to build an n8n web hook give that a click I'm not going to have any authentication let me just turn all this off basically what we want is we we want to use this as our test event we're going to go back to the API paste this in save",
        "importance": 0.2553354799747467
      },
      {
        "content": "so we basically have everything we need now to set up a flow where we can schedule something in this web scraper service that maybe monitors some I don't know list of e-commerce product or something every 12 hours",
        "importance": 0.23405103385448456
      },
      {
        "content": "and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping um we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks like so we just got the data from web hook let's set up an HTTP request to their API now where we basically get the ID of the thing",
        "importance": 0.22035901248455048
      },
      {
        "content": "so got my API token over here I'm going head over to their API documentation first",
        "importance": 0.2504524290561676
      },
      {
        "content": "and then what we want to do is download these scrape data in CSV format at least in my case I imagine most of you guys are going to add this to a spreadsheet or whatever um you can very easily do whatever you want there's also a Json format endpoint here",
        "importance": 0.23939038813114166
      },
      {
        "content": "um but let's just do CSV for Simplicity",
        "importance": 0.1830766499042511
      },
      {
        "content": "so I've already gone ahead and I've gotten the method which was a get request so I've added that up here the URL was this over here with the scraping job ID and then your API token there so what I've done is I've grabbed the API token and the scraping job ID",
        "importance": 0.1508023738861084
      },
      {
        "content": "I mean I hardcoded it in here just while I was doing the testing let's actually make this Dynamic now drag the scraping job ID right over here voila",
        "importance": 0.2078200876712799
      },
      {
        "content": "so just copy that over uh great and now if I run this I'm actually selecting that specific job then from here we have all the data that we just scraped as you can see there's like a uh the way that CSV Works actually let me just copy this over here I just wanted to give this to you guys as an example of a different data type",
        "importance": 0.1972634345293045
      },
      {
        "content": "but maybe some people here aren't really familiar with it basically the way that it works is if I just paste this into like a Google sheet you see how it looks like this what what you can do is if you just um split the text to columns you kind of see how kind of see how there's like these four pettings there's web scraper order web scraper startup products and product descriptions",
        "importance": 0.2109224945306778
      },
      {
        "content": "yeah you can you can put in like a number of formats and I just wanted to give you guys an example what that looks like the next way to scrape websites in naden is using appify if you guys are no strangers to this channel you know that I do appify all the time",
        "importance": 0.280901163816452
      },
      {
        "content": "so you probably get 30% off your first three months just check the um description if you want that",
        "importance": 0.09382498264312744
      },
      {
        "content": "but Cent how appify is is it is a Marketplace very similar to Rapid API um although extraordinarily well Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application",
        "importance": 0.23396845161914825
      },
      {
        "content": "so just as we had earlier we have Instagram scrapers we have Tik Tok scrapers we have email scrapers we have map scrapers Google Maps we could do I don't know Twitter scrapers uh medium scrapers right basically any any service out there that has this Dynamic aspect to it that's not a simple HTTP request you can make you could scrape it using ampify and then obviously you you have things too like just like basic website crawlers you can generate screenshots of sites I mean there's just there's so many things let me walk you guys through what it looks like now in my case I'm not actually going to sign up to appify because I have like 400 accounts but trust me when I say it is a very easy and simple process you go to app ay.com you go get started you put in your email and your password they'll give you $5 in free platform credit you don't need any credit card and you can just get up and running and start using this for yourself super easily then the second that you have all that you'll be Creed with this screen it is a console screen don't be concerned when you see this um you know this is super simple and and easy and and not a big deal this is one of my free accounts",
        "importance": 0.2625422179698944
      },
      {
        "content": "um so I just wanted to show you guys what you can do with a free account uh but from here what you do is you go to the store and as you can see I'm just dark mode all this is the same thing we were just looking at before",
        "importance": 0.17853979766368866
      },
      {
        "content": "and then um we're just going to run a test on the thing that we want to scrape",
        "importance": 0.23714615404605865
      },
      {
        "content": "so what I'm going to want to do is for the purposes of this I'm now going to do something different from what I was doing before like which was just left click over and over and",
        "importance": 0.2045401632785797
      },
      {
        "content": "over I think that kind of gets boring what I'm going to do is I'm going to scrape Instagram posts",
        "importance": 0.23173788189888
      },
      {
        "content": "so what I'm going to do is I'm going to feed in a name nickf this is just my um Instagram uh which almost hit 10K in God like 15 days or something like that",
        "importance": 0.1770632266998291
      },
      {
        "content": "and then I'm just going to grab like I don't know the last 10 posts okay save and start this is now going to run an actor actor is just their term for scraper which will go out it'll extract data from my Nick surve Instagram and as you can see will get a ton of fields caption owner full name owner Instagram URL comments count first comment likes count timestamp query tag we get everything from these guys which is really cool this might take you know 30 40 50 seconds we are spinning up a server in real time every time you do this as you see in bottom left hand corner there's a little memory tab which shows that we are legitimately running a server with one gigabyte of memory right now so generally my recommendation when you use appify is not to use it for oneoff requests like this feed in 5 to 10 15 20 Instagram Pages",
        "importance": 0.23386311531066895
      },
      {
        "content": "um so the question is obviously how do you get this in NN well appify has a really easy to use um API which I like doing all you have is if we wanted to get the uh let's see get data set items",
        "importance": 0.15461885929107666
      },
      {
        "content": "okay all I'm going to do is I'm just going to copy this go back here and then connect this to an HTTP request module as you could see we have this big long field here with my API appify API token",
        "importance": 0.1977359801530838
      },
      {
        "content": "but I just wanted to like allow you to see how to get data in naden really quickly now if we go to the schem of view we can see we legitimately we we already have all of the data that we we had from appify a second ago okay super easy and quick and simple to get up and running um we have the input URL field the ID field the type the short code caption now this is Instagram um every looks like we have some comments I don't have any style how do I create my man you just got to fake it till you make it",
        "importance": 0.24962207674980164
      },
      {
        "content": "I don't have any style either just some nerd in my mom's basement",
        "importance": 0.05200066789984703
      },
      {
        "content": "um obviously I was scraping an Instagram resource but like if you were scraping something else there'd be no change to this at all no change whatsoever now uh basically what we need in order to make this Dynamic basically make us able to run something in appify and then get it in NN so we need to set up an integration so just head over to this tab set up integration and then all you want to do is you just want to do web hook send an HTTP post web Hook when a specific actor event happens the actor event that we're going to want is basically when the run is succeeded the URL we're going to want to send this to if you think about it we just actually make another web hook request here web hook the URL we're going to want to send it to is going to be this test URL over here now I'm just going to delete all the header off stuff here because um it just uh complicates it especially for beginners",
        "importance": 0.2340211570262909
      },
      {
        "content": "um but we're going to copy this over head back over here paste in this URL and then let me see this is a post request I think I don't actually remember so we're going to have to double check I think it's a post request",
        "importance": 0.23663853108882904
      },
      {
        "content": "and then what I'm going to do is I'm going to listen for a test event run the test web hook so we're listening we're making a get request",
        "importance": 0.2207076996564865
      },
      {
        "content": "so the fact that it hasn't connected yet probably tells me it's a post request so let's move over here move this down to post now let's listen to a test event let's run this puppy one more time so we just dispatched it",
        "importance": 0.17448116838932037
      },
      {
        "content": "so I'm going to listen for this test event I'm going to run the same scraper again maybe we'll make it five posts per profile just to make it a little faster and um once this is done what it's going to do is it's going to send a record of all the information we need to get the data over to Ann we're going to catch that information",
        "importance": 0.2594025433063507
      },
      {
        "content": "and then we're going to use it to query the the the database basically that it created for that particular Instagram run which will then enable us to do whatever the heck we want with it",
        "importance": 0.219512939453125
      },
      {
        "content": "so it's now starting to crawl as we see here we had five requests so it should be able to do this in like the next 5 seconds",
        "importance": 0.27351024746894836
      },
      {
        "content": "and once that's done we now have an actor succeeded event um",
        "importance": 0.06557410955429077
      },
      {
        "content": "and then we have uh let me see the data that we want would be the default data set ID down over here",
        "importance": 0.21790827810764313
      },
      {
        "content": "so if we just go to that next HTTP request node what I can do is I can feed that in as a variable right here let going to a default data set ID drag that in between these two little lines and now we can test that step with actual live data now we have everything that we need so I don't know maybe now you want to feed this into Ai",
        "importance": 0.19408747553825378
      },
      {
        "content": "right you can now do super Dynamic and structured Outreach you could take that data and use it to like draft up your own post",
        "importance": 0.1928081512451172
      },
      {
        "content": "I mean the options are ultimately unlimited that's why I love appify so much the sixth way to scrape websites with NN is data for Co this is another thirdparty service but it's a very high quality one that's specifically geared towards search engine optimization requests you guys haven't seen data for SEO before it's basically this big API stack that allows you to do things like automatically query a service maybe some e-commerce website or some content website and then like extract things in nicely structured formatting um again specifically for SEO purposes tons of apis here as well I mean a lot of these services are now going towards like more Marketplace style stuff",
        "importance": 0.20567730069160461
      },
      {
        "content": "and then you could like feed that into one of any of the other scrapers that we set up here to get data on stuff you could go Google Images Google Maps you could do Bing BYO YouTube Google's uh their own data set feature I don't really know what that is",
        "importance": 0.23067739605903625
      },
      {
        "content": "uh and then you can you can take this data and do really fun stuff with it",
        "importance": 0.22690671682357788
      },
      {
        "content": "so I'm just going to click try for free over here in the top right hand corner show you guys what that looks like and as you see here um I signed in to data for SEO to my own account looks like I have 38 million bajillion dollars",
        "importance": 0.2357318252325058
      },
      {
        "content": "so why don't actually just do that with you",
        "importance": 0.14220553636550903
      },
      {
        "content": "and then I'll just use that account that is 38 million bajillion dollars we'll click try for free we'll go Nikki Wiki uh let's use a different email I need a business email",
        "importance": 0.18318232893943787
      },
      {
        "content": "okay I do agree to the terms of use absolutely uh bicycle is that a bicycle that's not a bicycle what does it mean when I can't answer these does it mean that I'm a robot if you look at some of my posts some of my comments people would absolutely say yes it means that that you're a robot um I don't know why people keep saying stuff like dude Nick nice AI Avatar bro",
        "importance": 0.0857735127210617
      },
      {
        "content": "so I need to activate my account doesn't look like it allows you to feed in the code here so I'm just going to feed it in myself",
        "importance": 0.2507554888725281
      },
      {
        "content": "uh it's obviously you're getting a lot of spammers hence this um bicycle stuff I don't know why the code isn't working here let me just copy this link address paste it in here instead there you go",
        "importance": 0.18075129389762878
      },
      {
        "content": "so now you can sign in and once you're in you got also um they're actually really big on on bicycles they're training um a model to convert all ads on planet Earth into bicycles they'll actually give you a dollar worth of API access uh credits which is pretty cool um I'm not going to do that I'm just going to go over to mine which is$ 38 million bajillion dollars with 99,999 estimated days to go um",
        "importance": 0.20028862357139587
      },
      {
        "content": "and yeah let's actually run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call um what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily",
        "importance": 0.24878953397274017
      },
      {
        "content": "um you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers um that do more like I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to like create your own very complex flows you can do other stuff as well um you grab a bunch of keyword data",
        "importance": 0.21538379788398743
      },
      {
        "content": "so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language um I'm just not going to select an language",
        "importance": 0.10117854923009872
      },
      {
        "content": "and then I'll do a request so now it's going to find us um a bunch of search volume related stuff",
        "importance": 0.22068294882774353
      },
      {
        "content": "so I don't actually know how many people are searching for me in 2025 apparently 390 is this per month H wonder if it's per month per day that's interesting uh I don't really know why they break it down by like the month date",
        "importance": 0.10780587047338486
      },
      {
        "content": "yeah looks like it's 390 per month so to the 390 people that are Googling me who are you and what do you want I'm just kidding um you can do things like you could find back links so you could find links um for I believe you feed in a website URL and then it finds back links to that website so this is you technically now scraping a bunch of other websites looking for links to the specific resource that you have that's kind of neat it looks like that found it basically immediately which is really really cool",
        "importance": 0.2413111925125122
      },
      {
        "content": "and it looks like they're referring top level links that are Dooms BG bgs would be interesting I wonder where that's coming from um there's a Content generation API playground",
        "importance": 0.18439927697181702
      },
      {
        "content": "but let's actually turn this into an API call if we head over to the API of do data for SEO so in my case docs.",
        "importance": 0.23585724830627441
      },
      {
        "content": "um well I have a curl just like this which I can feed into um an API request that's what I'm going to do",
        "importance": 0.27567392587661743
      },
      {
        "content": "AI um and then we have sort of like a gacha here that a lot of people don't understand this is the um authorization the authorization is a little bit different from most of the easy authorizations we've had so far we actually have to convert it",
        "importance": 0.17225311696529388
      },
      {
        "content": "um we have to go one one more step basically to make this work if I check out the let's see um authorization here what we need is we need to um get the login and then the P",
        "importance": 0.1317727118730545
      },
      {
        "content": "but we have to convert it into something called base 64 um this is just how they do their API key stuff I guess it's kind of annoying",
        "importance": 0.16596992313861847
      },
      {
        "content": "but it's just part and parcel of working with some apis you're just not always going to have it available to you really easily",
        "importance": 0.23032017052173615
      },
      {
        "content": "so what we need to do is we need to base 64 encode the username and the password um I'm just going to leave that at what I've done is I've actually gone through and done it in this edit Fields node um basically what you need to do is you need to have your username or your login",
        "importance": 0.09451727569103241
      },
      {
        "content": "and then once you feed it in over here where you're going to want to do is you're going to want to base 64 encode it like this they just require you to use these creds um or to operate with these creds as base 64 encoded versions Bas 64 is just a way to like translate into a slightly different number format so once you have that you would just feed in the variable right over here Ju",
        "importance": 0.09828568249940872
      },
      {
        "content": "Just as follows and then you can make a request to their API and receive data",
        "importance": 0.2428586781024933
      },
      {
        "content": "so uh it looks like I was doing their content parsing live you know what I wanted to do is I just wanted to call their endpoint which I think was their like instant Pages this one right over here",
        "importance": 0.27206555008888245
      },
      {
        "content": "you're literally doing is just swapping out the requests",
        "importance": 0.21790023148059845
      },
      {
        "content": "so you know if you wanted to do instant Pages all I'm doing is pumping that in there",
        "importance": 0.20413024723529816
      },
      {
        "content": "I'll use my business email here and then continue with Emil email we got to add a phone number obviously we're going to do less than a thousand",
        "importance": 0.1612958014011383
      },
      {
        "content": "I'm a CTO I don't want to what's the animal right is that an animal",
        "importance": 0.10244721919298172
      },
      {
        "content": "yes it's an animal good God beep boop uh we're going to head over to my Gmail and receive this now so we need to confirm my account just going to copy this link address that I can do this in one page",
        "importance": 0.19475224614143372
      },
      {
        "content": "we should be good to log in so that's what's happening we need to select the animal again just doesn't it doesn't believe really just doesn't believe okay",
        "importance": 0.14149440824985504
      },
      {
        "content": "so now we have a crawling API smart proxy thing if you guys want to run like uh I don't know use in apps that have a proxy field specifically I'm just going to keep things simple we're doing this in n8n",
        "importance": 0.21365804970264435
      },
      {
        "content": "so we're going to go crawl base API we have a th000 free crawls remaining very first thing we're going to want to do is just click start crawling now just to get up and running with the API um and as you see here the these guys have probably one of the simplest apis possible all API URLs start with the folling base part click",
        "importance": 0.234652578830719
      },
      {
        "content": "and then all you need to do in order to make an API call is run the following sort of line",
        "importance": 0.21065004169940948
      },
      {
        "content": "so well I'm going to import it as you can see here we have a token field then we just have the URL field of the place we want to crawl so I'm going to do left click.",
        "importance": 0.2527564764022827
      },
      {
        "content": "for now um I don't know if this token field was actually my real token",
        "importance": 0.16726616024971008
      },
      {
        "content": "I don't believe so maybe we'll give it a try",
        "importance": 0.13052427768707275
      },
      {
        "content": "so I'm now running this and it looks like we just received a bunch of very spooky data I don't like the spooky data no spooky data for us um sometimes spooky data like this H this seems kind of weird to me actually just give me one second to make sure that's right we are receiving a data parameter back which is nice",
        "importance": 0.1446417272090912
      },
      {
        "content": "no it looks like we um we got the data from from Amazon which is pretty cool if you feed that into the markdown converter like we had before it's going to feed in the HTML here pump it into a data key we've now converted this into uh this is very long let's go tabular",
        "importance": 0.21830424666404724
      },
      {
        "content": "we've now converted this into markdown which is cool and this is pretty long right obviously has all of the images and has all of the information on the site which is cool",
        "importance": 0.21574848890304565
      },
      {
        "content": "and then we're just going to feed in the code here and then because I didn't feed in this we should now run this we're going to grab data from the site",
        "importance": 0.29249244928359985
      },
      {
        "content": "and I mean you know we kind of all know what Amazon is and what it does right",
        "importance": 0.13966594636440277
      },
      {
        "content": "obviously it's just shortening and truncating it for us",
        "importance": 0.10361899435520172
      },
      {
        "content": "right um sorry just jumping around the place here you can send 72,000 requests basically an hour which is crazy um and you can do so as quickly and as easily as just like adding an API call like",
        "importance": 0.24495276808738708
      },
      {
        "content": "so if you made it to this part of the uh tutorial and you have yet to sign up to one of these Services give octoparse um give octoparse your thoughts let's double check that I haven't actually created an account using this",
        "importance": 0.20697356760501862
      },
      {
        "content": "um I don't know if I have enough credits to actually do anything",
        "importance": 0.13950830698013306
      },
      {
        "content": "but if I'm not then I'll start that trial in a second what you're going to have to do in order to make this work is you're going to want to have to download you're going to want to download the octoparse desktop app",
        "importance": 0.13803909718990326
      },
      {
        "content": "um if you are using something that is not Mac OS you will not have this strange drag and drop feature here once that is done you will have octo parse accessible just open that up",
        "importance": 0.11666375398635864
      },
      {
        "content": "yes I want to open this thank you and the cool thing about octoparse um kind of relative to what else you know like the other scraping applications I talked about is this is just running in a desktop app um like kind of in in your computer",
        "importance": 0.25690189003944397
      },
      {
        "content": "so like it's cool because it's just easy to get up and running with um",
        "importance": 0.08190924674272537
      },
      {
        "content": "and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and basically what'll happen is um it'll actually launch like an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm actually like scraping all of this stuff",
        "importance": 0.24839499592781067
      },
      {
        "content": "and then um you can also do things like click elements and so on and so forth extract the text Data here",
        "importance": 0.2123706191778183
      },
      {
        "content": "so as you see I'm now mapping each of these very similarly to how I was doing before between the first field which is the title of the product and then the second field which is like the field to uh so that's pretty sweet we could do the same thing with a number of things you could extract like the headings and then the values and so on and so on and so forth but I'll kind of leave it there um so once you're done selecting all the elements that you want all you do is you click run and you have a choice between running it on your device versus running it on the cloud so um on the cloud is API supported that's how you're going to get stuff in NM",
        "importance": 0.16251154243946075
      },
      {
        "content": "but I just want you guys to know that you can also just run it here you could run it here load up the URL scrape all the things that you want on the specific page you're feeding in",
        "importance": 0.28143423795700073
      },
      {
        "content": "and then you can be done with it",
        "importance": 0.16866259276866913
      },
      {
        "content": "so I just selected run in the cloud it's now going to open up said Cloud instances as we could see we have this little field where it's running and extracting the data we're now done so I can export this data locally",
        "importance": 0.13355790078639984
      },
      {
        "content": "um but I could also do a lot of other stuff which we'll show you in a second",
        "importance": 0.16882732510566711
      },
      {
        "content": "so um you can dump this automatically to Google Sheets you could do zapier to connect to Google Sheets do like some sort of web Hook connection export to cloud storage uh similar stuff to the the web scraping Chrome extension um but for now let's just export this as Json give ourselves a little ad Json file here thank you",
        "importance": 0.19277921319007874
      },
      {
        "content": "yeah now we have it locally now in order to connect with the octop par CPI what you're going to have to do is first you get up to request an access token the way that you do this is you send a post request to this URL here and the way that you format it is you need to send your username your password and then have the grantor type as password okay now password obviously just put in whatever your password is don't store it in PL text like I'm doing um with my hypothetical password put it somewhere else and then grab that data and then use it um but the the output of this is we have this big long access token variable which is great after that if I just go back to their API here um once we're here we can actually extract the data that we need so basically the thing that you're going to want is you're going to want um get data by Offset you can also use get non-exported data which is interesting so I think this just like dumps all of the data as not exported um and then sends that over to you",
        "importance": 0.22182679176330566
      },
      {
        "content": "octop course.com SL all and then I just send a header with the URL parameter this is a get request uh we're going to send a header with the token so authorization Bearer and then feed in the access token here just make sure that this is just one space",
        "importance": 0.18749076128005981
      },
      {
        "content": "no it's two if I feed this in um it's saying that it's a bad request let me just triple check why I think we need three Fields",
        "importance": 0.1843961626291275
      },
      {
        "content": "right obviously we need to feed in the task ID so you need task ID offset or size",
        "importance": 0.10321841388940811
      },
      {
        "content": "so um we'll feed this in as query parameters here so send query parameters the first value was task ID second one was offset and uh offset is no Capital the third was size offset's going to be zero size going to be I don't know let's just do 1,000 and what we need now is we need the task ID of the specific run that we just finished oh in order to get the task list you head over to task list top right hand corner here task ID API",
        "importance": 0.09391895681619644
      },
      {
        "content": "so we now have access to this so if we go back to our NN instance we could feed that in here by test the step you'll see that we now have all the data that we just asked for earlier so a variety of ways to do this um in practice like octop par allows you to schedule runs you could schedule them um using their you know whatever it is uh uh like cloud service you could use it to scrape I don't know Twitter",
        "importance": 0.23501937091350555
      },
      {
        "content": "uh they have a variety of like other scrapers um that you can check out just heading over to this new here uh if we just go sorry go down to templates um there's a variety of other ways to scrape Google job scraper glass door scraper super Pages scraper you could schedule these right",
        "importance": 0.23360399901866913
      },
      {
        "content": "and then what you can do in na is you can just query it once a day grab all the data like I showed you how to do a moment ago dump that into some sheet octoparse is pretty cool",
        "importance": 0.17495208978652954
      },
      {
        "content": "but I I like the idea that you can also just scrape locally which is pretty sweet and the last of our nine best ways to scrape websites in nadn is browserless now browserless runs a headless Chrome instance in the cloud this stuff is great for dynamic or heavy JavaScript websites if you've never used browser list before the cool part about browser list is allows you to actually bypass captas which is a big issue that a lot of people have um so I'm going to click try it for free I'm going to enter my email address over here verify I need to submit a code",
        "importance": 0.2510031461715698
      },
      {
        "content": "so let's head back over here thank you thank you thank you thank you we have a ton of free trial signups obviously I don't have a promo code or anything don't have a company name",
        "importance": 0.18795475363731384
      },
      {
        "content": "I'm just going to enter a password I'm using this to get past uh to avoid setting up a puppeteer and playright server sure I'm going to click complete we're now going to have a th credits inside of browser list which is pretty sweet um and we'll get a we'll get a full plan eventually we now have an API token",
        "importance": 0.25377973914146423
      },
      {
        "content": "so I can figure out how all of the stuff works here I'm just going to dive right into the API I can figure out how all of the API stuff works using their API docs which are fantastic by the way",
        "importance": 0.2291221022605896
      },
      {
        "content": "um and we don't want to do any of this stuff we just want to do HTTP apis brow list API index",
        "importance": 0.2521708905696869
      },
      {
        "content": "so here's where we're at um if you want to send and receive a request what you need to do is uh you send a request to one of these endpoints content unblock download function PDF screenshot scrape or performance what we want for the purpose of this is just uh let's do content",
        "importance": 0.22994160652160645
      },
      {
        "content": "a I'm going to run test step we are now quering the pi and in seconds we have access to the data same thing that we had before but now we're using a pass through and browser list is a great pass through um because you know uh they they allow you to scrape things that go far beyond the usual static site thing so like honestly",
        "importance": 0.2697588801383972
      },
      {
        "content": "and I'm just leaving this as a secret and sort of a little I guess Easter egg for people that have made it this far in the video like my go-to when scraping websites is as I mentioned do that HTTP request trans forg that works then do something like fir C.D",
        "importance": 0.27005907893180847
      },
      {
        "content": "but if that doesn't work I I do something like browserless that has all of this stuff built in um and I especially use browser list anytime that there's some sort of you know application where I'm just going to save this",
        "importance": 0.16771215200424194
      },
      {
        "content": "so I can make all my HTP requests really easy um especially when you know there's issues with captas and and accessing resources and stuff check this out not only can you do um the actual scrape you can do a screenshot of the page as well and because I've entered my token up here the requests that I'm going to setting up are as simple as importing the curl then clicking test step so straightforward we now have a file which is the screenshot now I used example domain there let's go left click.",
        "importance": 0.2343319207429886
      },
      {
        "content": "run this test now you can see we've actually like received a screenshot of the of the website view very sexy and my website's pretty long so keep in mind um and",
        "importance": 0.20506159961223602
      },
      {
        "content": "yeah you know obviously a lot you could do with that you can download the site you can turn the site into a PDF",
        "importance": 0.20537471771240234
      },
      {
        "content": "I don't think I've actually used this one before but for the purposes of this demonstration why don't we give it a try we'll go over here import the curl paste it in voila the website",
        "importance": 0.21851392090320587
      },
      {
        "content": "I'm going to do is left click.",
        "importance": 0.1829460710287094
      },
      {
        "content": "aai going to test this step so now there servers doing a couple things like I'm scraping the site then converting it all into PDF format um probably screenshotting a bunch of stuff too if I view this now we now have my my file looks like it didn't capture all of the color aspects um that might just be difficult or whatever",
        "importance": 0.21184323728084564
      },
      {
        "content": "I hope you guys appreciated the nine best ways to scrape websites in nadn as you guys could see it's a combination of on platform scraping using the HTTP request module a lot of like API documentation stuff like that if you want to get good at this I'm releasing a master class on API stuff um uh as part of my next na tutorial video uh",
        "importance": 0.24605019390583038
      },
      {
        "content": "and then you know navigating this and then and then taking the data from these services and using them to do something that you want to do like artificial intelligence to give you a summary of the site or generate ice breakers for you or do something else um whether you're using a local application like octop parse or maybe the web scraping CH uh Chrome extension or using something like firra browserless appify rapid API and so on and so forth um you now have everything that you need in order to scrape static sites Dynamic sites super Js heavy websites and even social media websites like Tik Tok Twitter and Instagram thanks so much for making it to this point in the video if you have any suggestions for future content drop them down below more than happy to take your idea and run with it assuming it's something that I haven't done before and then if you guys could do me a really big solid like subscribe do all that fun YouTube stuff",
        "importance": 0.23626679182052612
      }
    ],
    "problems": [],
    "comparisons": [
      {
        "content": "so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language um I'm just not going to select an language",
        "context": "um you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers um that do more like I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to like create your own very complex flows you can do other stuff as well um you grab a bunch of keyword data"
      }
    ]
  },
  "roles": {
    "user": [
      {
        "content": "so let me give you guys a website that I'm going to be scraping here this is my own site it's called left click I'm about to do a redesign",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so if I were to zoom in over here you see where it says I don't know let's let's go to my website let's just find a little bit of little bit of texture build hands off growth systems",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "so I'll go down to open Ai",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection um and grab your API key and paste it in there's some instructions that allow you to do so right over here uh what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the uh I want to say most cost effective one as of the time of this recording",
        "matched_patterns": [
          "open",
          "go to"
        ]
      },
      {
        "content": "and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B uh SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't actually do anything for us",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "right uh what we what we want is we want this instead we want left click aka the root of the domain and then um left click _ logogram and that's how we get to the actual file asset",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so this is our example I'm going to say your website URL is left click URL for the relative to Absolute conversions is left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and then the final thing is I'm going to add one more user prompt I'm just going to draw drag all of that markdown data in here then I'm going to click output content as Json I'm going to test the step I'm going to take a sip of my coffee while this puppy processes and we now have our output on the right hand side if we go to schema view what you can see is we've now generated basically an array of links on the rightand side which contains every link on this website very cool looks like the vast majority of these are type form links for some reason don't really know what's about that",
        "matched_patterns": [
          "click",
          "go to",
          "type"
        ]
      },
      {
        "content": "and then now you know we have basically the same thing you also get it in Json which is pretty cool um and you know you can slot this into any workflow this is basically like the simple and easy way of getting started um what we're going to be showing you today is the extract endpoint which allows you to extract data just using a natural language prompt which is pretty cool and from here we're going to be able to take any URL and just turn it into structure data but we're not actually going to have to know how to parse we're not going to have to know any code we're not going to have to know any of that stuff so let me actually run through the signup process with you guys go to fire.",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "Dev here just going to open this up in an incognito to show you guys what this looks like all you do is you just go sign up I'm going to add a password we're then going to have to validate this one",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "so um let's go from the homepage at left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "but all I need to do in order to make this work is I click generate parameters it's going to basically now generate me a big object with a bunch of things so copy summary Icebreaker company name",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so if we go to curl as you can see what we need to do is we need to format a request that looks something like this",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "and then what I need to do is just open up an HTTP request module and then click import curl just paste all the stuff inside now this is an example request",
        "matched_patterns": [
          "click",
          "open"
        ]
      },
      {
        "content": "so what I'm going to do is I'm going to delete most of these I'll go back to my left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so I'm going to go Copy Type string then summary",
        "matched_patterns": [
          "type"
        ]
      },
      {
        "content": "so we're going to go summary type string then Icebreaker it's going to be Icebreaker type string then guess what we have last but not least company name which is going to be type string we're also going to want to make these fields required like uh you know you can set it up so they're not actually required when you do a request",
        "matched_patterns": [
          "type"
        ]
      },
      {
        "content": "so now we have the API request formatted correctly um all we need to do at this point is just click test step it looks like we're getting a Json breaking um error",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and now we have all of the data available to us automate your business in the copy field summary field left clicks an ad performance optimization agency Icebreaker",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website uh and then company name left click so quick and easy way uh you're going to have access to this template obviously without my API key in it um and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is basically a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for um you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away um and then you just request uh to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful um data that you could use for basically anything so this is what rapid API looks like it's basically a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data like emails phone numbers and stuff like that from a website you could ping the ah refs SEO API you could find uh I don't know like unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work um just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just like a wrapper around what we're doing right now in nadn um but this website scraper API allows you to scrape some more Dynamic data um now I'm not signed up to this",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so I'm going to type website in here",
        "matched_patterns": [
          "type"
        ]
      },
      {
        "content": "uh I'm just going to go to the um basic plan",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "and why don't we actually run through what this would look like if we were to run a curl request you see how it's automatically just formatting it as curl well that just means we just jump back here connect this to my HTTP request module click import curl paste it in like this import",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and it's actually going to go through and it's going to automatically map all these fields for me right query parameter URL left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "but maybe if you want to scrape like 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool um in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but um that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites",
        "matched_patterns": [
          "select"
        ]
      },
      {
        "content": "um so we're going to run through what it looks like so first thing I'm going to want to do is I'm going to want to let's just go Cloud login or sorry um start free 7-Day trial as you can see you know there's a free browser extension here if you wanted to do uh I don't know like highs scale stuff you'd choose probably their project um endpoint where we Sorry project plan where we have 5,000 URL credits we can run a bunch of tasks in parallel we could scrape Dynamic sites JavaScript sites we have a bunch of different export options then we can also just connect it directly to all of these um what I'm going to do just because I want this to kind of work as a first go is I'm just going to sign up to a free Tri here beautiful just created my account just go left click give it a phone number we'll go left click.",
        "matched_patterns": [
          "click",
          "choose"
        ]
      },
      {
        "content": "so now we have it right over here I'm just going to pin it to my browser to make my life easier go to left click.",
        "matched_patterns": [
          "click",
          "go to"
        ]
      },
      {
        "content": "a open up this puppy now there's a bunch of like tutorials and how to use this stuff um that's not that big of a deal but basically the thing you need is you need to hold command plus option plus I to open up your developer tools and you'll just find it on the in my case the far right",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "I that'll open up Dev tools you see all the way on the right hand side here I have a couple other things like make and and cookie editor but all the way on the right hand side here we have this web scraper thing",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "um so we got what you're going to want to do first you're going to want to create a site map for the resource that you're going to want to scrape I'm just going to call it left click",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and I just want to scrape left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "once we have our sitemap if I just give a quick little click I can then add a new selector and the really cool thing about this web scraper is um if I just zoom out a little bit here uh what you can do is you can you can select the elements on the website that you want scraped so for instance it's a very quick and easy way to do this if you think about it is like just to show you guys an example structure data is uh sort of like an e-commerce application let's say you have like the title of a product and you have like I don't know the the description of a product so on my website really quick and easy way to do this is let's just call this products and it's a type text what I'm going to do is I'm going to click select then I'll just click on this I'll click on this as well and as you see it'll automatically find all of the headings that I'm looking for so that's products we are going to then click data I'm going to click done selecting data preview as you can see it only selected one of them the very first",
        "matched_patterns": [
          "click",
          "select",
          "type"
        ]
      },
      {
        "content": "so now we have a basically like a list of headings um from here I'm going to save this selector I'm add a new one let's go product descriptions and then going to select this this it'll select all of them",
        "matched_patterns": [
          "select",
          "save"
        ]
      },
      {
        "content": "oh sorry I didn't actually select the um didn't actually finish it now we're getting product descriptions that's pretty cool um this is me doing this sort of like one at a time you can also um group",
        "matched_patterns": [
          "select"
        ]
      },
      {
        "content": "The selectors there you go it's actually um offered to group it for me so we can uh group this into one object with products and then product descriptions",
        "matched_patterns": [
          "select"
        ]
      },
      {
        "content": "it now we have wrapper for products and products descriptions then we have products and product descriptions buried underneath we could go as far as we want with this but basically what I'm what I'm trying to show you guys is very simple and easy just drag your mouse over the specific thing you want if you select more than one it'll automatically find all of them on the website which is really cool",
        "matched_patterns": [
          "select"
        ]
      },
      {
        "content": "I'll just call this left click scraper and I'm going to import this to my cloud scraper uh I think I'm running into",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "I don't think we can do a space there my bad just call it left click and now what we can do is we can actually just like run a server instance that goes out and then scrapes this for us",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so I'm going to click scrape it looks like I need to verify my email so just make sure you do that before you try and get ahead of yourself like I was okay looks like we just verified the email let's head back over here refresh then scrape",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and then we can set like a web hook URL where we we receive the request so um let me check we need a scraping for testing you need a scraping job that has already been finished I think our scraping job has already been finished I'm just going to go htps uh back to my n8n flow I'm actually going to build an n8n web hook give that a click I'm not going to have any authentication let me just turn all this off basically what we want is we we want to use this as our test event we're going to go back to the API paste this in save",
        "matched_patterns": [
          "click",
          "save"
        ]
      },
      {
        "content": "so just copy that over uh great and now if I run this I'm actually selecting that specific job then from here we have all the data that we just scraped as you can see there's like a uh the way that CSV Works actually let me just copy this over here I just wanted to give this to you guys as an example of a different data type",
        "matched_patterns": [
          "select",
          "type"
        ]
      },
      {
        "content": "so just as we had earlier we have Instagram scrapers we have Tik Tok scrapers we have email scrapers we have map scrapers Google Maps we could do I don't know Twitter scrapers uh medium scrapers right basically any any service out there that has this Dynamic aspect to it that's not a simple HTTP request you can make you could scrape it using ampify and then obviously you you have things too like just like basic website crawlers you can generate screenshots of sites I mean there's just there's so many things let me walk you guys through what it looks like now in my case I'm not actually going to sign up to appify because I have like 400 accounts but trust me when I say it is a very easy and simple process you go to app ay.com you go get started you put in your email and your password they'll give you $5 in free platform credit you don't need any credit card and you can just get up and running and start using this for yourself super easily then the second that you have all that you'll be Creed with this screen it is a console screen don't be concerned when you see this um you know this is super simple and and easy and and not a big deal this is one of my free accounts",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "um so I just wanted to show you guys what you can do with a free account uh but from here what you do is you go to the store and as you can see I'm just dark mode all this is the same thing we were just looking at before",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "so what I'm going to want to do is for the purposes of this I'm now going to do something different from what I was doing before like which was just left click over and over and",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and then I'm just going to grab like I don't know the last 10 posts okay save and start this is now going to run an actor actor is just their term for scraper which will go out it'll extract data from my Nick surve Instagram and as you can see will get a ton of fields caption owner full name owner Instagram URL comments count first comment likes count timestamp query tag we get everything from these guys which is really cool this might take you know 30 40 50 seconds we are spinning up a server in real time every time you do this as you see in bottom left hand corner there's a little memory tab which shows that we are legitimately running a server with one gigabyte of memory right now so generally my recommendation when you use appify is not to use it for oneoff requests like this feed in 5 to 10 15 20 Instagram Pages",
        "matched_patterns": [
          "save"
        ]
      },
      {
        "content": "but I just wanted to like allow you to see how to get data in naden really quickly now if we go to the schem of view we can see we legitimately we we already have all of the data that we we had from appify a second ago okay super easy and quick and simple to get up and running um we have the input URL field the ID field the type the short code caption now this is Instagram um every looks like we have some comments I don't have any style how do I create my man you just got to fake it till you make it",
        "matched_patterns": [
          "go to",
          "type",
          "input"
        ]
      },
      {
        "content": "so if we just go to that next HTTP request node what I can do is I can feed that in as a variable right here let going to a default data set ID drag that in between these two little lines and now we can test that step with actual live data now we have everything that we need so I don't know maybe now you want to feed this into Ai",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "so I'm just going to click try for free over here in the top right hand corner show you guys what that looks like and as you see here um I signed in to data for SEO to my own account looks like I have 38 million bajillion dollars",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and then I'll just use that account that is 38 million bajillion dollars we'll click try for free we'll go Nikki Wiki uh let's use a different email I need a business email",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "and yeah let's actually run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call um what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily",
        "matched_patterns": [
          "go to"
        ]
      },
      {
        "content": "so maybe you wanted to find a keyword and maybe again it's Nicks or location you want let's do United States that'll probably be better language um I'm just not going to select an language",
        "matched_patterns": [
          "select"
        ]
      },
      {
        "content": "and it's basically going to um parse out all these fields that I'm interested in with the URL which I'll go htps left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so maybe this is me searching Nix or have Reddit uh Nick left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "we should be good to log in so that's what's happening we need to select the animal again just doesn't it doesn't believe really just doesn't believe okay",
        "matched_patterns": [
          "select"
        ]
      },
      {
        "content": "so we're going to go crawl base API we have a th000 free crawls remaining very first thing we're going to want to do is just click start crawling now just to get up and running with the API um and as you see here the these guys have probably one of the simplest apis possible all API URLs start with the folling base part click",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so well I'm going to import it as you can see here we have a token field then we just have the URL field of the place we want to crawl so I'm going to do left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "okay anyway they give you two types of tokens here um this is why I'm talking about it to begin with I'm also because I just used it before for a couple of applications",
        "matched_patterns": [
          "type"
        ]
      },
      {
        "content": "and then we can feed it into open AI like I did before where I message a model",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "um if you are using something that is not Mac OS you will not have this strange drag and drop feature here once that is done you will have octo parse accessible just open that up",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "yes I want to open this thank you and the cool thing about octoparse um kind of relative to what else you know like the other scraping applications I talked about is this is just running in a desktop app um like kind of in in your computer",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and basically what'll happen is um it'll actually launch like an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm actually like scraping all of this stuff",
        "matched_patterns": [
          "click",
          "select"
        ]
      },
      {
        "content": "okay now we have access to this sort of drag and drop or um selector thing similar to what we had before if you click on one of these you'll see it allow you to select all similar Elements which is pretty sweet",
        "matched_patterns": [
          "click",
          "select"
        ]
      },
      {
        "content": "and then um you can also do things like click elements and so on and so forth extract the text Data here",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "so as you see I'm now mapping each of these very similarly to how I was doing before between the first field which is the title of the product and then the second field which is like the field to uh so that's pretty sweet we could do the same thing with a number of things you could extract like the headings and then the values and so on and so on and so forth but I'll kind of leave it there um so once you're done selecting all the elements that you want all you do is you click run and you have a choice between running it on your device versus running it on the cloud so um on the cloud is API supported that's how you're going to get stuff in NM",
        "matched_patterns": [
          "click",
          "select"
        ]
      },
      {
        "content": "so I just selected run in the cloud it's now going to open up said Cloud instances as we could see we have this little field where it's running and extracting the data we're now done so I can export this data locally",
        "matched_patterns": [
          "select",
          "open"
        ]
      },
      {
        "content": "yeah now we have it locally now in order to connect with the octop par CPI what you're going to have to do is first you get up to request an access token the way that you do this is you send a post request to this URL here and the way that you format it is you need to send your username your password and then have the grantor type as password okay now password obviously just put in whatever your password is don't store it in PL text like I'm doing um with my hypothetical password put it somewhere else and then grab that data and then use it um but the the output of this is we have this big long access token variable which is great after that if I just go back to their API here um once we're here we can actually extract the data that we need so basically the thing that you're going to want is you're going to want um get data by Offset you can also use get non-exported data which is interesting so I think this just like dumps all of the data as not exported um and then sends that over to you",
        "matched_patterns": [
          "type"
        ]
      },
      {
        "content": "but anyway you could also get the data by offset so if I go a get request to open api.",
        "matched_patterns": [
          "open"
        ]
      },
      {
        "content": "it's like more of like an industrial Enterprise level application um to be honest so there might be some gotas if you're not super familiar with working with like desktop apps and stuff",
        "matched_patterns": [
          "enter"
        ]
      },
      {
        "content": "but I I like the idea that you can also just scrape locally which is pretty sweet and the last of our nine best ways to scrape websites in nadn is browserless now browserless runs a headless Chrome instance in the cloud this stuff is great for dynamic or heavy JavaScript websites if you've never used browser list before the cool part about browser list is allows you to actually bypass captas which is a big issue that a lot of people have um so I'm going to click try it for free I'm going to enter my email address over here verify I need to submit a code",
        "matched_patterns": [
          "click",
          "enter",
          "submit"
        ]
      },
      {
        "content": "I'm just going to enter a password I'm using this to get past uh to avoid setting up a puppeteer and playright server sure I'm going to click complete we're now going to have a th credits inside of browser list which is pretty sweet um and we'll get a we'll get a full plan eventually we now have an API token",
        "matched_patterns": [
          "click",
          "enter"
        ]
      },
      {
        "content": "and where it says your API token here I'm going to feed that in what I want as a website is just left click.",
        "matched_patterns": [
          "click"
        ]
      },
      {
        "content": "but if that doesn't work I I do something like browserless that has all of this stuff built in um and I especially use browser list anytime that there's some sort of you know application where I'm just going to save this",
        "matched_patterns": [
          "save"
        ]
      },
      {
        "content": "so I can make all my HTP requests really easy um especially when you know there's issues with captas and and accessing resources and stuff check this out not only can you do um the actual scrape you can do a screenshot of the page as well and because I've entered my token up here the requests that I'm going to setting up are as simple as importing the curl then clicking test step so straightforward we now have a file which is the screenshot now I used example domain there let's go left click.",
        "matched_patterns": [
          "click",
          "enter"
        ]
      },
      {
        "content": "I'm going to do is left click.",
        "matched_patterns": [
          "click"
        ]
      }
    ],
    "developer": [
      {
        "content": "I scaled my automation agency to 72k a month using no code tools like make and nadn and scraping was a big part of that",
        "matched_patterns": [
          "code",
          "api"
        ]
      },
      {
        "content": "all right I'm going to jump into NN in a minute and actually build these alongside you and one other thing I'm going to do is I'm actually going to sign up to all the services in front of you walk you through the Authentication and the onboarding flows and get your API keys and stuff like that but just before I do want to explain very quickly the difference between a static site and then a dynamic site because if you don't know this um scraping just gets a lot harder and so we're just going to cover this in like 30 seconds and we can move on so basically um if this is you",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so um there's actually that intermediate step okay where basically you are pinging some sort of uh you know domain name or whatever then that domain name shoots some code over forces a server to generate all of the contents on that domain and then you get it this is obviously kind of a two-step process and then this is a three-step process",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "so if you just understand that um you know when you scrape a dynamic resource what you're really doing is you're sending a request to a page which sends a request back to another server which then fills your thing this element eliminates 99% of the confusion because most of the time like scraping issues are hey",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so hopefully we at least understand that there's that difference between static and dynamic sites here um I'm not going to go into it more than that we're actually just going to dive in with both feet start doing a little bit of scraping and then we'll kind of see where we land I find the best way to do this stuff is just by example",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and and you know being practical about it so the first major way to scrape websites in NN is using direct h HTTP requests this is also what I like to think of as the Magic in scraping itself what we're going to do is we're going to use a node called the HTTP request node to send a get request to the website we want this is going to work with static websites and non JavaScript resources",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so let me give you guys a website that I'm going to be scraping here this is my own site it's called left click I'm about to do a redesign",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "I I did it in code and basically all this is is just a document somewhere on my or on on a server some more so what I'm going to want to do is",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "so uh we're just going to go HTTP request HTTP request node looks like this we have a method field a URL field authentication field query parameters headers body and then some options down here as well all",
        "matched_patterns": [
          "method",
          "parameter"
        ]
      },
      {
        "content": "okay then I'm just going to test the step it's that easy now the response from this on the right hand side see all this code over here this is what's called HTML if you're unfamiliar and",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "HTML is basically just the like it's it's the code behind the site",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "right so all that this HTML is is this is the code that is sent to my browser which is Google Chrome in this case then my browser takes the code and it just renders it into this beautiful looking thing well beautiful is a subjective State I would say but this uh wonderful looking thing in front of us which is this website with like sizing and the tabs and the divs and all that fun stuff",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "so basically what I'm trying to say is everything over here on the right hand side this is the entire site we can do anything we want with this information um and we can carry this information forward to to do any one of our any one of many flows so in my case right looking at a bunch of code isn't really very pretty so one big thing that you'll find in the vast majority of modern um scraping applications is you'll find that they'll take that HTML which we saw earlier and they'll convert it to something called markdown",
        "matched_patterns": [
          "code",
          "api"
        ]
      },
      {
        "content": "and then what I'm going to do is I'll do the message a model just have to connect my credential here I'm assuming that you've already connected a credential if not you're going to have to go to opena website when you do the connection um and grab your API key and paste it in there's some instructions that allow you to do so right over here uh what I'm going to do is I'm going to grab the G PT 40 Mini model that's just the uh I want to say most cost effective one as of the time of this recording",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then what I'm going to do is I'm going to add three prompt I'm going to add a system prompt first I'll say you are a helpful intelligent web scraping assistant",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then I'm going to give it an example of what I want in what's called Json JavaScript object notation format so the very first thing I'm going to do is I'm going to have it just pull out all the links on the website because I find that that's a very common scraping application so I go links",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then I'm just going to show an example of un array of we'll go absolute URLs this is very important that they're absolute URLs any thing that we're going to build after this is going to be making use of the absolute URLs not the relative URLs if you're unfamiliar with what that means if we Zoom way in here you see how there's this B uh SL left click log.png this is what's called a relative URL if you were to copy this and paste this into here this wouldn't actually do anything for us",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "right then we have a oneline summary of the site so this is a very simple example of scraping we're scraping a static resource obviously but when I build scrapers for clients or for my own business this is always my first pass I will always just make a basic HTTP request to the resource that I'm looking at because if I can make that http request work whether it's a get request or whatever the the the rest of my life building scraper building the scraper is so easy I just take the data I process it usually using AI or some very cheap",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then now you know we have basically the same thing you also get it in Json which is pretty cool um and you know you can slot this into any workflow this is basically like the simple and easy way of getting started um what we're going to be showing you today is the extract endpoint which allows you to extract data just using a natural language prompt which is pretty cool and from here we're going to be able to take any URL and just turn it into structure data but we're not actually going to have to know how to parse we're not going to have to know any code we're not going to have to know any of that stuff so let me actually run through the signup process with you guys go to fire.",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "but all I need to do in order to make this work is I click generate parameters it's going to basically now generate me a big object with a bunch of things so copy summary Icebreaker company name",
        "matched_patterns": [
          "parameter"
        ]
      },
      {
        "content": "okay this is the URL it just parsed as well let's give it a run what it's doing now is it's scraping the pages using their high throughput server I just love this thing like I'm not sponsored by fire crawl or anything like that",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "uh well it's pretty simple as you see there's an integrate Now button you can either get code or you can use it in zap here basically what we're going to want to do is we're going to want to run a request to um their endpoint",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "and then what I need to do is just open up an HTTP request module and then click import curl just paste all the stuff inside now this is an example request",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "so we're sending a bunch of headers this is the endpoint that we're calling api. fire.",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so kind of a kind of a middleman and then all I'm going to do so if I go back to my example we have an API key here which we're going to need so I'm going to go here and then paste in an API key so that's how that work works right authorization is going to be the name value is going to be bear with a capital b space",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then the API key",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "logically maybe not all the websites we're going to be scraping using this service are going to have the company names visible on the website I don't know",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so now we have the API request formatted correctly um all we need to do at this point is just click test step it looks like we're getting a Json breaking um error",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "right so instead of just having the data available to us right now immediately what we need to do is we need to basically wait a little while wait until it's done and we need to Ping it and the reason why they've given us this ID parameter so that we could do the pinging so the way that you do this is you'd have to send a second HTTP request using this structure so the good news is we could just copy this over",
        "matched_patterns": [
          "parameter"
        ]
      },
      {
        "content": "but I guess I'm just going to create it over here I'm going to import the curl to this request just like that then keep in mind that we just need to add our API key again because the previous node had it",
        "matched_patterns": [
          "api",
          "import"
        ]
      },
      {
        "content": "hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website uh and then company name left click so quick and easy way uh you're going to have access to this template obviously without my API key in it um and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is basically a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for um you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away um and then you just request uh to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful um data that you could use for basically anything so this is what rapid API looks like it's basically a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data like emails phone numbers and stuff like that from a website you could ping the ah refs SEO API you could find uh I don't know like unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work um just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just like a wrapper around what we're doing right now in nadn um but this website scraper API allows you to scrape some more Dynamic data um now I'm not signed up to this",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but yeah we're going to we're going to run through an API request to Rapid API which is going to make this a lot easier just going to put in all of my information here",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then I'm going to do the classic email verification",
        "matched_patterns": [
          "class"
        ]
      },
      {
        "content": "thank you rise I use a time management app called rise and every time I go on my Gmail I set my Gmail up as like a definitely do not uh do during your workday let's just call it personal projects they don't ask me all these questions my goal today is to browse available apis awesome",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "uh I'm going to look for wherever it was earlier website scraper API and now check this out what we have is we have the app which is the name of the specific API that we're requesting we have an x-raid api-key and this is the API key we're going to use to make the request then we have the request URL which is basically what we're pinging and what we can do here is we can feed in the parameters",
        "matched_patterns": [
          "api",
          "parameter"
        ]
      },
      {
        "content": "and I just ran through the payment let's actually head over here and let's just run a test using my website URL we're going to test this endpoint now and now this actually going to go through Rapid API it's going to spin up the server",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then it's going to send it and what we see here is we have multiple fields that Rapid apis or this particular scraper gives us let me just make this easier for you all to see we have a text content field with all of the content of the website which is cool this is basically what I did earlier um but instead of me having to formulate this request try and parse it and try and use AI tokens what I did is I sent the request to uh rapid API and did it all for me then we also have an HTML content field I think we have one more here scroll all the way down to the bottom as you can see there is a ton of HTML",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but um if they find anything that's at their Twitter Instagram whatever then we have the link right over here it looks like they even give you the scraping time and if they scrape emails or phone numbers um they'll be there as well",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so I mean rapid AP is obviously fantastic this is a high throughput sort of thing",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and why don't we actually run through what this would look like if we were to run a curl request you see how it's automatically just formatting it as curl well that just means we just jump back here connect this to my HTTP request module click import curl paste it in like this import",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "and it's actually going to go through and it's going to automatically map all these fields for me right query parameter URL left click.",
        "matched_patterns": [
          "parameter"
        ]
      },
      {
        "content": "beautiful um API key x-raid API host here's the host here's the name of the API key here's everything we need",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "well I can actually just recreate this request now inside of NN as opposed to being on rapid API and then I have all the data accessible to me here how cool is that so we can do this for any any major website",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "really um you know there are a lot of specific bespoke scrapers obviously which um I don't know if you wanted to scrape uh let's go back to Discovery if you wanted to scrape like Instagram or something you could scrape um Instagram uh you could do like Facebook scraping you could scrape these large giants that are quite difficult to do",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "right like if you're scraping uh I don't know 50 every day or 100 every day or something might be a dollar or two a day which is reasonable",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but maybe if you want to scrape like 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool um in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but um that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites",
        "matched_patterns": [
          "code",
          "install"
        ]
      },
      {
        "content": "and then you can um export that data as a cloud run to then send back sorry big sneeze to then send back to some API or some service um and then automatically do parsing and stuff like that so very cool I'm going to show you guys what that looks like um this is sort of a more customized way to build the stuff",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so now we can import and run our own site map or we can use a premade Community sit map um what I'm going to do is I'm just going to import this we're then going to get the Chrome extension web scraper let me add that extension and it's going to download it do all that fun stuff beautiful",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "okay great once we have this um what I can do is I can actually go export sitemap so now I have all of the code on the website that actually goes and finds it for me then I can paste this in here",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "I'll just call this left click scraper and I'm going to import this to my cloud scraper uh I think I'm running into",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "we've now scheduled a scraping job for this sitemap scheduling you know in their lingo just means that it's now part of their big long queue of thousands of other things that they're probably scraping through their server and that's fine",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "okay I just gave this a refresh and as we see we have now finished said scraping job we have all of the data available to us using their UI but now that we've gone through this process of you know building out this this thing um how do we actually take that and then use it in our nadn flows so variety of ways um if you wanted to connect this let's say to specific service like Dropbox um Google you know dump anow or something Google Drive I'd recommend just doing it directly through their integration it's just a lot easier to get the data there",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then you can just connect it to n and watch the data as it comes in or something you can also use the web scraper API uh this is pretty neat because you can you know that's what we're going to end up using it was pretty neat because you can uh like schedule jobs you can send jobs you can do basically everything just through the NN interface",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then we can just retrieve the data afterwards which is pretty neat um this is basically what you end up getting you end up with scraping job ID status sitemap all this fun stuff",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then we can set like a web hook URL where we we receive the request so um let me check we need a scraping for testing you need a scraping job that has already been finished I think our scraping job has already been finished I'm just going to go htps uh back to my n8n flow I'm actually going to build an n8n web hook give that a click I'm not going to have any authentication let me just turn all this off basically what we want is we we want to use this as our test event we're going to go back to the API paste this in save",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so I'm going to stop listening change your HTTP HTTP method here to post there's basically two ways to call a website and this is one of them I'm going to listen for test events go back here and then retest awesome looks like we've now triggered the beginning of our workflow using this data let's see what sort of information was in it",
        "matched_patterns": [
          "method"
        ]
      },
      {
        "content": "we have the scraping job ID the status execution mode",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping um we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks like so we just got the data from web hook let's set up an HTTP request to their API now where we basically get the ID of the thing",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so got my API token over here I'm going head over to their API documentation first",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so I've already gone ahead and I've gotten the method which was a get request so I've added that up here the URL was this over here with the scraping job ID and then your API token there so what I've done is I've grabbed the API token and the scraping job ID",
        "matched_patterns": [
          "api",
          "method"
        ]
      },
      {
        "content": "I mean I hardcoded it in here just while I was doing the testing let's actually make this Dynamic now drag the scraping job ID right over here voila",
        "matched_patterns": [
          "code",
          "api"
        ]
      },
      {
        "content": "and then the API token if you guys remember back here on the API page you have your access to API token",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "I'm imagine scraping this for some lead genen applica sorry some some e-commerce application list of products here product descriptions maybe product prices maybe product whatever the heck you want um",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but Cent how appify is is it is a Marketplace very similar to Rapid API um although extraordinarily well Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "um so the question is obviously how do you get this in NN well appify has a really easy to use um API which I like doing all you have is if we wanted to get the uh let's see get data set items",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "okay all I'm going to do is I'm just going to copy this go back here and then connect this to an HTTP request module as you could see we have this big long field here with my API appify API token",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but I just wanted to like allow you to see how to get data in naden really quickly now if we go to the schem of view we can see we legitimately we we already have all of the data that we we had from appify a second ago okay super easy and quick and simple to get up and running um we have the input URL field the ID field the type the short code caption now this is Instagram um every looks like we have some comments I don't have any style how do I create my man you just got to fake it till you make it",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "um obviously I was scraping an Instagram resource but like if you were scraping something else there'd be no change to this at all no change whatsoever now uh basically what we need in order to make this Dynamic basically make us able to run something in appify and then get it in NN so we need to set up an integration so just head over to this tab set up integration and then all you want to do is you just want to do web hook send an HTTP post web Hook when a specific actor event happens the actor event that we're going to want is basically when the run is succeeded the URL we're going to want to send this to if you think about it we just actually make another web hook request here web hook the URL we're going to want to send it to is going to be this test URL over here now I'm just going to delete all the header off stuff here because um it just uh complicates it especially for beginners",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so if we just go to that next HTTP request node what I can do is I can feed that in as a variable right here let going to a default data set ID drag that in between these two little lines and now we can test that step with actual live data now we have everything that we need so I don't know maybe now you want to feed this into Ai",
        "matched_patterns": [
          "variable"
        ]
      },
      {
        "content": "I mean the options are ultimately unlimited that's why I love appify so much the sixth way to scrape websites with NN is data for Co this is another thirdparty service but it's a very high quality one that's specifically geared towards search engine optimization requests you guys haven't seen data for SEO before it's basically this big API stack that allows you to do things like automatically query a service maybe some e-commerce website or some content website and then like extract things in nicely structured formatting um again specifically for SEO purposes tons of apis here as well I mean a lot of these services are now going towards like more Marketplace style stuff",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so I need to activate my account doesn't look like it allows you to feed in the code here so I'm just going to feed it in myself",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "uh it's obviously you're getting a lot of spammers hence this um bicycle stuff I don't know why the code isn't working here let me just copy this link address paste it in here instead there you go",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "so now you can sign in and once you're in you got also um they're actually really big on on bicycles they're training um a model to convert all ads on planet Earth into bicycles they'll actually give you a dollar worth of API access uh credits which is pretty cool um I'm not going to do that I'm just going to go over to mine which is$ 38 million bajillion dollars with 99,999 estimated days to go um",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and yeah let's actually run through this the first thing that I recommend you do is go over to playground on the Le hand side there's all of their different API endpoints that you can call um what I'm going to do is I'll just go to serp for now just to show you that you could scrape Google with this pretty easily",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "okay then I'm going to send a request to this API there's there's a bunch of other terms here that are going to make more sense if you're a SEO person um but now we receive as output a structured object with a ton of stuff",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "um you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers um that do more like I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to like create your own very complex flows you can do other stuff as well um you grab a bunch of keyword data",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "yeah looks like it's 390 per month so to the 390 people that are Googling me who are you and what do you want I'm just kidding um you can do things like you could find back links so you could find links um for I believe you feed in a website URL and then it finds back links to that website so this is you technically now scraping a bunch of other websites looking for links to the specific resource that you have that's kind of neat it looks like that found it basically immediately which is really really cool",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and it looks like they're referring top level links that are Dooms BG bgs would be interesting I wonder where that's coming from um there's a Content generation API playground",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but I think we're kind of getting away from um the actual thing that matters which is the scraping of the uh scraping of the websites so yeah lots of stuff lots of stuff for sure now that's all good",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but let's actually turn this into an API call if we head over to the API of do data for SEO so in my case docs.",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "um well I have a curl just like this which I can feed into um an API request that's what I'm going to do",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and I'm just going to import this curl import",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "but we have to convert it into something called base 64 um this is just how they do their API key stuff I guess it's kind of annoying",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "but it's just part and parcel of working with some apis you're just not always going to have it available to you really easily",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so what we need to do is we need to base 64 encode the username and the password um I'm just going to leave that at what I've done is I've actually gone through and done it in this edit Fields node um basically what you need to do is you need to have your username or your login",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "and then my password is What's called the API password you can find that really quickly and easily just by going over here to API access and then API password if you just signed up it'll be visible right here if it's been more than 24 hours you actually have to send it by email",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so that's um that's where i' get the API password from uh",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then once you feed it in over here where you're going to want to do is you're going to want to base 64 encode it like this they just require you to use these creds um or to operate with these creds as base 64 encoded versions Bas 64 is just a way to like translate into a slightly different number format so once you have that you would just feed in the variable right over here Ju",
        "matched_patterns": [
          "code",
          "variable"
        ]
      },
      {
        "content": "Just as follows and then you can make a request to their API and receive data",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "I just sent a request and now I receive a bunch of links with different headings and and so on and so forth that's easy the seventh way to scrape websites and Ed end is using a third party application called crawl Bas they're known for their rotating proxies which allow you to send very high volume um API requests",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so um it's very proxy driven this is their website so it's a scraping platform similar to Rapid API um and uh you know appify they support many of the major websites here and um the reason why they're so good at this is just because they you know as I mentioned they rotate the hell out of these proxies",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so now we have a crawling API smart proxy thing if you guys want to run like uh I don't know use in apps that have a proxy field specifically I'm just going to keep things simple we're doing this in n8n",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so we're going to go crawl base API we have a th000 free crawls remaining very first thing we're going to want to do is just click start crawling now just to get up and running with the API um and as you see here the these guys have probably one of the simplest apis possible all API URLs start with the folling base part click",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then all you need to do in order to make an API call is run the following sort of line",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so this is a curl request obviously we're in n8n and one of the value valuable parts of NN is we can just import a COR request",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "so well I'm going to import it as you can see here we have a token field then we just have the URL field of the place we want to crawl so I'm going to do left click.",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "so I'm now running this and it looks like we just received a bunch of very spooky data I don't like the spooky data no spooky data for us um sometimes spooky data like this H this seems kind of weird to me actually just give me one second to make sure that's right we are receiving a data parameter back which is nice",
        "matched_patterns": [
          "parameter"
        ]
      },
      {
        "content": "so the reason why that's valuable is because if you're scraping one of these websites I talked about before where when you send a simple HTTP request nothing pops up like this is the this is the purpose of this you actually feed in a JavaScript",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then they grabbed the code afterwards",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "yeah we have some some API call stuff over here um this one's just using Amazon this is pretty interesting",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and then we're just going to feed in the code here and then because I didn't feed in this we should now run this we're going to grab data from the site",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "but yeah very quick and easy way to use crawl base for this now the value in crawl base is not necessarily just to send them to static websites like I talked about it's to use like highly scalable scraping where you're scraping any applications consistently um as you see here the average API response time is between 4 to 10 seconds",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "right um sorry just jumping around the place here you can send 72,000 requests basically an hour which is crazy um and you can do so as quickly and as easily as just like adding an API call like",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "okay the eighth way to scrape data in nadn specifically website resources is octop parse octoparse is very similar to some of the other services that we've talked about um it is a web scraping tool that actually gives you quote unquote free web crawlers and I'm just a fan of their ux",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so I should be able to jump through and show you guys what this looks like we have a verification code I'm going to paste in if you're not familiar with jumping around and stuff like this um or if you're wondering how I'm jumping around I'm just using a bunch of website hotkeys",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "yes I want to open this thank you and the cool thing about octoparse um kind of relative to what else you know like the other scraping applications I talked about is this is just running in a desktop app um like kind of in in your computer",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and it's also local as opposed to a lot of these other ones which are not so I'm going to Auto log in on my desktop app remember my password beautiful the simplest and easiest way to scrape a s a service is just to pump in the the URL here then click Start and basically what'll happen is um it'll actually launch like an instance of your browser here with this little tool that allow you similarly the web scraping Chrome extension select the elements on the page you want scraped so I don't know maybe I want these logos scraped the second that I tapped one you'll see it automatically found six similar elements so now I'm actually like scraping all of this stuff",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so as you see I'm now mapping each of these very similarly to how I was doing before between the first field which is the title of the product and then the second field which is like the field to uh so that's pretty sweet we could do the same thing with a number of things you could extract like the headings and then the values and so on and so on and so forth but I'll kind of leave it there um so once you're done selecting all the elements that you want all you do is you click run and you have a choice between running it on your device versus running it on the cloud so um on the cloud is API supported that's how you're going to get stuff in NM",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so um you can dump this automatically to Google Sheets you could do zapier to connect to Google Sheets do like some sort of web Hook connection export to cloud storage uh similar stuff to the the web scraping Chrome extension um but for now let's just export this as Json give ourselves a little ad Json file here thank you",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "yeah now we have it locally now in order to connect with the octop par CPI what you're going to have to do is first you get up to request an access token the way that you do this is you send a post request to this URL here and the way that you format it is you need to send your username your password and then have the grantor type as password okay now password obviously just put in whatever your password is don't store it in PL text like I'm doing um with my hypothetical password put it somewhere else and then grab that data and then use it um but the the output of this is we have this big long access token variable which is great after that if I just go back to their API here um once we're here we can actually extract the data that we need so basically the thing that you're going to want is you're going to want um get data by Offset you can also use get non-exported data which is interesting so I think this just like dumps all of the data as not exported um and then sends that over to you",
        "matched_patterns": [
          "api",
          "variable"
        ]
      },
      {
        "content": "but anyway you could also get the data by offset so if I go a get request to open api.",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "octop course.com SL all and then I just send a header with the URL parameter this is a get request uh we're going to send a header with the token so authorization Bearer and then feed in the access token here just make sure that this is just one space",
        "matched_patterns": [
          "parameter"
        ]
      },
      {
        "content": "so um we'll feed this in as query parameters here so send query parameters the first value was task ID second one was offset and uh offset is no Capital the third was size offset's going to be zero size going to be I don't know let's just do 1,000 and what we need now is we need the task ID of the specific run that we just finished oh in order to get the task list you head over to task list top right hand corner here task ID API",
        "matched_patterns": [
          "api",
          "parameter"
        ]
      },
      {
        "content": "but I I like the idea that you can also just scrape locally which is pretty sweet and the last of our nine best ways to scrape websites in nadn is browserless now browserless runs a headless Chrome instance in the cloud this stuff is great for dynamic or heavy JavaScript websites if you've never used browser list before the cool part about browser list is allows you to actually bypass captas which is a big issue that a lot of people have um so I'm going to click try it for free I'm going to enter my email address over here verify I need to submit a code",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "so let's head back over here thank you thank you thank you thank you we have a ton of free trial signups obviously I don't have a promo code or anything don't have a company name",
        "matched_patterns": [
          "code"
        ]
      },
      {
        "content": "I'm just going to enter a password I'm using this to get past uh to avoid setting up a puppeteer and playright server sure I'm going to click complete we're now going to have a th credits inside of browser list which is pretty sweet um and we'll get a we'll get a full plan eventually we now have an API token",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so I can figure out how all of the stuff works here I'm just going to dive right into the API I can figure out how all of the API stuff works using their API docs which are fantastic by the way",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "um and we don't want to do any of this stuff we just want to do HTTP apis brow list API index",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so here's where we're at um if you want to send and receive a request what you need to do is uh you send a request to one of these endpoints content unblock download function PDF screenshot scrape or performance what we want for the purpose of this is just uh let's do content",
        "matched_patterns": [
          "function"
        ]
      },
      {
        "content": "so I'm just going to paste my API token up here copy this request feed it into nadn in the HTTP request module as per usual nice quick and easy I'm going to grab my API token",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and where it says your API token here I'm going to feed that in what I want as a website is just left click.",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "and I'm just leaving this as a secret and sort of a little I guess Easter egg for people that have made it this far in the video like my go-to when scraping websites is as I mentioned do that HTTP request trans forg that works then do something like fir C.D",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "so I can make all my HTP requests really easy um especially when you know there's issues with captas and and accessing resources and stuff check this out not only can you do um the actual scrape you can do a screenshot of the page as well and because I've entered my token up here the requests that I'm going to setting up are as simple as importing the curl then clicking test step so straightforward we now have a file which is the screenshot now I used example domain there let's go left click.",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "I don't think I've actually used this one before but for the purposes of this demonstration why don't we give it a try we'll go over here import the curl paste it in voila the website",
        "matched_patterns": [
          "import"
        ]
      },
      {
        "content": "aai going to test this step so now there servers doing a couple things like I'm scraping the site then converting it all into PDF format um probably screenshotting a bunch of stuff too if I view this now we now have my my file looks like it didn't capture all of the color aspects um that might just be difficult or whatever",
        "matched_patterns": [
          "api"
        ]
      },
      {
        "content": "I hope you guys appreciated the nine best ways to scrape websites in nadn as you guys could see it's a combination of on platform scraping using the HTTP request module a lot of like API documentation stuff like that if you want to get good at this I'm releasing a master class on API stuff um uh as part of my next na tutorial video uh",
        "matched_patterns": [
          "api",
          "class"
        ]
      },
      {
        "content": "and then you know navigating this and then and then taking the data from these services and using them to do something that you want to do like artificial intelligence to give you a summary of the site or generate ice breakers for you or do something else um whether you're using a local application like octop parse or maybe the web scraping CH uh Chrome extension or using something like firra browserless appify rapid API and so on and so forth um you now have everything that you need in order to scrape static sites Dynamic sites super Js heavy websites and even social media websites like Tik Tok Twitter and Instagram thanks so much for making it to this point in the video if you have any suggestions for future content drop them down below more than happy to take your idea and run with it assuming it's something that I haven't done before and then if you guys could do me a really big solid like subscribe do all that fun YouTube stuff",
        "matched_patterns": [
          "api"
        ]
      }
    ],
    "config": [
      {
        "content": "so uh we're just going to go HTTP request HTTP request node looks like this we have a method field a URL field authentication field query parameters headers body and then some options down here as well all",
        "matched_patterns": [
          "options"
        ]
      },
      {
        "content": "but this one doesn't so just going to go over here I'm going to copy this puppy go back over here I'm going to paste this in now technically what this is called is this is called polling um polling uh is where you know you're you're you're attempting to request a resource that you don't know whether or not is ready and there's a fair amount of logic that I'd recommend like putting into a polling flow where like when you try it and if it doesn't work basically you wait a certain amount of time and you retry again for the purpose of this video I'm not going to put all that stuff inside but um what I'm going to do is just set up this request I'm going to give this puppy a test let's just feed that in on the back end we got to put the extract ID right right over here where it said extract ID",
        "matched_patterns": [
          "set up"
        ]
      },
      {
        "content": "but maybe if you want to scrape like 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool um in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but um that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites",
        "matched_patterns": [
          "install"
        ]
      },
      {
        "content": "um so we're going to run through what it looks like so first thing I'm going to want to do is I'm going to want to let's just go Cloud login or sorry um start free 7-Day trial as you can see you know there's a free browser extension here if you wanted to do uh I don't know like highs scale stuff you'd choose probably their project um endpoint where we Sorry project plan where we have 5,000 URL credits we can run a bunch of tasks in parallel we could scrape Dynamic sites JavaScript sites we have a bunch of different export options then we can also just connect it directly to all of these um what I'm going to do just because I want this to kind of work as a first go is I'm just going to sign up to a free Tri here beautiful just created my account just go left click give it a phone number we'll go left click.",
        "matched_patterns": [
          "options"
        ]
      },
      {
        "content": "so we basically have everything we need now to set up a flow where we can schedule something in this web scraper service that maybe monitors some I don't know list of e-commerce product or something every 12 hours",
        "matched_patterns": [
          "set up"
        ]
      },
      {
        "content": "and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping um we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks like so we just got the data from web hook let's set up an HTTP request to their API now where we basically get the ID of the thing",
        "matched_patterns": [
          "set up"
        ]
      },
      {
        "content": "but Cent how appify is is it is a Marketplace very similar to Rapid API um although extraordinarily well Main ained and they also have a ton of guides set up to help you get you know up and running with scraping any sort of application",
        "matched_patterns": [
          "set up"
        ]
      },
      {
        "content": "um obviously I was scraping an Instagram resource but like if you were scraping something else there'd be no change to this at all no change whatsoever now uh basically what we need in order to make this Dynamic basically make us able to run something in appify and then get it in NN so we need to set up an integration so just head over to this tab set up integration and then all you want to do is you just want to do web hook send an HTTP post web Hook when a specific actor event happens the actor event that we're going to want is basically when the run is succeeded the URL we're going to want to send this to if you think about it we just actually make another web hook request here web hook the URL we're going to want to send it to is going to be this test URL over here now I'm just going to delete all the header off stuff here because um it just uh complicates it especially for beginners",
        "matched_patterns": [
          "set up"
        ]
      },
      {
        "content": "I mean the options are ultimately unlimited that's why I love appify so much the sixth way to scrape websites with NN is data for Co this is another thirdparty service but it's a very high quality one that's specifically geared towards search engine optimization requests you guys haven't seen data for SEO before it's basically this big API stack that allows you to do things like automatically query a service maybe some e-commerce website or some content website and then like extract things in nicely structured formatting um again specifically for SEO purposes tons of apis here as well I mean a lot of these services are now going towards like more Marketplace style stuff",
        "matched_patterns": [
          "options"
        ]
      },
      {
        "content": "and then you could like feed that into one of any of the other scrapers that we set up here to get data on stuff you could go Google Images Google Maps you could do Bing BYO YouTube Google's uh their own data set feature I don't really know what that is",
        "matched_patterns": [
          "set up"
        ]
      },
      {
        "content": "um you know we have a bun bunch of data here bunch of data you know you can use this to get URLs of specific things and then with the URLs you can then feed that into scrapers um that do more like I talked about earlier maybe appify or maybe rap API maybe fir crawl so a lot of options here to like create your own very complex flows you can do other stuff as well um you grab a bunch of keyword data",
        "matched_patterns": [
          "options"
        ]
      }
    ],
    "troubleshoot": [
      {
        "content": "so if you just understand that um you know when you scrape a dynamic resource what you're really doing is you're sending a request to a page which sends a request back to another server which then fills your thing this element eliminates 99% of the confusion because most of the time like scraping issues are hey",
        "matched_patterns": [
          "issue"
        ]
      },
      {
        "content": "so now we have the API request formatted correctly um all we need to do at this point is just click test step it looks like we're getting a Json breaking um error",
        "matched_patterns": [
          "error"
        ]
      },
      {
        "content": "then I'm just going to give this a test uh looks like I've issued a malformed request we just have to make sure that everything here is okay specify body let me just make sure there's nothing else in here it was a get request this is a get cool",
        "matched_patterns": [
          "issue"
        ]
      },
      {
        "content": "hi Nick I came across left click I'm impressed by you help B2B Founders scale their business automation keep in mind I never gave it my name it went it found my name on the website uh and then company name left click so quick and easy way uh you're going to have access to this template obviously without my API key in it um and feel free to you know use fir craw go nuts check out their documentation build out as complex a scraping flow as need be the third way to scrape websites in nadn is using rapid API for those of you that are unfamiliar rapid API is basically a giant Marketplace of third party scrapers similar to appify which I'll cover in a moment but instead of looking for um you know building out your own scraper for a resource let's say you're wanting to scrape Instagram or something that's not a simple static site what you can do is you could just get a scraper that somebody's already developed that does specifically that using proxies and all that tough stuff that I tend to abstract away um and then you just request uh to Rapid API which automatically handles the API request to the other thing that they want and then they format it and send it all back to and then you know you have beautiful um data that you could use for basically anything so this is what rapid API looks like it's basically a big Marketplace I just pumped in a search for website over here and we see 2,97 results to give you guys some context you can do everything from you know scraping social data like emails phone numbers and stuff like that from a website you could ping the ah refs SEO API you could find uh I don't know like unofficial medium data that they don't necessarily allow people to do so this is just a quick and easy way to I guess do a first pass after you've run through fir crawl maybe that doesn't work after you've run through HTTP request that doesn't work um just do a first pass look for something that scrapes the exact resource you're looking for and then take it from there so obviously for the purpose of this I'm just going to use the website to scraper API which is sort of just like a wrapper around what we're doing right now in nadn um but this website scraper API allows you to scrape some more Dynamic data um now I'm not signed up to this",
        "matched_patterns": [
          "handle"
        ]
      },
      {
        "content": "but maybe if you want to scrape like 5,000 doing it the way that I was doing it a moment ago might might be infusible the next way to scrape websites in nadn is using the web scraper Chrome extension and then tying that to a cloud service that delivers the data that you just created using their no code tool um in nicely bundled formats it's called Cloud sync as of the time of this recording I think they changed the name a couple of times but um that's where we're at here is the name of the service web scraper here is their website essentially what happens is you install a little Chrome plugin which I'll show you guys how to do then you select the fields that you want scraped in various data formats and then what you do is it handles JavaScript sites",
        "matched_patterns": [
          "handle"
        ]
      },
      {
        "content": "and then we can set up a web hook in NN that will catch the notification get the update now we can do is we can ping um we can ping the web scraping API which I'll show you to set up in a second to request the data from that particular scraping run and from here we can take that data do whatever the heck we want with it but obviously let me show you an example of what the the actual data looks like so we just got the data from web hook let's set up an HTTP request to their API now where we basically get the ID of the thing",
        "matched_patterns": [
          "catch"
        ]
      },
      {
        "content": "so I'm going to listen for this test event I'm going to run the same scraper again maybe we'll make it five posts per profile just to make it a little faster and um once this is done what it's going to do is it's going to send a record of all the information we need to get the data over to Ann we're going to catch that information",
        "matched_patterns": [
          "catch"
        ]
      },
      {
        "content": "but I I like the idea that you can also just scrape locally which is pretty sweet and the last of our nine best ways to scrape websites in nadn is browserless now browserless runs a headless Chrome instance in the cloud this stuff is great for dynamic or heavy JavaScript websites if you've never used browser list before the cool part about browser list is allows you to actually bypass captas which is a big issue that a lot of people have um so I'm going to click try it for free I'm going to enter my email address over here verify I need to submit a code",
        "matched_patterns": [
          "issue"
        ]
      },
      {
        "content": "so I can make all my HTP requests really easy um especially when you know there's issues with captas and and accessing resources and stuff check this out not only can you do um the actual scrape you can do a screenshot of the page as well and because I've entered my token up here the requests that I'm going to setting up are as simple as importing the curl then clicking test step so straightforward we now have a file which is the screenshot now I used example domain there let's go left click.",
        "matched_patterns": [
          "issue"
        ]
      },
      {
        "content": "and I'll catch you on the next video thank you very much",
        "matched_patterns": [
          "catch"
        ]
      }
    ]
  },
  "stats": {
    "total_length": 76831,
    "pattern_matches": 72,
    "semantic_matches": 221,
    "role_matches": 222
  }
}