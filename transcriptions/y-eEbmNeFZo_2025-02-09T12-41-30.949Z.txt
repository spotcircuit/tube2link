Title: The 9 Best Ways to Scrape Any Website in N8N
Video ID: y-eEbmNeFZo
Transcribed at: 2025-02-09T12:41:30.949Z

Transcription:
[0.1s] hey Nick here today I'm going to show
[1.4s] you the nine best ways to scrape any
[3.4s] website in nadn you're going to be able
[5.2s] to scrape static sites Dynamic sites
[7.4s] JavaScript social media whatever the
[9.4s] heck you want by the end of the video
[10.5s] you'll know how to do it I scaled my
[12.3s] automation agency to 72k a month using
[14.4s] no code tools like make and nadn and
[16.2s] scraping was a big part of that so this
[18.0s] video is just going to give you all the
[19.0s] sauce you're going to learn everything
[19.9s] you need to scrape websites like that on
[21.6s] your own let's get into it all right I'm
[23.2s] going to jump into NN in a minute and
[24.6s] actually build these alongside you and
[25.9s] one other thing I'm going to do is I'm
[26.8s] actually going to sign up to all the
[27.7s] services in front of you walk you
[29.1s] through the Authentication and the
[30.4s] onboarding flows and get your API keys
[32.4s] and stuff like that but just before I do
[34.6s] want to explain very quickly the
[36.0s] difference between a static site and
[38.1s] then a dynamic site because if you don't
[40.5s] know this um scraping just gets a lot
[42.4s] harder and so we're just going to cover
[43.6s] this in like 30 seconds and we can move
[45.0s] on so basically um if this is you okay
[47.4s] you're just this wonderful smiley person
[50.1s] and you want to access a static website
[52.3s] what you're doing is you're sending a
[54.3s] request over basically to just like some
[56.4s] document you know think about this as
[58.1s] just like a piece of paper on a cupboard
[60.0s] and there's a bunch of text on this
[61.0s] piece of paper and what you do is you
[62.7s] say hey can I have this piece of paper
[64.6s] and then the piece of paper just comes
[66.1s] back to you with all of the information
[67.7s] inside of the piece of paper okay this
[69.6s] is a very simplified version of what's
[71.6s] actually going on but static sites are
[73.4s] by far the easiest thing to scrape and
[75.6s] so um this is where you know a lot of
[77.6s] people think all websites are are at and
[80.4s] then they kind of confuse it with this
[81.5s] next step which is dynamic a dynamic
[84.0s] site essentially is not like that at all
[86.7s] basically what you're doing is you're
[87.8s] sending a request to a piece of paper
[90.3s] but the piece of paper has nothing on it
[92.2s] okay what happens is this piece of paper
[94.2s] then sends a request to some other dude
[96.4s] which I guess in this case is just a
[97.7s] server really who will then he has a
[100.2s] trusty pen in his hand and he'll
[101.8s] actually write all of the stuff on said
[104.3s] piece of paper and then you'll get the
[106.3s] piece of paper back so um there's
[108.6s] actually that intermediate step okay
[110.2s] where basically you are pinging some
[111.8s] sort of uh you know domain name or
[113.4s] whatever then that domain name shoots
[115.1s] some code over forces a server to
[117.0s] generate all of the contents on that
[118.6s] domain and then you get it
[120.5s] this is obviously kind of a two-step
[122.0s] process and then this is a three-step
[124.8s] process so if you just understand that
[128.2s] um you know when you scrape a dynamic
[129.6s] resource what you're really doing is
[130.9s] you're sending a request to a page which
[132.4s] sends a request back to another server
[134.0s] which then fills your thing this element
[136.0s] eliminates 99% of the confusion because
[138.2s] most of the time like scraping issues
[139.6s] are hey I just ping this page but I got
[141.4s] nothing back you know the HTTP request
[143.9s] or or or whatever I I sent you know it
[146.1s] was fine but for some weird reason
[147.6s] there's nothing on this page of of any
[149.2s] substance well if that was the case for
[151.0s] you it was most likely um because this
[152.9s] was empty before you sent it over um
[155.2s] whereas you know simple scraped static
[157.1s] resources tend to just like give you
[158.4s] what you want really quickly and it's
[159.9s] really easy all right so hopefully we at
[162.2s] least understand that there's that
[163.2s] difference between static and dynamic
[164.6s] sites here um I'm not going to go into
[166.5s] it more than that we're actually just
[167.5s] going to dive in with both feet start
[169.0s] doing a little bit of scraping and then
[170.2s] we'll kind of see where we land I find
[171.4s] the best way to do this stuff is just by
[173.3s] example and and you know being practical
[175.0s] about it so the first major way to
[177.5s] scrape websites in NN is using direct h
[179.8s] HTTP requests this is also what I like
[182.6s] to think of as the Magic in scraping
[185.2s] itself what we're going to do is we're
[187.0s] going to use a node called the HTTP
[188.9s] request node to send a get request to
[191.0s] the website we want this is going to
[192.7s] work with static websites and non
[194.2s] JavaScript resources so let me give you
[196.4s] guys a website that I'm going to be
[197.4s] scraping here this is my own site it's
[199.5s] called left click I'm about to do a
[200.5s] redesign um but this is a static
[203.0s] resource I know this because I built the
[204.4s] site you know I I did it in code and
[206.3s] basically all this is is just a document
[207.9s] somewhere on my or on on a server some
[209.9s] more so what I'm going to want to do is
[211.8s] and I'm just going to pretend that I
[212.7s] haven't done any of this so uh we're
[214.7s] just going to go HTTP
[217.2s] request HTTP request node looks like
[219.8s] this we have a method field a URL field
[222.5s] authentication field query parameters
[224.2s] headers body and then some options down
[226.1s] here as well all I'm going to do is I'm
[228.2s] just going to paste in the website that
[230.2s] I want to visit okay then I'm just going
[232.8s] to test the step it's that easy now the
[235.6s] response from this on the right hand
[236.9s] side see all this code over here this is
[239.3s] what's called HTML if you're unfamiliar
[241.6s] and HTML is basically just the like it's
[244.9s] it's the code behind the site so if I
[247.4s] were to zoom in over here you see where
[249.1s] it says I don't know let's let's go to
[250.7s] my website let's just find a little bit
[252.0s] of little bit of texture build hands off
[254.3s] growth systems okay if I just command F
[257.2s] and paste this in we actually have that
[259.4s] text buried somewhere in this big long
[261.9s] HTML string right so all that this HTML
[265.7s] is is this is the code that is sent to
[268.5s] my browser which is Google Chrome in
[270.5s] this case then my browser takes the code
[272.9s] and it just renders it into this
[274.4s] beautiful looking thing well beautiful
[276.0s] is a subjective State I would say but
[278.1s] this uh wonderful looking thing in front
[279.6s] of us which is this website with like
[281.4s] sizing and the tabs and the divs and all
[284.5s] that fun stuff okay so basically what
[287.1s] I'm trying to say is everything over
[288.6s] here on the right hand side this is the
[290.3s] entire site we can do anything we want
[292.7s] with this information um and we can
[294.4s] carry this information forward to to do
[296.0s] any one of our any one of many flows so
[299.0s] in my case right looking at a bunch of
[300.6s] code isn't really very pretty so one big
[302.8s] thing that you'll find in the vast
[303.8s] majority of modern um scraping
[305.3s] applications is you'll find that they'll
[306.9s] take that HTML which we saw earlier and
[308.7s] they'll convert it to something called
[309.8s] markdown okay so um this is a markdown
[313.8s] node we have a mode of HTML to markdown
[316.6s] and all I'm going to do is I'm going to
[318.4s] grab that data and I'm going stick it in
[320.3s] the HTML section of the HTML to markdown
[322.6s] converter what do you think is going to
[324.1s] happen when I test this well we're going
[325.3s] to convert this from this big long ugly
[328.0s] super dense uh thing with a much these
[330.0s] like greater than and less than symbols
[331.9s] and we're just going to convert it into
[333.1s] something a little bit shorter a little
[334.6s] bit simpler This Is Us just manipulating
[336.8s] file formats by the way and I find that
[338.3s] manipulating file formats is a big part
[339.8s] of what makes a good scraper a good
[341.1s] scraper so now we have something in
[343.4s] what's called markdown format what's the
[345.3s] value there well markdown format does
[346.8s] two things for us one it's much easier
[348.5s] to parse parse just means we can extract
[350.7s] different sections of the text we want
[352.4s] we can structure it in some sort of
[354.1s] other data format um and then in my case
[356.5s] because I love using AI for everything
[358.2s] it's much easier and shorter for us to
[360.2s] use with AI so I'm going to give you
[361.9s] guys a very simple example where we take
[363.6s] this text from the static resource and
[365.9s] then we just use um AI to tell us
[368.2s] something about it so I'll go down to
[369.9s] open Ai and then what I'm going to do is
[371.4s] I'll do the message a model just have to
[374.7s] connect my credential here I'm assuming
[376.0s] that you've already connected a
[376.9s] credential if not you're going to have
[378.5s] to go to opena website when you do the
[380.5s] connection um and grab your API key and
[382.4s] paste it in there's some instructions
[384.2s] that allow you to do so right over
[386.5s] here uh what I'm going to do is I'm
[388.6s] going to grab the G PT 40 Mini model
[391.4s] that's just the uh I want to say most
[393.3s] cost effective one as of the time of
[394.6s] this recording and then what I'm going
[396.4s] to do is I'm going to add three prompt
[397.6s] I'm going to add a system prompt first
[399.4s] I'll say you are a helpful intelligent
[402.2s] web scraping
[403.9s] assistant then I'm going to add a user
[406.0s] prompt and I'll say your task is to take
[408.7s] the
[409.8s] raw
[411.8s] markdown of a website and convert it
[414.4s] into structured data use the following
[417.5s] format and then I'm going to give it an
[419.1s] example of what I want in what's called
[420.8s] Json JavaScript object notation format
[422.9s] so the very first thing I'm going to do
[424.3s] is I'm going to have it just pull out
[425.5s] all the links on the website because I
[427.2s] find that that's a very common scraping
[428.6s] application so I go links and then I'm
[430.9s] just going to show an example of un
[433.9s] array of we'll go absolute URLs this is
[439.1s] very important that they're absolute
[440.2s] URLs any thing that we're going to build
[442.4s] after this is going to be making use of
[443.8s] the absolute URLs not the relative URLs
[445.7s] if you're unfamiliar with what that
[446.8s] means if we Zoom way in here you see how
[448.8s] there's this B uh SL left click log.png
[451.7s] this is what's called a relative URL if
[454.0s] you were to copy this and paste this
[455.4s] into here this wouldn't actually do
[457.7s] anything for us right uh what we what we
[459.7s] want is we want this instead we want
[461.3s] left click aka the root of the domain
[463.9s] and then um left click _ logogram and
[466.0s] that's how we get to the actual file
[467.2s] asset so uh if we go back over
[470.4s] here so if we go back over here um yeah
[473.7s] you know basically we want a link array
[476.4s] of absolute URLs and then I'm just going
[478.3s] to want main text website copy this is
[482.6s] going to be a long string containing all
[485.1s] of the website
[486.8s] copy containing just the text of the
[490.0s] site no
[491.4s] formatting and then why don't we do one
[493.5s] more thing why don't we have like a
[496.2s] summarized or summary let's do one line
[499.9s] summary just to show you guys you can
[501.6s] also use AI to do other cool stuff you
[503.8s] could take this oneline summary and feed
[505.4s] it into some big sequence you could have
[506.9s] ai write an icebreaker for an email you
[509.0s] could do a things with this but I'll say
[510.5s] on line summary um brief summarization
[514.4s] of what the site is and
[517.8s] how what the site is about let's do that
[522.3s] okay so this is our example I'm going to
[525.9s] say your website URL
[528.0s] is left click URL for the relative to
[533.0s] Absolute
[534.2s] conversions is left click. and then the
[537.0s] final thing is I'm going to add one more
[538.1s] user prompt I'm just going to draw drag
[539.7s] all of that markdown data in here then
[541.6s] I'm going to click output content as
[543.1s] Json I'm going to test the step I'm
[546.1s] going to take a sip of my coffee while
[547.4s] this puppy processes and we now have our
[549.9s] output on the right hand side if we go
[552.0s] to schema view what you can see is we've
[554.3s] now
[555.2s] generated basically an array of links on
[557.9s] the rightand side which contains every
[559.6s] link on this website very cool looks
[561.9s] like the vast majority of these are type
[563.3s] form links for some reason don't really
[564.8s] know what's about that oh right it's
[566.6s] because that's basically the only thing
[567.9s] on my website it's just a one P with a
[569.9s] bunch of different links to time for
[571.3s] that's funny um anyway you could
[573.4s] obviously just get it to Output one link
[575.0s] or tell it like make sure all the links
[576.4s] are unique or something um and then we
[578.4s] have a big chunk of plain text website
[580.2s] copy right then we have a oneline
[581.9s] summary of the site so this is a very
[584.4s] simple example of scraping we're
[586.2s] scraping a static resource obviously but
[588.4s] when I build scrapers for clients or for
[590.4s] my own business this is always my first
[592.1s] pass I will always just make a basic
[594.6s] HTTP request to the resource that I'm
[596.8s] looking at because if I can make that
[598.7s] http request work whether it's a get
[600.8s] request or whatever the the the rest of
[603.5s] my life building scraper building the
[605.2s] scraper is so easy I just take the data
[608.0s] I process it usually using AI or some
[609.7s] very cheap Tok cheap per token thing and
[612.4s] then voila you know like we've basically
[614.0s] built out a scraper in this case and
[615.4s] it's only taken us what three nodes
[617.5s] right so that's number one the second
[619.5s] way to scrape websites in NN is using a
[621.3s] third party service called fir crawl and
[623.0s] making an HTTP request to it I'm using
[625.4s] something called their extract endpoint
[627.3s] but just to make a long story short fire
[629.5s] craw is a very simple but High uh
[632.7s] bandwidth service that turns websites
[635.2s] into large language model ready data and
[637.4s] basically you know how earlier we had to
[638.8s] do HTTP request and then we had to
[640.4s] convert all that stuff into markdown and
[641.8s] then we had to you know manipulate that
[643.2s] markdown what this does is it just does
[645.3s] a lot of that stuff for you it'll
[646.7s] actually allow you to go scrape and then
[648.1s] it will automatically convert text into
[649.9s] markdown for you um so that you can do
[652.0s] whatever the heck you want they turn it
[653.1s] into structure data using Ai and and so
[654.7s] on and so on and so forth so if I were
[656.7s] to do the same thing that I just did
[658.0s] earlier
[659.7s] with my own
[660.8s] website then I were to you know run an
[663.0s] example of this what it would go do is
[665.5s] it would basically spin up a server for
[667.1s] me and that would actually go and
[668.8s] generate markdown of the same format um
[670.7s] the only difference here is it's
[671.7s] actually generated new lines between
[673.4s] sections of text how beautiful um and
[675.9s] then now you know we have basically the
[677.2s] same thing you also get it in Json which
[679.2s] is pretty cool um and you know you can
[680.7s] slot this into any workflow this is
[682.3s] basically like the simple and easy way
[684.0s] of getting started um what we're going
[686.3s] to be showing you today is the extract
[688.2s] endpoint which allows you to extract
[690.6s] data just using a natural language
[692.6s] prompt which is pretty cool and from
[694.4s] here we're going to be able to take any
[695.5s] URL and just turn it into structure data
[697.2s] but we're not actually going to have to
[698.0s] know how to parse we're not going to
[698.8s] have to know any code we're not going to
[699.6s] have to know any of that stuff so let me
[701.2s] actually run through the signup process
[702.8s] with you guys go to fire. Dev here just
[707.0s] going to open this up in an incognito to
[708.6s] show you guys what this looks like all
[709.8s] you do is you just go sign up I'm going
[713.8s] to add a password we're then going to
[715.8s] have to validate this one
[718.2s] oh guess these guys are Mega
[721.4s] secure so now I'm going to go back to my
[723.4s] email
[724.9s] address I'm going to count this up and
[729.2s] we have a call back here I just need to
[730.9s] paste this URL and put it in here that's
[732.7s] just because I you know I'm doing this
[734.4s] in an incognito tab normally when you do
[736.0s] this you're not going to have that step
[737.8s] okay great now we're inside a fir craw
[740.7s] they give you I think something like 500
[742.6s] um free credits something of that nature
[744.3s] anyway so what I'm going to do is I'm
[746.0s] going to go through and just like give
[748.2s] give this extracting point just a basic
[750.4s] natural language query so um let's go
[754.5s] from the homepage at left
[757.2s] click. I want to
[760.8s] extract a oneline summary of the website
[765.9s] let's do all of the text on the
[770.7s] website all of the copy on the website
[773.8s] in plain text let's do a oneline summary
[776.1s] of the website a oneline Icebreaker I
[778.9s] can use as the first line of of an of a
[781.9s] cold email to the
[785.2s] owner and uh the company name and a list
[791.5s] of the services they provide let's do
[794.4s] that this is a lot of requests we're
[797.6s] asking it to do like seven or eight
[799.5s] things but all I need to do in order to
[801.2s] make this work is I click generate
[802.4s] parameters it's going to basically now
[804.5s] generate me a big object with a bunch of
[806.2s] things so copy summary Icebreaker
[808.2s] company name and now I can actually go
[810.4s] and I can run this okay this is the URL
[813.1s] it just parsed as well let's give it a
[815.8s] run what it's doing now is it's scraping
[818.4s] the pages using their high throughput
[819.8s] server I just love this thing like I'm
[822.3s] not sponsored by fire crawl or anything
[823.6s] like that but I love their uh I don't
[825.8s] know I just love the design I love this
[827.2s] little like burning Ember or whatever
[829.8s] the heck you want to call it I love how
[831.5s] simple they've tried to make everything
[832.9s] it's it's great honestly okay awesome
[835.2s] and now you guys see we have basically a
[837.4s] big array with a bunch of sub objects we
[840.9s] have a summary like I asked for a list
[842.8s] of services looks like we even have
[844.7s] links to the specific places oh okay
[846.7s] links from the resource we have an
[848.6s] icebreaker and then we have the company
[850.4s] name as well so we can do a lot with
[851.5s] this right but right now this is just um
[853.4s] this is just on on a website how do we
[854.7s] actually bring this in naden uh well
[856.4s] it's pretty simple as you see there's an
[857.6s] integrate Now button you can either get
[859.9s] code or you can use it in zap here
[861.8s] basically what we're going to want to do
[862.8s] is we're going to want to run a request
[864.3s] to um their endpoint and then we're
[866.8s] going to want to turn that into
[869.2s] basically our HTTP request let me show
[870.8s] you what that looks like I'm just going
[873.1s] to do all of this stuff in curl so if we
[875.4s] go to curl as you can see what we need
[878.1s] to do is we need to format a request
[880.0s] that looks something like this but we
[881.7s] need to make sure it's using the extract
[883.3s] endpoint okay so I'm going to go down to
[885.3s] extract and then now I have this big
[887.7s] long beautiful string what I'm going to
[890.3s] do is I'm going to copy
[891.7s] this I'm going to go back to my NN
[895.2s] instance which is right over here and
[896.8s] then what I need to do is just open up
[898.0s] an HTTP request module and then click
[899.6s] import curl just paste all the stuff
[901.0s] inside now this is an example request
[903.6s] but that's okay we can actually use that
[904.7s] example request to very quickly and
[905.9s] easily format our our real request okay
[908.5s] so we're sending a bunch of headers this
[910.0s] is the endpoint that we're calling api.
[911.8s] fire. dv1 extract so basically what
[914.3s] we're doing now is we're like we're
[915.2s] sending a request to fir craw which will
[916.6s] then send a request to the website right
[918.0s] so kind of a kind of a middleman and
[920.4s] then all I'm going to do so if I go back
[922.2s] to my example we have an API key here
[924.8s] which we're going to need so I'm going
[925.7s] to go here and then paste in an API key
[928.3s] so that's how that work works right
[929.4s] authorization is going to be the name
[930.7s] value is going to be bear with a capital
[932.1s] b space and then the API key and then we
[935.2s] also have a body that we need to uh
[937.1s] adjust or edit and this body is where
[939.0s] we're going to put the links that we
[940.2s] want to actually have scraped with the
[941.5s] extract end point so what I'm going to
[944.2s] do is I'm going to delete most of these
[946.2s] I'll go back to my left click. a just
[948.5s] like this the prompts um because you
[950.6s] know I was just using their playground
[951.8s] before we're actually going to need to
[952.6s] convert this into a request for my
[956.1s] service so I'm just going to paste The
[957.7s] Prompt in here voila
[959.8s] and now we need to put together what's
[960.9s] called a schema where we have the
[962.8s] objects that we asked for so in my case
[964.7s] we asked for copy right so I'm going to
[966.5s] go Copy Type string then summary so
[969.2s] we're going to go summary type string
[972.3s] then Icebreaker it's going to be
[974.1s] Icebreaker type string then guess what
[976.6s] we have last but not least company name
[978.8s] which is going to be type string we're
[981.3s] also going to want to make these fields
[982.3s] required like uh you know you can set it
[984.2s] up so they're not actually required when
[985.4s] you do a request a fire call I'm I'm
[987.2s] going to make it so they're required so
[988.3s] I'll go copy
[989.8s] summary Icebreaker and then company name
[992.6s] actually you know what maybe I'll leave
[994.4s] company name as unrequired if you think
[996.1s] about it logically maybe not all the
[998.0s] websites we're going to be scraping
[999.1s] using this service are going to have the
[1000.8s] company names visible on the website I
[1002.8s] don't know but maybe so maybe I'll
[1004.6s] actually leave that as off okay great so
[1006.7s] now we have the API request formatted
[1008.5s] correctly um all we need to do at this
[1010.2s] point is just click test step it looks
[1012.6s] like we're getting a Json breaking um
[1015.0s] error and I think that's because I have
[1016.4s] this last comma and I'm just going to
[1018.1s] check to see if there are any commas in
[1019.5s] Jason you can't actually have the last
[1021.0s] element in an array have a comma on it
[1023.8s] so I think that's okay let me test it
[1025.2s] again all right so as you can see we
[1026.9s] just received an ID we've got a success
[1028.9s] and then we have a URL Trace array which
[1030.9s] is empty um if you think about this
[1032.4s] logically we don't actually get all the
[1034.0s] data that we send immediately because we
[1036.3s] need fir crawl to whip up the scraper
[1038.4s] you know do things to the data we could
[1040.3s] be feeding in 50 URLs here right so
[1042.6s] instead of just having the data
[1043.7s] available to us right now immediately
[1045.0s] what we need to do is we need to
[1045.9s] basically wait a little while wait until
[1047.5s] it's done and we need to Ping it and the
[1049.7s] reason why they've given us this ID
[1051.0s] parameter so that we could do the
[1052.2s] pinging so the way that you do this is
[1054.0s] you'd have to send a second HTTP
[1056.7s] request using this
[1059.0s] structure so the good news is we could
[1061.0s] just copy this
[1062.6s] over and then we can
[1065.8s] add a second um HTTP request I don't
[1069.5s] know where that went but I guess I'm
[1071.0s] just going to create it over here I'm
[1073.5s] going to import the curl to this request
[1075.4s] just like
[1076.6s] that then keep in mind that we just need
[1078.6s] to add our API key again because the
[1080.4s] previous node had it but this one
[1081.6s] doesn't so just going to go over here
[1083.1s] I'm going to copy this
[1085.2s] puppy go back over here I'm going to
[1088.0s] paste this in now technically what this
[1090.8s] is called is this is called polling um
[1092.8s] polling uh is where you know you're
[1095.2s] you're you're attempting to request a
[1097.4s] resource that you don't know whether or
[1098.8s] not is ready and there's a fair amount
[1100.4s] of logic that I'd recommend like putting
[1102.1s] into a polling flow where like when you
[1104.3s] try it and if it doesn't work basically
[1105.6s] you wait a certain amount of time and
[1106.6s] you retry again for the purpose of this
[1108.1s] video I'm not going to put all that
[1109.4s] stuff inside but um what I'm going to do
[1111.4s] is just set up this request I'm going to
[1113.6s] give this puppy a test let's just feed
[1116.2s] that in on the back end we got to put
[1117.6s] the extract ID right right over here
[1120.1s] where it said extract ID then I'm just
[1121.7s] going to give this a test uh looks like
[1124.2s] I've issued a malformed request we just
[1126.2s] have to make sure that everything here
[1128.0s] is okay specify body let me just make
[1130.0s] sure there's nothing else in here it was
[1131.5s] a get request this is a get cool we're
[1134.6s] not going to send a body
[1136.6s] then awesome and now we have all of the
[1138.9s] data available to us automate your
[1141.2s] business in the copy field summary field
[1142.9s] left clicks an ad performance
[1144.2s] optimization agency Icebreaker hi Nick I
[1146.3s] came across left click I'm impressed by
[1147.7s] you help B2B Founders scale their
[1149.0s] business automation keep in mind I never
[1151.3s] gave it my name it went it found my name
[1152.9s] on the website uh and then company name
[1155.0s] left click so quick and easy way uh
[1157.2s] you're going to have access to this
[1158.2s] template obviously without my API key in
[1159.9s] it um and feel free to you know use fir
[1161.4s] craw go nuts check out their
[1162.5s] documentation build out as complex a
[1164.0s] scraping flow as need be the third way
[1165.8s] to scrape websites in nadn is using
[1168.2s] rapid API for those of you that are
[1169.8s] unfamiliar rapid API is basically a
[1171.9s] giant Marketplace of third party
[1173.3s] scrapers similar to appify which I'll
[1175.0s] cover in a moment but instead of looking
[1176.8s] for um you know building out your own
[1178.7s] scraper for a resource let's say you're
[1180.2s] wanting to scrape Instagram or something
[1181.6s] that's not a simple static site what you
[1183.2s] can do is you could just get a scraper
[1184.7s] that somebody's already developed that
[1186.0s] does specifically that using proxies and
[1188.2s] all that tough stuff that I tend to
[1190.2s] abstract away um and then you just
[1192.6s] request uh to Rapid API which
[1195.1s] automatically handles the API request to
[1196.8s] the other thing that they want and then
[1198.0s] they format it and send it all back to
[1199.4s] and then you know you have beautiful um
[1201.1s] data that you could use for basically
[1202.5s] anything so this is what rapid API looks
[1204.4s] like it's basically a big Marketplace I
[1205.8s] just pumped in a search for website over
[1208.2s] here and we see 2,97 results to give you
[1211.2s] guys some context you can do everything
[1212.8s] from you know scraping social data like
[1216.0s] emails phone numbers and stuff like that
[1217.6s] from a website you could ping the ah
[1219.7s] refs SEO API you could find uh I don't
[1223.7s] know like unofficial medium data that
[1225.5s] they don't necessarily allow people to
[1226.8s] do so this is just a quick and easy way
[1228.7s] to I guess do a first pass after you've
[1231.0s] run through fir crawl maybe that doesn't
[1232.5s] work after you've run through HTTP
[1233.8s] request that doesn't work um just do a
[1235.6s] first pass look for something that
[1236.8s] scrapes the exact resource you're
[1238.0s] looking for and then take it from there
[1239.5s] so obviously for the purpose of this I'm
[1240.6s] just going to use the website to scraper
[1242.0s] API which is sort of just like a wrapper
[1244.0s] around what we're doing right now in
[1245.4s] nadn um but this website scraper API
[1248.0s] allows you to scrape some more Dynamic
[1249.4s] data um now I'm not signed up to this so
[1251.4s] I'm going to have to go through the
[1252.2s] signup process and I'm going to show you
[1253.4s] guys what that looks like um but yeah
[1255.0s] we're going to we're going to run
[1255.8s] through an API request to Rapid API
[1257.5s] which is going to make this a lot easier
[1260.6s] just going to put in all of my
[1262.0s] information
[1264.9s] here and then I'm going to do the
[1267.9s] classic email verification okay just
[1271.6s] copy this puppy over no thank you rise I
[1274.0s] use a time management app called rise
[1276.0s] and every time I go on my Gmail I set my
[1278.4s] Gmail up as like a definitely do
[1282.2s] not uh do during your workday let's just
[1285.4s] call it personal projects they don't ask
[1286.7s] me all these questions my goal today is
[1288.6s] to browse available apis awesome so
[1291.1s] that's their onboarding I think we're
[1292.1s] going to have to like pay a little bit
[1293.6s] of money or something like that which
[1294.5s] I'll sort out in a moment um but the
[1296.1s] scraper that I want is I just want the
[1297.4s] website one right so I'm going to type
[1298.6s] website in
[1299.8s] here uh I'm going to look
[1302.1s] for wherever it was earlier website
[1304.6s] scraper API and now check this out what
[1307.6s] we have is we have the app which is the
[1309.6s] name of the specific API that we're
[1311.6s] requesting we have an x-raid api-key and
[1315.2s] this is the API key we're going to use
[1316.4s] to make the request then we have the
[1318.2s] request URL which is basically what
[1320.0s] we're pinging and what we can do here is
[1321.7s] we can feed in the parameters okay what
[1323.5s] website we want to we want to scrape and
[1325.4s] then we can actually just like give it
[1326.6s] give it a run so I'm going to have to
[1328.0s] subscribe to this in order to test it uh
[1330.2s] I'm just going to go to the um basic
[1332.8s] plan and I'm going to pay money per
[1334.2s] month that probably seems the simplest
[1335.8s] way to do so okay and I just ran through
[1337.5s] the payment let's actually head over
[1338.7s] here and let's just run a test using my
[1340.6s] website URL we're going to test this
[1342.1s] endpoint now and now this actually going
[1344.3s] to go through Rapid API it's going to
[1345.7s] spin up the server and then it's going
[1347.3s] to send it and what we see here is we
[1349.1s] have multiple fields that Rapid apis or
[1351.0s] this particular scraper gives us let me
[1352.7s] just make this easier for you all to see
[1354.2s] we have a text content field with all of
[1356.0s] the content of the website which is cool
[1357.7s] this is basically what I did earlier um
[1359.6s] but instead of me having to formulate
[1360.7s] this request try and parse it and try
[1362.0s] and use AI tokens what I did is I sent
[1363.6s] the request to uh rapid API and did it
[1366.0s] all for me then we also have an HTML
[1367.6s] content
[1368.9s] field I think we have one more here
[1371.6s] scroll all the way down to the bottom as
[1373.2s] you can see there is a ton of HTML um
[1375.2s] and then we also have a list of all of
[1376.6s] the images on the website which is very
[1378.6s] very cool and easily formatted again
[1380.4s] something that I tried to do manually
[1381.7s] using AI but now you know we have
[1383.2s] everything in that nice absolute URL
[1384.6s] format um and then if they find any
[1386.7s] social media links I don't believe um
[1389.2s] there were more than Twitter but um if
[1391.6s] they find anything that's at their
[1392.5s] Twitter Instagram whatever then we have
[1394.0s] the link right over here it looks like
[1395.3s] they even give you the scraping time and
[1397.0s] if they scrape emails or phone numbers
[1398.6s] um they'll be there as well so I mean
[1400.3s] rapid AP is obviously fantastic this is
[1401.9s] a high throughput sort of thing and why
[1404.0s] don't we actually run through what this
[1405.1s] would look like if we were to run a curl
[1406.6s] request you see how it's automatically
[1409.3s] just formatting it as curl well that
[1410.7s] just means we just jump back here
[1412.5s] connect this to my HTTP request module
[1415.0s] click import curl paste it in like this
[1418.0s] import and it's actually going to go
[1419.8s] through and it's going to automatically
[1421.0s] map all these fields for me right query
[1423.2s] parameter URL left click. beautiful um
[1425.9s] API key x-raid API host here's the host
[1429.0s] here's the name of the API key here's
[1431.4s] everything we need well I can actually
[1432.7s] just recreate this request now inside of
[1434.4s] NN as opposed to being on rapid API and
[1437.4s] then I have all the data accessible to
[1438.9s] me here how cool is that so we can do
[1441.3s] this for any any major website really um
[1444.0s] you know there are a lot of specific
[1446.0s] bespoke scrapers obviously which um I
[1448.0s] don't know if you wanted to scrape uh
[1449.8s] let's go back to Discovery if you wanted
[1451.7s] to scrape like Instagram or something
[1454.1s] you could scrape um Instagram uh you
[1456.3s] could do like Facebook scraping you
[1458.8s] could scrape these large giants that are
[1460.3s] quite difficult to do So Meta ad Library
[1462.2s] Facebook ad scraper and depending on the
[1464.2s] plan that you're at it might be more
[1465.2s] cost- effective for you to sign up to
[1467.0s] some sort of monthly recurring thing
[1468.5s] rather than just pay two cents every
[1469.9s] single time you make one of these
[1470.8s] requests you just kind of got to do that
[1472.2s] determination yourself right like if
[1473.9s] you're scraping uh I don't know 50 every
[1476.8s] day or 100 every day or something might
[1478.7s] be a dollar or two a day which is
[1479.8s] reasonable but maybe if you want to
[1481.0s] scrape like 5,000 doing it the way that
[1482.6s] I was doing it a moment ago might might
[1484.0s] be infusible the next way to scrape
[1485.5s] websites in nadn is using the web
[1487.4s] scraper Chrome extension and then tying
[1489.4s] that to a cloud service that delivers
[1491.6s] the data that you just created using
[1493.2s] their no code tool um in nicely bundled
[1495.8s] formats it's called Cloud sync as of the
[1497.7s] time of this recording I think they
[1498.9s] changed the name a couple of times but
[1500.8s] um that's where we're at here is the
[1502.7s] name of the service web scraper here is
[1505.3s] their website essentially what happens
[1507.0s] is you install a little Chrome plugin
[1508.6s] which I'll show you guys how to do then
[1510.5s] you select the fields that you want
[1512.0s] scraped in various data formats and then
[1514.8s] what you do is it handles JavaScript
[1516.9s] sites Dynamic sites all that fun stuff
[1519.5s] and then you can um export that
[1522.8s] data as a cloud run to then send back
[1528.9s] sorry big sneeze to then send back to
[1531.0s] some API or some service um and then
[1533.0s] automatically do parsing and stuff like
[1534.7s] that so very cool I'm going to show you
[1536.0s] guys what that looks like um this is
[1537.4s] sort of a more customized way to build
[1538.9s] the stuff but I've seen a lot of people
[1540.4s] do this with naden um so we're going to
[1542.5s] run through what it looks like so first
[1543.9s] thing I'm going to want to do is I'm
[1545.1s] going to want to let's just go Cloud
[1547.4s] login or sorry um start free 7-Day trial
[1551.0s] as you can see you know there's a free
[1552.5s] browser extension here if you wanted to
[1554.0s] do uh I don't know like highs scale
[1556.2s] stuff you'd choose probably their
[1558.4s] project um endpoint where we Sorry
[1561.0s] project plan where we have 5,000 URL
[1563.0s] credits we can run a bunch of tasks in
[1565.0s] parallel we could scrape Dynamic sites
[1567.0s] JavaScript sites we have a bunch of
[1568.4s] different export options then we can
[1569.8s] also just connect it directly to all of
[1571.4s] these um what I'm going to do just
[1573.0s] because I want this to kind of work as a
[1574.4s] first go is I'm just going to sign up to
[1575.6s] a free Tri here beautiful just created
[1577.2s] my account just go left click give it a
[1581.1s] phone number we'll go left click. a
[1584.4s] we're going to go I don't know academic
[1586.7s] records needed per month we'll go 0 to
[1588.4s] th000 length of the project uh I don't
[1590.1s] know let's go two to 3
[1591.6s] months okay great so now we can import
[1594.4s] and run our own site map or we can use a
[1596.0s] premade Community sit map um what I'm
[1598.2s] going to do is I'm just going to import
[1599.9s] this we're then going to get the Chrome
[1601.7s] extension web
[1603.8s] scraper let me add that extension and
[1606.0s] it's going to download it do all that
[1607.0s] fun stuff beautiful so now we have it
[1610.5s] right over here I'm just going to pin it
[1612.1s] to my browser to make my life easier go
[1614.2s] to left click. a open up this puppy now
[1617.0s] there's a bunch of like tutorials and
[1618.4s] how to use this stuff um that's not that
[1619.8s] big of a deal but basically the thing
[1621.3s] you need is you need to hold command
[1623.1s] plus option plus I to open up your
[1624.5s] developer tools and you'll just find it
[1625.8s] on the in my case the far right so
[1627.6s] command option I that'll open up Dev
[1630.2s] tools you see all the way on the right
[1632.0s] hand side here I have a couple other
[1632.9s] things like make and and cookie editor
[1634.7s] but all the way on the right hand side
[1635.8s] here we have this web scraper thing um
[1638.0s] so we got what you're going to want to
[1639.3s] do first you're going to want to create
[1640.3s] a site map for the resource that you're
[1641.6s] going to want to scrape I'm just going
[1642.8s] to call it left click and I just want to
[1644.9s] scrape left click. okay once we have our
[1647.2s] sitemap if I just give a quick little
[1648.7s] click I can then add a new selector and
[1650.7s] the really cool thing about this web
[1652.0s] scraper is um if I just zoom out a
[1654.1s] little bit here uh what you can do is
[1655.7s] you can you can select the elements on
[1657.4s] the website that you want scraped so for
[1659.2s] instance it's a very quick and easy way
[1660.6s] to do this if you think about it is like
[1662.9s] just to show you guys an example
[1663.8s] structure data is uh sort of like an
[1665.6s] e-commerce application let's say you
[1666.8s] have like the title of a product and you
[1668.0s] have like I don't know the the
[1669.1s] description of a product so on my
[1670.3s] website really quick and easy way to do
[1672.0s] this is let's just call this
[1673.8s] products and it's a type text what I'm
[1676.7s] going to do is I'm going to click select
[1678.7s] then I'll just click on this I'll click
[1681.8s] on this as well and as you see it'll
[1683.8s] automatically find all of the headings
[1686.6s] that I'm looking for so that's products
[1689.0s] we are going to then click data I'm
[1690.8s] going to click done selecting data
[1692.3s] preview as you can see it only selected
[1694.3s] one of them the very first so what we're
[1695.8s] going to want to do is go multiple and
[1697.4s] now if I data preview we get all of the
[1699.2s] headings which is very cool so now we
[1701.5s] have a basically like a list of headings
[1703.9s] um from here I'm going to save this
[1705.1s] selector I'm add a new one let's go
[1707.3s] product
[1708.9s] descriptions and then going to select
[1711.9s] this this it'll select all of them I'll
[1714.4s] go multiple data preview just to make
[1716.4s] sure that it looks good I'm getting no
[1717.8s] data extracted here oh sorry I didn't
[1719.4s] actually select the um didn't actually
[1721.3s] finish it now we're getting product
[1722.9s] descriptions that's pretty cool um this
[1724.9s] is me doing this sort of like one at a
[1726.7s] time you can also um group The selectors
[1729.7s] there you go it's actually um offered to
[1731.8s] group it for me so we can uh group this
[1734.9s] into one object with products and then
[1736.7s] product descriptions so it's automatic
[1738.3s] group it now we have wrapper for
[1739.9s] products and products descriptions then
[1741.4s] we have products and product
[1742.3s] descriptions buried underneath we could
[1744.1s] go as far as we want with this but
[1745.9s] basically what I'm what I'm trying to
[1746.9s] show you guys is very simple and easy
[1748.4s] just drag your mouse over the specific
[1750.5s] thing you want if you select more than
[1752.0s] one it'll automatically find all of them
[1753.5s] on the website which is really cool okay
[1755.8s] great once we have this um what I can do
[1757.4s] is I can actually go export sitemap so
[1760.6s] now I have all of the code on the
[1761.9s] website that actually goes and finds it
[1763.3s] for me then I can paste this in here
[1766.0s] I'll just call this left click scraper
[1768.8s] and I'm going to import this to my cloud
[1771.1s] scraper uh I think I'm running into oh
[1774.4s] sorry I don't think we can do a space
[1776.0s] there my bad just call it left click and
[1778.0s] now what we can do is we can actually
[1779.1s] just like run a server instance that
[1781.0s] goes out and then scrapes this for us
[1783.0s] okay so I'm going to click scrape it
[1785.2s] looks like I need to verify my email so
[1787.0s] just make sure you do that before you
[1788.2s] try and get ahead of yourself like I
[1792.0s] was okay looks like we just verified the
[1794.2s] email let's head back over here refresh
[1797.3s] then scrape we've now scheduled a
[1799.4s] scraping job for this sitemap scheduling
[1802.0s] you know in their lingo just means that
[1803.6s] it's now part of their big long queue of
[1805.4s] thousands of other things that they're
[1806.5s] probably scraping through their server
[1807.8s] and that's fine okay I just gave this a
[1809.2s] refresh and as we see we have now
[1811.0s] finished said scraping job we have all
[1812.7s] of the data available to us using their
[1814.4s] UI but now that we've gone through this
[1816.5s] process of you know building out this
[1818.5s] this thing um how do we actually take
[1820.0s] that and then use it in our nadn flows
[1822.6s] so variety of ways um if you wanted to
[1824.6s] connect this let's say to specific
[1825.9s] service like Dropbox um Google
[1828.5s] you know dump anow or something Google
[1830.0s] Drive I'd recommend just doing it
[1831.4s] directly through their integration it's
[1832.7s] just a lot easier to get the data there
[1834.9s] and then you can just connect it to n
[1836.1s] and watch the data as it comes in or
[1837.9s] something you can also use the web
[1839.8s] scraper API uh this is pretty neat
[1842.5s] because you can you know that's what
[1844.0s] we're going to end up using it was
[1845.4s] pretty neat because you can uh like
[1847.2s] schedule jobs you can send jobs you can
[1849.0s] do basically everything just through the
[1850.0s] NN interface and then we can just
[1852.2s] retrieve the data afterwards which is
[1853.6s] pretty neat um this is basically what
[1855.4s] you end up getting you end up with
[1857.0s] scraping job ID status sitemap all this
[1859.7s] fun stuff and then we can set like a web
[1862.2s] hook URL where we we receive the request
[1865.4s] so um let me check we need a scraping
[1867.9s] for testing you need a scraping job that
[1869.4s] has already been finished I think our
[1870.9s] scraping job has already been finished
[1872.7s] I'm just going to go htps uh back to my
[1875.4s] n8n flow I'm actually going to build an
[1878.7s] n8n web hook give that a click I'm not
[1882.8s] going to have any authentication let me
[1884.5s] just turn all this off basically what we
[1886.6s] want is we we want to use this as our
[1887.9s] test
[1889.0s] event we're going to go back to the
[1892.0s] API paste this in
[1895.0s] save and I'm just going to want to give
[1897.0s] it a test endpoint here so
[1900.0s] test looks like um the push notification
[1902.8s] was failed the reason why is because
[1904.6s] it's saying this web Hook is not
[1905.6s] registered for post request did you mean
[1907.0s] to make a get request beautiful thank
[1908.5s] you naden we absolutely did so I'm going
[1910.9s] to stop listening change your HTTP HTTP
[1913.2s] method here to post there's basically
[1914.6s] two ways to call a website and this is
[1916.0s] one of them I'm going to listen for test
[1917.5s] events go back here and then
[1919.4s] retest awesome looks like we've now
[1921.8s] triggered the beginning of our workflow
[1923.7s] using this data let's see what sort of
[1925.2s] information was in
[1926.9s] it okay great we have the scraping job
[1929.1s] ID the status execution
[1931.3s] mode okay great so we basically have
[1933.4s] everything we need now to set up a flow
[1936.1s] where we can schedule something in this
[1937.5s] web scraper service that maybe monitors
[1939.2s] some I don't know list of e-commerce
[1941.1s] product or something every 12 hours and
[1943.2s] then we can set up a web hook in NN that
[1945.0s] will catch the notification get the
[1947.4s] update now we can do is we can ping um
[1950.4s] we can ping the web scraping API which
[1952.3s] I'll show you to set up in a second to
[1954.4s] request the data from that particular
[1956.0s] scraping run and from here we can take
[1958.2s] that data do whatever the heck we want
[1959.7s] with it but obviously let me show you an
[1961.2s] example of what the the actual data
[1963.0s] looks like so we just got the data from
[1964.8s] web hook let's set up an HTTP request to
[1968.2s] their API now where we basically get the
[1970.3s] ID of the thing and then we can call uh
[1973.1s] we can call that back so got my API
[1975.1s] token over here I'm going head over to
[1976.9s] their API documentation first okay and
[1979.4s] then what we want to do is download
[1980.9s] these scrape data in CSV format at least
[1983.1s] in my case I imagine most of you guys
[1984.4s] are going to add this to a spreadsheet
[1985.9s] or whatever um you can very easily do
[1988.0s] whatever you want there's also a Json
[1989.8s] format endpoint here um but let's just
[1992.4s] do CSV for Simplicity so I've already
[1994.9s] gone ahead and I've gotten the method
[1996.6s] which was a get request so I've added
[1998.4s] that up here the URL was this over here
[2001.6s] with the scraping job ID and then your
[2003.8s] API token there so what I've done is
[2005.5s] I've grabbed the API token and the
[2007.1s] scraping job ID I mean I hardcoded it in
[2008.8s] here just while I was doing the testing
[2010.9s] let's actually make this Dynamic now
[2013.3s] drag the scraping job ID right over here
[2016.3s] voila and then the API token if you guys
[2019.0s] remember back here on the API page you
[2021.0s] have your access to API token so just
[2022.5s] copy that
[2023.7s] over uh great and now if I run this I'm
[2027.4s] actually selecting that specific job
[2029.0s] then from here we have all the data that
[2030.4s] we just scraped as you can see there's
[2031.8s] like a uh the way that CSV Works
[2034.2s] actually let me just copy this over here
[2035.6s] I just wanted to give this to you guys
[2036.8s] as an example of a different data type
[2038.4s] but maybe some people here aren't really
[2040.1s] familiar with it basically the way that
[2041.6s] it works is if I just paste this into
[2042.9s] like a Google sheet you see how it looks
[2044.2s] like this what what you can do is if you
[2045.9s] just um split the text to columns you
[2048.1s] kind of see
[2049.2s] how kind of see how there's like these
[2051.4s] four pettings there's web scraper order
[2053.1s] web scraper startup products and product
[2055.3s] descriptions right I'm imagine scraping
[2057.2s] this for some lead genen applica sorry
[2059.2s] some some e-commerce application list of
[2061.2s] products here product descriptions maybe
[2063.1s] product prices maybe product whatever
[2064.8s] the heck you want um so yeah you can you
[2067.4s] can put in like a number of formats and
[2069.0s] I just wanted to give you guys an
[2069.9s] example what that looks like the next
[2071.4s] way to scrape websites in naden is using
[2073.3s] appify if you guys are no strangers to
[2075.2s] this channel you know that I do appify
[2077.2s] all the time and I talk about them all
[2078.6s] the time because I think that they're
[2079.7s] just a great service um they've now
[2081.6s] given me a 30% discount where anybody
[2084.0s] can use it for I was initially under the
[2086.0s] impression it was lifetime I think it's
[2087.2s] three months so you probably get 30% off
[2088.7s] your first three months just check the
[2090.2s] um description if you want that but Cent
[2092.1s] how appify is is it is a Marketplace
[2094.4s] very similar to Rapid API um although
[2096.6s] extraordinarily well Main ained and they
[2098.7s] also have a ton of guides set up to help
[2101.0s] you get you know up and running with
[2102.3s] scraping any sort of application so just
[2105.1s] as we had earlier we have Instagram
[2106.5s] scrapers we have Tik Tok scrapers we
[2109.6s] have email scrapers we have map scrapers
[2112.6s] Google Maps we could do I don't know
[2115.1s] Twitter
[2115.9s] scrapers uh medium scrapers right
[2118.9s] basically any any service out there that
[2122.3s] has this Dynamic aspect to it that's not
[2124.3s] a simple HTTP request you can make you
[2125.9s] could scrape it using ampify and then
[2127.1s] obviously you you have things too like
[2128.6s] just like basic website crawlers you can
[2130.9s] generate screenshots of sites I mean
[2132.7s] there's just there's so many things let
[2134.0s] me walk you guys through what it looks
[2135.7s] like now in my case I'm not actually
[2137.6s] going to sign up to appify because I
[2138.6s] have like 400 accounts but trust me when
[2140.2s] I say it is a very easy and simple
[2141.9s] process you go to app ay.com you go get
[2145.5s] started you put in your email and your
[2147.8s] password they'll give you $5 in free
[2150.1s] platform credit you don't need any
[2151.7s] credit card and you can just get up and
[2153.0s] running and start using this for
[2154.2s] yourself super easily then the second
[2156.4s] that you have all that you'll be Creed
[2158.0s] with this screen it is a console screen
[2160.6s] don't be concerned when you see this um
[2163.2s] you know this is super simple and and
[2165.0s] easy and and not a big deal this is one
[2166.7s] of my free accounts um so I just wanted
[2168.4s] to show you guys what you can do with a
[2169.7s] free account uh but from here what you
[2172.0s] do is you go to the store and as you can
[2173.8s] see I'm just dark mode all this is the
[2175.1s] same thing we were just looking at
[2176.0s] before and then um we're just going to
[2177.9s] run a test on the thing that we want to
[2179.7s] scrape okay so what I'm going to want to
[2181.8s] do is for the purposes of this I'm now
[2183.8s] going to do something different from
[2185.2s] what I was doing before like which was
[2186.4s] just left click over and over and over I
[2188.0s] think that kind of gets boring what I'm
[2189.4s] going to do is I'm going to scrape
[2190.2s] Instagram posts okay so what I'm going
[2192.3s] to do is I'm going to feed in a name
[2195.0s] nickf this is just my um
[2198.2s] Instagram uh which almost hit 10K in God
[2201.0s] like 15 days or something like that but
[2203.0s] I'm going to feed in my Instagram here
[2204.7s] and then I'm just going to grab like I
[2205.8s] don't know the last 10 posts okay save
[2208.1s] and start this is now going to run an
[2210.0s] actor actor is just their term for
[2211.6s] scraper which will go out it'll extract
[2214.0s] data from my Nick surve Instagram and as
[2217.1s] you can see will get a ton of fields
[2218.5s] caption owner full name owner Instagram
[2220.3s] URL comments count first comment likes
[2223.0s] count timestamp query tag we get
[2225.2s] everything from these guys which is
[2226.3s] really cool this might take you know 30
[2228.4s] 40 50 seconds we are spinning up a
[2230.3s] server in real time every time you do
[2232.0s] this as you see in bottom left hand
[2233.1s] corner there's a little memory tab which
[2234.4s] shows that we are legitimately running a
[2236.4s] server with one gigabyte of memory right
[2238.0s] now so generally my recommendation when
[2239.8s] you use appify is not to use it for
[2241.1s] oneoff requests like this feed in 5 to
[2243.4s] 10 15 20 Instagram Pages uh but you know
[2246.9s] I just got the back and voila we we have
[2248.6s] it it's right in front of us we have all
[2249.9s] of the data of that person's Instagram
[2252.0s] profile so you can see it's quite
[2253.3s] scalable in that way um so the question
[2255.7s] is obviously how do you get this in NN
[2257.0s] well appify has a really easy to use um
[2259.1s] API which I like
[2260.6s] doing all you have is if we wanted to
[2264.2s] get the uh let's see get data set items
[2267.7s] okay all I'm going to do is I'm just
[2269.5s] going to copy
[2270.7s] this go back here and then connect this
[2273.4s] to an HTTP request
[2275.2s] module as you could see we have this big
[2278.3s] long field here with my API appify API
[2281.3s] token and this specific data set that
[2283.0s] I'm looking for I'll show you how to get
[2284.5s] it dynamically but I just wanted to like
[2286.1s] allow you to see how to get data in
[2288.0s] naden really quickly now if we go to the
[2290.3s] schem of view we can see we legitimately
[2292.1s] we we already have all of the data that
[2293.6s] we we had from appify a second ago okay
[2296.1s] super easy and quick and simple to get
[2297.6s] up and running um we have the input URL
[2300.0s] field the ID field the type the short
[2301.9s] code caption now this is Instagram um
[2305.4s] every looks like we have some comments I
[2307.3s] don't have any style how do I create my
[2309.7s] man you just got to fake it till you
[2311.8s] make it I don't have any style either
[2313.6s] just some nerd in my mom's basement uh
[2316.6s] yeah so you you can scrape any resource
[2318.8s] you want here um obviously I was
[2320.0s] scraping an Instagram resource but like
[2321.9s] if you were scraping something else
[2323.7s] there'd be no change to this at all no
[2325.1s] change whatsoever now uh basically what
[2327.7s] we need in order to make this Dynamic
[2329.6s] basically make us able to run something
[2331.5s] in appify and then get it in NN so we
[2334.1s] need to set up an integration so just
[2335.6s] head over to this tab set up integration
[2338.0s] and then all you want to do is you just
[2339.2s] want to do web hook send an HTTP post
[2342.2s] web Hook when a specific actor event
[2343.8s] happens the actor event that we're going
[2345.5s] to want is basically when the run is
[2347.8s] succeeded the URL we're going to want to
[2349.8s] send this to if you think about it we
[2351.8s] just actually make another web hook
[2353.6s] request here web
[2355.3s] hook the URL we're going to want to send
[2357.5s] it to is going to be this test URL over
[2361.2s] here now I'm just going to delete all
[2362.2s] the header off stuff here because um it
[2364.6s] just uh complicates it especially for
[2366.2s] beginners um but we're going to copy
[2368.0s] this over head back over here paste in
[2370.1s] this URL and then let me see this is a
[2372.8s] post request I think I don't actually
[2374.1s] remember so we're going to have to
[2375.1s] double check I think it's a post
[2377.3s] request yeah and then what I'm going to
[2379.6s] do is I'm going to listen for a test
[2380.9s] event run the test web
[2383.7s] hook so we're listening we're making a
[2385.9s] get request okay so the fact that it
[2387.5s] hasn't connected yet probably tells me
[2388.9s] it's a post request so let's move over
[2390.4s] here move this down to post now let's
[2392.3s] listen to a test event let's run this
[2395.1s] puppy one more time so we just
[2397.4s] dispatched it and yeah the post request
[2399.0s] succeeded and what did we get we got
[2400.5s] tons of information we got a body with a
[2402.6s] user ID created at event data joke right
[2405.0s] looks like when you test something out
[2406.4s] they just send you a joke about how
[2408.3s] Chuck nurse can sketi a cow in two
[2410.0s] minutes have you ever heard of the word
[2411.3s] sketi before this moment I haven't I
[2413.6s] want to be known for my ability to sketi
[2416.7s] we'll go Instagram website
[2419.4s] scraper okay and now if we go back here
[2422.2s] right we're now listening for a test
[2424.2s] event so I'm going to listen for this
[2426.0s] test event I'm going to run the same
[2427.4s] scraper again maybe we'll make it five
[2428.8s] posts per profile just to make it a
[2429.9s] little
[2430.8s] faster and um once this is done what
[2433.4s] it's going to do is it's going to send a
[2435.4s] record of all the information we need to
[2437.8s] get the data over to Ann we're going to
[2440.5s] catch that information and then we're
[2442.2s] going to use it to query the the the
[2444.2s] database basically that it created for
[2445.7s] that particular Instagram run which will
[2448.4s] then enable us to do whatever the heck
[2449.6s] we want with it so it's now starting to
[2451.8s] crawl as we see here we had five
[2453.2s] requests so it should be able to do this
[2454.4s] in like the next 5 seconds or so okay
[2456.2s] and once that's done we now have an
[2457.5s] actor succeeded event um and then we
[2460.9s] have uh let me see the data that we want
[2464.3s] would be the default data set ID down
[2466.5s] over here so if we just go to that next
[2468.4s] HTTP request node what I can do is I can
[2471.0s] feed that in as a variable right
[2474.4s] here let going to a default data set
[2478.6s] ID drag that in between these two little
[2483.0s] lines and now we can test that step with
[2485.1s] actual live data now we have everything
[2486.8s] that we need
[2487.8s] so I don't know maybe now you want to
[2489.3s] feed this into Ai and you want to have
[2490.4s] ai tell you something about the last
[2492.6s] five posts tell you wow those last five
[2495.0s] posts were amazing Nick I loved the
[2497.4s] specifically the one on Korea and I just
[2499.2s] wanted to send you over some quick
[2500.5s] assets to help you out right you can now
[2502.9s] do super Dynamic and structured Outreach
[2506.1s] you could take that data and use it to
[2508.3s] like draft up your own post I mean the
[2510.4s] options are ultimately unlimited that's
[2512.2s] why I love appify so much the sixth way
[2514.5s] to scrape websites with NN is data for
[2517.2s] Co this is another thirdparty service
[2519.2s] but it's a very high quality one that's
[2520.9s] specifically geared towards search
[2522.6s] engine optimization requests you guys
[2524.8s] haven't seen data for SEO before it's
[2526.9s] basically this big API stack that allows
[2528.7s] you to do things like automatically
[2530.4s] query a service maybe some e-commerce
[2532.5s] website or some content website and then
[2534.8s] like extract things in nicely structured
[2537.0s] formatting um again specifically for SEO
[2539.4s] purposes tons of apis here as well I
[2541.6s] mean a lot of these services are now
[2542.9s] going towards like more Marketplace
[2544.8s] style stuff but just to give you guys an
[2546.5s] example you could like Google really
[2548.2s] quickly to scrape a big list of Google
[2550.3s] search results for a term and then you
[2551.6s] could like feed that into one of any of
[2553.3s] the other scrapers that we set up here
[2554.6s] to get data on stuff you could go Google
[2556.6s] Images Google Maps you could do Bing BYO
[2559.4s] YouTube Google's uh their own data set
[2561.4s] feature I don't really know what that is
[2562.4s] but I imagine it's pretty cool uh and
[2564.4s] then you can you can take this data and
[2565.5s] do really fun stuff with it so I'm just
[2567.4s] going to click try for free over here in
[2568.8s] the top right hand corner show you guys
[2570.0s] what that looks
[2572.2s] like and as you see here um I signed in
[2575.4s] to data for SEO to my own account looks
[2577.6s] like I have 38 million bajillion
[2580.7s] dollars um but obviously you'd have to
[2583.6s] go through the rig Rolla creating your
[2584.8s] own account so why don't actually just
[2586.2s] do that with you and then I'll just use
[2587.3s] that account that is 38 million
[2588.6s] bajillion dollars we'll click try for
[2591.8s] free we'll go Nikki
[2595.2s] Wiki uh let's use a different email I
[2597.5s] need a business email huh that's
[2601.4s] unfortunate okay I do agree to the terms
[2603.6s] of use
[2605.1s] absolutely uh bicycle
[2608.9s] is that a bicycle that's not a
[2612.3s] bicycle what does it mean when I can't
[2615.7s] answer these does it mean that I'm a
[2618.2s] robot if you look at some of my posts
[2620.6s] some of my comments people would
[2622.8s] absolutely say yes it means that that
[2625.3s] you're a robot um I don't know why
[2627.6s] people keep saying stuff like dude Nick
[2629.3s] nice AI Avatar bro but I'm it's not an
[2632.7s] AI Avatar it's not an AI Avatar at all
[2635.4s] it's actually just me okay anyway so I
[2637.8s] need to activate my account doesn't look
[2639.8s] like it allows you to feed in the code
[2641.1s] here so I'm just going to feed it in
[2642.4s] myself uh it's obviously you're getting
[2644.3s] a lot of spammers hence
[2645.9s] this um bicycle stuff I don't know why
[2649.3s] the code isn't working here let me just
[2652.1s] copy this link address paste it in here
[2655.0s] instead there you go okay great so now
[2657.6s] you can sign
[2659.1s] in and once you're in you got also um
[2663.0s] they're actually really big on on
[2664.4s] bicycles they're training um a model to
[2666.2s] convert all ads on planet Earth into
[2668.3s] bicycles they'll actually give you a
[2669.8s] dollar worth of API access uh credits
[2672.6s] which is pretty cool um I'm not going to
[2674.2s] do that I'm just going to go over to
[2675.0s] mine which is$ 38 million bajillion
[2676.5s] dollars with 99,999 estimated days to go
[2680.1s] um and yeah let's actually run through
[2681.4s] this the first thing that I recommend
[2682.5s] you do is go over to playground on the
[2683.9s] Le hand side there's all of their
[2685.8s] different API endpoints that you can
[2687.4s] call um what I'm going to do is I'll
[2689.0s] just go to serp for now just to show you
[2690.3s] that you could scrape Google with this
[2691.5s] pretty easily so maybe I'm in the UK and
[2694.2s] I want to scrape um let me see
[2698.0s] a keyword ni arrive okay then I'm going
[2701.3s] to send a request to this
[2703.7s] API there's there's a bunch of other
[2705.7s] terms here that are going to make more
[2707.0s] sense if you're a SEO person um but now
[2709.8s] we receive as output a structured object
[2712.5s] with a ton of stuff right we have um the
[2715.2s] first result here it's like an organic
[2716.9s] one with some big URL a bunch of chips
[2720.6s] um I'm I have like a Knowledge Graph
[2722.0s] profile which is cool apparently it
[2723.8s] finds it says I'm a freelance writer um
[2727.4s] you know we have a bun bunch of data
[2728.9s] here bunch of data you know you can use
[2730.4s] this to get URLs of specific things and
[2732.9s] then with the URLs you can then feed
[2734.4s] that into scrapers um that do more like
[2736.7s] I talked about earlier maybe appify or
[2738.4s] maybe rap API maybe fir crawl so a lot
[2742.7s] of options here to like create your own
[2744.0s] very complex
[2745.5s] flows you can do other stuff as well um
[2748.6s] you grab a bunch of keyword data so
[2750.7s] maybe you wanted to find a keyword and
[2753.7s] maybe again it's Nicks or location you
[2756.0s] want let's do United States that'll
[2757.3s] probably be
[2758.3s] better language um I'm just not going to
[2761.2s] select an language and then I'll do a
[2763.5s] request so now it's going to find us um
[2766.2s] a bunch of search volume related stuff
[2768.4s] so I don't actually know how many people
[2770.0s] are searching for me in 2025 apparently
[2773.3s] 390 is this per month H wonder if it's
[2776.9s] per month per day that's interesting uh
[2780.0s] I don't really know why they break it
[2781.3s] down by like the month date yeah looks
[2784.0s] like it's 390 per month so to the 390
[2786.4s] people that are Googling me who are you
[2788.3s] and what do you want I'm just kidding um
[2791.0s] you can do things like you could find
[2792.2s] back links so you could find links um
[2795.4s] for I believe you feed in a website URL
[2798.5s] and then it finds back links to that
[2800.1s] website so this is you technically now
[2802.0s] scraping a bunch of other websites
[2803.7s] looking for links to the specific
[2805.2s] resource that you have that's kind of
[2809.2s] neat it looks like that found it
[2811.4s] basically immediately which is really
[2812.9s] really
[2814.2s] cool and it looks like they're referring
[2816.6s] top level links that are Dooms BG bgs
[2819.8s] would be interesting I wonder where
[2820.7s] that's coming from um there's a Content
[2823.3s] generation API playground so you could
[2825.4s] you know feed in some text and then have
[2827.2s] it generate other stuff but I think
[2828.5s] we're kind of getting away from um the
[2830.2s] actual thing that matters which is the
[2831.8s] scraping of the uh scraping of the
[2833.7s] websites so yeah lots of stuff lots of
[2836.6s] stuff for sure now that's all good um
[2838.4s] but let's actually turn this into an API
[2840.0s] call if we head over to the API of do
[2842.5s] data for SEO so in my case docs. datafor
[2844.9s] seo.com V3 _ page SL contentor parsing
[2850.2s] live that's what I'm I'm curious about
[2852.4s] you'll see that we have a post request
[2853.9s] that we need to send to this URL um well
[2856.2s] I have a curl just like this which I can
[2859.3s] feed into um an API request that's what
[2862.0s] I'm going to do so I'm going to go back
[2863.4s] over here and I'm just going to import
[2865.6s] this curl
[2867.9s] import and it's going to go through and
[2870.0s] it's basically going to um parse out all
[2872.3s] these fields that I'm interested in with
[2873.6s] the URL which I'll go htps
[2877.5s] left click. AI um and then we have sort
[2880.6s] of like a gacha here that a lot of
[2882.0s] people don't understand this is the um
[2883.6s] authorization the authorization is a
[2885.0s] little bit different from most of the
[2886.1s] easy authorizations we've had so far we
[2887.7s] actually have to convert it um we have
[2888.9s] to go one one more step basically to
[2890.6s] make this work if I check out the let's
[2895.6s] see um
[2897.7s] authorization
[2899.4s] here what we need is we need to um get
[2903.3s] the login and then the P so this is your
[2906.3s] username and then your password then we
[2908.4s] have to Hash it or not hash it but we
[2910.0s] have to convert it into something called
[2911.0s] base 64 um this is just how they do
[2913.8s] their API key stuff I guess it's kind of
[2915.7s] annoying but it's just part and parcel
[2917.2s] of working with some apis you're just
[2918.8s] not always going to have it available to
[2920.5s] you really easily so I'm just going to
[2922.3s] go back to data for SEO and then I'm
[2925.1s] going to grab my credentials okay so
[2928.0s] what we need to do is we need to base 64
[2930.1s] encode the username and the password um
[2932.3s] I'm just going to leave that at what
[2933.4s] I've done is I've actually gone through
[2934.4s] and done it in this edit Fields node um
[2937.0s] basically what you need to do is you
[2938.3s] need to have your username or your login
[2942.2s] so maybe this is me searching Nix or
[2944.1s] have Reddit uh Nick left click. so that
[2947.7s] might be my username and then my
[2949.1s] password is What's called the API
[2950.6s] password you can find that really
[2951.7s] quickly and easily just by going over
[2953.5s] here to API access and then API password
[2956.2s] if you just signed up it'll be visible
[2957.5s] right here if it's been more than 24
[2958.8s] hours you actually have to send it by
[2959.8s] email but anyway so that's um that's
[2962.0s] where i' get the API password from uh
[2964.0s] and then once you feed it in over here
[2966.4s] where you're going to want to do is
[2967.1s] you're going to want to base 64 encode
[2968.7s] it like this they just require you to
[2971.2s] use these creds um or to operate with
[2975.1s] these creds as base 64 encoded versions
[2977.0s] Bas 64 is just a way to like translate
[2978.6s] into a slightly different number format
[2980.1s] so once you have that you would just
[2981.4s] feed in the variable right over
[2983.8s] here Ju Just as follows and then you can
[2986.1s] make a request to their API and receive
[2987.9s] data so uh it looks like I was doing
[2991.0s] their content parsing live you know what
[2993.5s] I wanted to do is I just wanted to
[2995.2s] call their endpoint which I think was
[2998.3s] their like instant
[3002.4s] Pages this one right over here so it's
[3004.2s] just V3 uh once you've sorted this out
[3006.4s] by the way the AP gets like
[3007.4s] extraordinarily easy to manage you just
[3009.1s] need to like figure out the
[3009.8s] authentication from there on out all
[3011.3s] you're literally doing is just swapping
[3012.7s] out the requests so you know if you
[3014.3s] wanted to do instant Pages all I'm doing
[3016.6s] is pumping that in there I just sent a
[3019.3s] request and now I receive a bunch of
[3020.9s] links with different headings and and so
[3022.6s] on and so forth that's easy the seventh
[3024.2s] way to scrape websites and Ed end is
[3025.8s] using a third party application called
[3027.4s] crawl Bas they're known for their
[3029.2s] rotating proxies which allow you to send
[3031.1s] very high volume um API requests so um
[3033.8s] it's very proxy driven this is their
[3035.5s] website so it's a scraping platform
[3037.2s] similar to Rapid API um and uh you know
[3040.6s] appify they support many of the major
[3042.7s] websites here and um the reason why
[3044.5s] they're so good at this is just because
[3045.9s] they you know as I mentioned they rotate
[3047.2s] the hell out of these proxies so we're
[3049.3s] just going to sign up to Tri it free
[3051.6s] I'll use my business email
[3054.6s] here and then continue with Emil
[3057.9s] email we got to add a phone number
[3060.2s] obviously we're going to do less than a
[3061.6s] thousand I'm a CTO I don't want to
[3064.1s] what's the animal right is that an
[3066.4s] animal yes it's an animal good God beep
[3070.4s] boop uh we're going to head over to my
[3074.4s] Gmail and receive this
[3078.4s] now so we need to confirm my account
[3080.8s] just going to copy this link address
[3082.2s] that I can do this in one
[3083.8s] page awesome we should be good to log in
[3086.3s] so that's what's
[3087.6s] happening we need to select the animal
[3089.9s] again just doesn't it doesn't believe
[3092.0s] really just doesn't
[3094.2s] believe okay great so now we have a
[3096.6s] crawling API smart proxy thing if you
[3099.0s] guys want to run like uh I don't know
[3101.5s] use in apps that have a proxy field
[3103.4s] specifically I'm just going to keep
[3104.4s] things simple we're doing this in n8n so
[3106.2s] we're going to go crawl base API we have
[3107.8s] a th000 free crawls remaining very first
[3110.1s] thing we're going to want to do is just
[3111.2s] click start crawling now just to get up
[3113.0s] and running with the
[3114.5s] API um and as you see here the these
[3116.7s] guys have probably one of the simplest
[3117.8s] apis possible all API URLs start with
[3120.3s] the folling base part click and then all
[3122.5s] you need to do in order to make an API
[3124.1s] call is run the following sort of line
[3126.3s] so this is a curl request obviously
[3128.8s] we're in n8n and one of the value
[3131.0s] valuable parts of NN is we can just
[3132.4s] import a COR request so well I'm going
[3134.8s] to import it as you can see here we have
[3137.1s] a token field then we just have the URL
[3139.6s] field of the place we want to crawl so
[3142.0s] I'm going to do left click. for now um I
[3144.9s] don't know if this token field was
[3146.1s] actually my real token I don't believe
[3148.0s] so maybe we'll give it a try maybe it's
[3149.4s] like a test token or
[3151.8s] something so I'm now running this and it
[3154.6s] looks like we just received a bunch of
[3156.0s] very spooky data I don't like the spooky
[3158.2s] data no spooky data for
[3161.1s] us um sometimes spooky data like
[3167.3s] this H this seems kind of weird to me
[3169.7s] actually just give me one second to make
[3171.8s] sure that's right we are receiving a
[3173.9s] data parameter back which is nice but
[3176.6s] yeah something about this is a little
[3177.5s] bit spooky um was it a get request or
[3180.2s] was it a post request no I guess it's a
[3181.8s] get request
[3183.0s] strange very very
[3186.7s] strange okay anyway they give you two
[3189.3s] types of tokens here um this is why I'm
[3191.0s] talking about it to begin with I'm also
[3192.7s] because I just used it before for a
[3194.0s] couple of applications and I found it
[3195.2s] very easy they give you a normal token
[3196.9s] and they give you a JavaScript um token
[3199.0s] as well so the reason why that's
[3201.0s] valuable is because if you're scraping
[3202.7s] one of these websites I talked about
[3203.8s] before where when you send a simple HTTP
[3205.4s] request nothing pops up like this is the
[3207.3s] this is the purpose of this you actually
[3208.6s] feed in a JavaScript token um when you
[3211.0s] use the JavaScript token it'll
[3212.4s] automatically launch a browser instance
[3214.6s] inside of um craw base for you so
[3217.4s] instead of you getting just like that
[3219.1s] empty thing back that I mentioned I'm
[3220.9s] you're actually going to get uh you're
[3222.2s] going to get like a JavaScript version
[3224.7s] of the website where somebody went on
[3226.5s] the website it loaded really briefly and
[3228.4s] then they grabbed the code
[3230.3s] afterwards so yeah we have some some API
[3232.9s] call stuff over here um this one's just
[3234.8s] using Amazon this is pretty interesting
[3236.2s] so I might actually give that a a go
[3238.8s] just to give you guys an example of said
[3240.4s] Amazon
[3242.4s] scrape uh let's just go
[3249.7s] www.amazon.com oh right Amazon might be
[3251.6s] JavaScript actually so maybe we give
[3253.1s] that a go no it looks like we um we got
[3255.1s] the data from from Amazon which is
[3256.8s] pretty cool if you feed that into the
[3258.6s] markdown converter like we had before
[3260.6s] it's going to feed in the HTML here pump
[3263.3s] it into a data
[3264.6s] key we've now converted this
[3268.0s] into uh this is very long let's go
[3270.5s] tabular we've now converted this into
[3272.8s] markdown which is cool and this is
[3274.7s] pretty long right obviously has all of
[3276.7s] the images and has all of the
[3278.0s] information on the site which is cool
[3279.8s] and then we can feed it into open AI
[3281.7s] like I did
[3282.9s] before where I message a model and I'm
[3286.2s] just going to copy um from my uh
[3288.3s] previous application here to make my
[3289.7s] life a little bit
[3290.8s] easier where the heck are you
[3296.8s] and then we're just going to feed in the
[3298.0s] code here and then because I didn't feed
[3302.7s] in this we should now run this we're
[3306.0s] going to grab data from the site and
[3308.1s] we're going to try and I mean you know
[3309.8s] we kind of all know what Amazon is and
[3311.2s] what it does right so I'm not expecting
[3313.3s] expecting anything spectacular but it's
[3315.0s] still going to go it's going to give me
[3316.3s] all of the text on this Amazon page and
[3318.1s] then I'm going to get a bunch of list of
[3319.5s] links basically absolute URLs ideally
[3322.6s] should play some Jeopardy
[3325.3s] music or going be able to play um Star
[3328.3s] Wars music that'd be kind of cool okay
[3331.4s] we now have a schema with all of the
[3333.0s] links on the page which is pretty cool
[3334.8s] we have the plain text website copy we
[3337.1s] have a on line summary uh you know plain
[3339.6s] text website copy is a lot longer than
[3341.2s] this obviously it's just shortening and
[3342.6s] truncating it for us but yeah very quick
[3344.9s] and easy way to use crawl base for this
[3346.9s] now the value in crawl base is not
[3348.4s] necessarily just to send them to static
[3350.0s] websites like I talked about it's to use
[3352.0s] like highly scalable scraping where
[3355.4s] you're scraping any applications
[3358.1s] consistently um as you see here the
[3360.2s] average API response time is between 4
[3362.1s] to 10 seconds so you you will receive
[3363.8s] results back pretty quick if you wanted
[3365.6s] to just send one request or 20 requests
[3368.2s] every second think about it like 20
[3370.7s] requests a second times 60 seconds a
[3372.4s] minute is 1,200 requests times 60
[3374.2s] minutes and an hour 72,000 requests
[3376.3s] right um sorry just jumping around the
[3378.7s] place here you can send 72,000 requests
[3380.8s] basically an hour which is crazy um and
[3383.2s] you can do so as quickly and as easily
[3384.8s] as just like adding an API call like
[3386.8s] and then it'll automatically distinguish
[3388.3s] between like a a plain text thing or a
[3390.2s] JavaScript thing okay the eighth way to
[3391.9s] scrape data in nadn specifically website
[3394.1s] resources is octop parse octoparse is
[3397.0s] very similar to some of the other
[3398.4s] services that we've talked about um it
[3400.4s] is a web scraping tool that actually
[3401.9s] gives you quote unquote free web
[3403.2s] crawlers and I'm just a fan of their ux
[3404.9s] I think it's very clean I think the way
[3406.4s] that they have their signup flow and
[3407.4s] stuff's really easy so if you made it to
[3409.4s] this part of the uh tutorial and you
[3411.0s] have yet to sign up to one of these
[3412.3s] Services give octoparse um give
[3414.7s] octoparse your thoughts
[3416.8s] let's double
[3418.3s] check that I haven't actually created an
[3420.5s] account using this no I haven't
[3421.8s] fantastic so I should be able to jump
[3423.8s] through and show you guys what this
[3425.7s] looks like we have a verification code
[3427.0s] I'm going to paste in if you're not
[3429.5s] familiar with jumping around and stuff
[3431.6s] like this um or if you're wondering how
[3433.9s] I'm jumping around I'm just using a
[3435.0s] bunch of website
[3439.0s] hotkeys okay great account is now ready
[3442.1s] so we can start a free premium trial if
[3444.1s] you want I think you're going to have to
[3445.0s] add a card um I don't know if I have
[3446.7s] enough credits to actually do anything
[3449.2s] but if I'm not then I'll start that
[3450.5s] trial in a second what you're going to
[3452.0s] have to do in order to make this work is
[3453.2s] you're going to want to have to download
[3454.4s] you're going to want to download the
[3456.1s] octoparse desktop app so let's give it a
[3459.5s] quick and easy go just going to drag
[3462.6s] this puppy um if you are using something
[3465.3s] that is not Mac OS you will not have
[3467.0s] this strange drag and drop feature here
[3470.5s] once that is done you will have octo
[3472.0s] parse accessible just open that up yes I
[3474.4s] want to open this thank you
[3477.9s] and the cool thing about
[3479.7s] octoparse um kind of relative to what
[3483.7s] else you know like the other scraping
[3485.8s] applications I talked about is this is
[3487.2s] just running in a desktop app um like
[3489.2s] kind of in in your
[3490.8s] computer so like it's cool because it's
[3493.2s] just easy to get up and running with um
[3494.8s] and it's also local as opposed to a lot
[3496.9s] of these other ones which are not so I'm
[3499.5s] going to Auto log in on my desktop app
[3501.0s] remember my password beautiful the
[3502.7s] simplest and easiest way to scrape a s a
[3504.8s] service is just to pump in the the URL
[3506.6s] here then click Start and basically
[3508.6s] what'll happen is um it'll actually
[3510.3s] launch like an instance of your browser
[3512.0s] here with this little tool that allow
[3513.6s] you similarly the web scraping Chrome
[3515.2s] extension select the elements on the
[3516.9s] page you want scraped so I don't know
[3519.1s] maybe I want these logos scraped the
[3521.2s] second that I tapped one you'll see it
[3522.7s] automatically found six similar elements
[3525.3s] so now I'm actually like scraping all of
[3527.8s] this stuff okay now we have access to
[3530.4s] this sort of drag and drop or um
[3532.9s] selector thing similar to what we had
[3535.0s] before if you click on one of these
[3536.9s] you'll see it allow you to select all
[3538.4s] similar
[3539.6s] Elements which is pretty sweet and then
[3542.6s] um you can also do things like click
[3544.2s] elements and so on and so forth extract
[3546.5s] the text Data here um you can also tie
[3549.1s] that to other things right so as you see
[3552.6s] I'm now mapping each of these very
[3554.0s] similarly to how I was doing before
[3555.8s] between the first field which is the
[3557.5s] title of the product and then the second
[3559.0s] field which is like the field to uh so
[3561.1s] that's pretty sweet we could do the same
[3562.5s] thing with a number of things you could
[3563.6s] extract like the headings and then the
[3565.1s] values and so on and so on and so forth
[3567.2s] but I'll kind of leave it there um so
[3569.0s] once you're done selecting all the
[3570.2s] elements that you want all you do is you
[3571.4s] click run and you have a choice between
[3573.9s] running it on your device versus running
[3575.7s] it on the cloud so um on the cloud is
[3578.5s] API supported that's how you're going to
[3579.6s] get stuff in NM but I just want you guys
[3580.9s] to know that you can also just run it
[3581.9s] here you could run it here load up the
[3583.6s] URL scrape all the things that you want
[3585.4s] on the specific page you're feeding in
[3586.8s] and then you can be done with it so I
[3588.1s] just selected run in the cloud it's now
[3589.8s] going to open up said Cloud instances as
[3591.6s] we could see we have this little field
[3593.5s] where it's running and extracting the
[3595.0s] data we're now done so I can export this
[3598.0s] data locally um but I could also do a
[3600.2s] lot of other stuff which we'll show you
[3601.4s] in a second so um you can dump this
[3603.8s] automatically to Google Sheets you could
[3605.2s] do zapier to connect to Google Sheets do
[3607.4s] like some sort of web Hook connection
[3608.6s] export to cloud storage uh similar stuff
[3610.8s] to the the web scraping Chrome extension
[3613.6s] um but for now let's just export this as
[3617.0s] Json give ourselves a little ad Json
[3619.4s] file here thank
[3621.1s] you and yeah now we have it locally now
[3623.4s] in order to connect with the octop par
[3624.8s] CPI what you're going to have to do is
[3626.3s] first you get up to request an access
[3628.0s] token the way that you do this is you
[3629.6s] send a post request to this URL here and
[3633.5s] the way that you format it is you need
[3634.9s] to send your username your password and
[3638.3s] then have the grantor type as password
[3641.0s] okay now password obviously just put in
[3642.9s] whatever your password is don't store it
[3644.0s] in PL text like I'm doing um with my
[3645.7s] hypothetical password put it somewhere
[3647.4s] else and then grab that data and then
[3649.0s] use it um but the the output of this is
[3651.0s] we have this big long access token
[3652.4s] variable which is great after that if I
[3655.2s] just go back to their API here um once
[3657.4s] we're here we can actually extract the
[3658.9s] data that we need so basically the thing
[3661.3s] that you're going to want is you're
[3662.2s] going to want um get data by Offset you
[3664.4s] can also use get non-exported data which
[3666.5s] is interesting so I think this just like
[3667.5s] dumps all of the data as not exported um
[3670.7s] and then sends that over to you I
[3672.6s] believe but anyway you could also get
[3674.8s] the data by offset so if I go a get
[3677.8s] request to open api. octop course.com SL
[3680.4s] all and then I just send a header with
[3681.8s] the URL parameter this is a get request
[3685.2s] uh we're going to send a header with the
[3687.3s] token so
[3689.0s] authorization
[3690.6s] Bearer and then feed in the access token
[3693.6s] here just make sure that this is just
[3695.7s] one space no it's
[3697.1s] two if I feed this in um it's saying
[3701.0s] that it's a bad request let me just
[3702.3s] triple check why I think we need three
[3706.9s] Fields yeah I think we need three Fields
[3709.4s] actually my bad um we need uh this get
[3713.9s] request then we need the authorization
[3716.5s] header like I talked about then we need
[3718.6s] three Fields task ID yeah right
[3720.2s] obviously we need to feed in the task ID
[3721.7s] so you need task ID offset or size so um
[3725.0s] we'll feed this in as query parameters
[3727.2s] here so send query parameters the first
[3730.1s] value was task ID second one was
[3734.7s] offset and uh offset is no Capital the
[3738.0s] third was size offset's going to be zero
[3740.8s] size going to be I don't know let's just
[3742.0s] do 1,000 and what we need now is we need
[3744.3s] the task ID of the specific run that we
[3746.3s] just finished oh in order to get the
[3747.7s] task list you head over to task list top
[3749.8s] right hand corner here task ID API so we
[3752.5s] now have access to this so if we go back
[3755.1s] to our NN instance we could feed that in
[3757.2s] here by test the step you'll see that we
[3760.3s] now have all the data that we just asked
[3762.1s] for earlier so a variety of ways to do
[3764.4s] this um in practice like octop par
[3766.0s] allows you to schedule runs you could
[3767.2s] schedule them um using their you know
[3769.5s] whatever it is uh uh like cloud service
[3772.7s] you could use it to scrape I don't know
[3774.5s] Twitter uh they have a variety of like
[3776.0s] other scrapers um that you can check out
[3778.1s] just heading over
[3780.0s] to this new here uh if we just go sorry
[3783.6s] go down to templates um there's a
[3785.0s] variety of other ways to scrape Google
[3786.6s] job scraper glass door scraper super
[3788.4s] Pages scraper you could schedule these
[3790.0s] right and then what you can do in na is
[3791.8s] you can just query it once a day grab
[3793.4s] all the data like I showed you how to do
[3794.8s] a moment ago dump that into some sheet
[3797.2s] octoparse is pretty cool it's like more
[3798.6s] of like an industrial Enterprise level
[3800.6s] application um to be honest so there
[3803.0s] might be some gotas if you're not super
[3804.9s] familiar with working with like desktop
[3807.0s] apps and stuff but I I like the idea
[3808.7s] that you can also just scrape locally
[3810.0s] which is pretty sweet and the last of
[3811.7s] our nine best ways to scrape websites in
[3813.5s] nadn is browserless now browserless runs
[3816.6s] a headless Chrome instance in the cloud
[3818.6s] this stuff is great for dynamic or heavy
[3820.8s] JavaScript websites if you've never used
[3822.8s] browser list before the cool part about
[3825.4s] browser list is allows you to actually
[3826.8s] bypass captas which is a big issue that
[3828.5s] a lot of people have um so I'm going to
[3830.3s] click try it for free I'm going to enter
[3832.9s] my email address over here verify I need
[3836.3s] to submit a code so let's head back over
[3838.5s] here thank you thank you thank you thank
[3842.5s] you we have a ton of free trial signups
[3847.5s] obviously I don't have a promo code or
[3849.5s] anything don't have a company name I'm
[3851.3s] just going to enter a password I'm using
[3853.1s] this to get past uh to avoid setting up
[3855.2s] a puppeteer and playright server sure
[3857.6s] I'm going to click complete we're now
[3859.8s] going to have a th credits inside of
[3861.8s] browser list which is pretty sweet um
[3863.8s] and we'll get a we'll get a full plan
[3865.0s] eventually we now have an API token so I
[3867.3s] can figure out how all of the stuff
[3868.8s] works here I'm just going to dive right
[3869.8s] into the API I can figure out how all of
[3871.4s] the API stuff works using their API docs
[3873.1s] which are fantastic by the way um and we
[3875.7s] don't want to do any of this stuff we
[3877.0s] just want to do HTTP apis brow list API
[3879.2s] index okay great so here's where we're
[3881.0s] at um if you want to send and receive a
[3882.8s] request what you need to do is uh you
[3886.5s] send a request to one of these endpoints
[3888.9s] content unblock download function PDF
[3893.1s] screenshot scrape or performance
[3896.6s] what we want for the purpose of this is
[3898.6s] just uh let's do content okay this is
[3901.8s] the request right over here so I'm just
[3903.6s] going to paste my API token up here copy
[3905.9s] this
[3906.8s] request feed it into nadn in the HTTP
[3909.9s] request module as per
[3911.9s] usual nice quick and easy I'm going to
[3914.6s] grab my API token and where it says your
[3916.8s] API token here I'm going to feed that in
[3919.1s] what I want as a website is just left
[3921.4s] click. a I'm going to run test step we
[3924.6s] are now quering the pi and in seconds we
[3927.9s] have access to the data same thing that
[3930.9s] we had before but now we're using a pass
[3933.0s] through and browser list is a great pass
[3935.3s] through um because you know uh they they
[3938.4s] allow you to scrape things that go far
[3940.2s] beyond the usual static site thing so
[3944.0s] like honestly and I'm just leaving this
[3946.2s] as a secret and sort of a little I guess
[3948.2s] Easter egg for people that have made it
[3949.4s] this far in the video like my go-to when
[3951.8s] scraping websites is as I mentioned do
[3954.3s] that HTTP request trans forg that works
[3956.0s] then do something like fir C.D but if
[3957.9s] that doesn't work I I do something like
[3959.6s] browserless that has all of this stuff
[3961.9s] built in um and I especially use browser
[3965.0s] list anytime that there's some sort of
[3967.4s] you know application where I'm just
[3968.9s] going to save this so I can make all my
[3970.0s] HTP requests really easy um especially
[3972.6s] when you know there's issues with captas
[3974.3s] and and accessing resources and stuff
[3976.4s] check this out not only can you do um
[3979.8s] the actual scrape you can do a
[3981.0s] screenshot of the page as well and
[3982.4s] because I've entered my token up here
[3984.5s] the requests that I'm going to setting
[3985.9s] up are as simple as importing the
[3988.4s] curl then clicking test step so
[3991.9s] straightforward we now have a file which
[3994.4s] is the screenshot now I used example
[3996.2s] domain there let's go left
[3999.2s] click. run this test now you can see
[4002.3s] we've actually like received a
[4003.6s] screenshot of the of the website
[4007.1s] view very sexy and my website's pretty
[4009.6s] long so keep in
[4011.3s] mind um and yeah you know obviously a
[4013.7s] lot you could do with that you can
[4014.8s] download the site you can turn the site
[4016.3s] into a PDF so um that's pretty neat I
[4019.8s] don't think I've actually used this one
[4020.9s] before but for the purposes of this
[4022.2s] demonstration why don't we give it a try
[4024.2s] we'll go over here import the curl paste
[4027.2s] it in voila the website I'm going to do
[4030.0s] is left click. aai going to test this
[4034.0s] step so now there servers doing a couple
[4036.5s] things like I'm scraping the site then
[4038.4s] converting it all into PDF format um
[4040.5s] probably screenshotting a bunch of stuff
[4041.8s] too if I view this now we now have my my
[4044.2s] file looks like it didn't capture all of
[4046.4s] the color aspects um that might just be
[4049.1s] difficult or whatever but I still have
[4050.3s] like a PDF of the site which is pretty
[4051.8s] neat um and yeah let you guys kind of
[4054.1s] screw around with this on your own but
[4055.5s] there are a variety of cool applications
[4056.9s] you can use browless for all right I
[4059.1s] hope you guys appreciated the nine best
[4060.6s] ways to scrape websites in nadn as you
[4062.5s] guys could see it's a combination of on
[4065.0s] platform scraping using the HTTP request
[4067.1s] module a lot of like API documentation
[4070.0s] stuff like that if you want to get good
[4071.0s] at this I'm releasing a master class on
[4072.8s] API stuff um uh as part of my next na
[4076.0s] tutorial video uh and then you know
[4078.6s] navigating this and then and then taking
[4080.1s] the data from these services and using
[4081.8s] them to do something that you want to do
[4083.3s] like artificial intelligence to give you
[4084.9s] a summary of the site or generate ice
[4087.1s] breakers for you or do something else um
[4089.8s] whether you're using a local application
[4091.4s] like octop parse or maybe the web
[4093.0s] scraping CH uh Chrome extension or using
[4096.2s] something like firra browserless appify
[4099.1s] rapid API and so on and so forth um you
[4101.4s] now have everything that you need in
[4102.4s] order to scrape static sites Dynamic
[4104.4s] sites super Js heavy websites and even
[4106.6s] social media websites like Tik Tok
[4108.1s] Twitter and Instagram thanks so much for
[4109.9s] making it to this point in the video if
[4111.0s] you have any suggestions for future
[4112.3s] content drop them down below more than
[4114.1s] happy to take your idea and run with it
[4116.1s] assuming it's something that I haven't
[4117.4s] done before and then if you guys could
[4119.0s] do me a really big solid like subscribe
[4121.1s] do all that fun YouTube stuff and I'll
[4122.2s] catch you on the next video thank you
[4123.6s] very much
